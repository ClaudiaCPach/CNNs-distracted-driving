{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî• 05 ‚Äî Grad-CAM Visualizations\n",
        "\n",
        "**Purpose:** Generate attention visualizations to understand what models focus on.\n",
        "\n",
        "**Sections:**\n",
        "1. Inline Setup\n",
        "2. Copy Hybrid Crops to /content\n",
        "3. Build Modality Bundles (load models + predictions)\n",
        "4. Confidence Analysis\n",
        "5. Grad-CAM Gallery (correct/wrong √ó high/low confidence)\n",
        "6. Confusion-Pair Grad-CAM\n",
        "\n",
        "**Prerequisites:**\n",
        "- Trained checkpoints exist on Drive\n",
        "- Predictions exist for modalities you want to visualize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- INLINE SETUP ---\n",
        "import os, subprocess, sys\n",
        "\n",
        "REPO_DIRNAME   = \"CNNs-distracted-driving\"\n",
        "PROJECT_ROOT   = f\"/content/{REPO_DIRNAME}\"\n",
        "DRIVE_PATH     = \"/content/drive/MyDrive/TFM\"\n",
        "DRIVE_DATA_ROOT = f\"{DRIVE_PATH}/data\"\n",
        "FAST_DATA      = \"/content/data\"\n",
        "DATASET_ROOT   = DRIVE_DATA_ROOT\n",
        "OUT_ROOT       = f\"{DRIVE_PATH}/outputs\"\n",
        "CKPT_ROOT      = f\"{DRIVE_PATH}/checkpoints\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "if not os.path.isdir(PROJECT_ROOT):\n",
        "    subprocess.call(f\"git clone https://github.com/ClaudiaCPach/CNNs-distracted-driving {PROJECT_ROOT}\", shell=True)\n",
        "subprocess.call(f\"pip install -q -e {PROJECT_ROOT}\", shell=True)\n",
        "!pip -q install timm grad-cam\n",
        "\n",
        "os.environ[\"DRIVE_PATH\"] = DRIVE_PATH\n",
        "os.environ[\"DATASET_ROOT\"] = DATASET_ROOT\n",
        "os.environ[\"OUT_ROOT\"] = OUT_ROOT\n",
        "os.environ[\"CKPT_ROOT\"] = CKPT_ROOT\n",
        "os.environ[\"FAST_DATA\"] = FAST_DATA\n",
        "\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "sys.path.insert(0, os.path.join(PROJECT_ROOT, \"src\"))\n",
        "\n",
        "!nvidia-smi || echo \"No GPU\"\n",
        "print(\"‚úÖ Setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Section 2: Copy Hybrid Crops to /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy hybrid crops (needed for Grad-CAM visualization)\n",
        "import os, shutil\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "\n",
        "# Copy BOTH variants for comparison\n",
        "for HYBRID_VARIANT in [\"face\", \"face_hands\"]:\n",
        "    LOCAL_ROOT = Path(\"/content/data/hybrid\")\n",
        "    DRIVE_ROOT = Path(OUT_ROOT) / \"hybrid\"\n",
        "    LOCAL_VARIANT_DIR = LOCAL_ROOT / HYBRID_VARIANT\n",
        "    DRIVE_VARIANT_DIR = DRIVE_ROOT / HYBRID_VARIANT\n",
        "    \n",
        "    def count_jpgs(p: Path) -> int:\n",
        "        return sum(1 for _ in p.rglob(\"*.jpg\")) if p.exists() else 0\n",
        "    \n",
        "    local_count = count_jpgs(LOCAL_VARIANT_DIR)\n",
        "    drive_count = count_jpgs(DRIVE_VARIANT_DIR)\n",
        "    \n",
        "    if local_count > 0:\n",
        "        print(f\"‚úÖ {HYBRID_VARIANT}: Already in /content ({local_count} jpgs)\")\n",
        "    elif drive_count > 0:\n",
        "        print(f\"üì¶ {HYBRID_VARIANT}: Copying from Drive...\")\n",
        "        LOCAL_VARIANT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "        file_count = 0\n",
        "        for src_dir, _, files in os.walk(DRIVE_VARIANT_DIR):\n",
        "            rel_dir = Path(src_dir).relative_to(DRIVE_VARIANT_DIR)\n",
        "            dst_dir = LOCAL_VARIANT_DIR / rel_dir\n",
        "            dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "            for fname in files:\n",
        "                if fname.lower().endswith(\".jpg\"):\n",
        "                    shutil.copy2(Path(src_dir) / fname, dst_dir / fname)\n",
        "                    file_count += 1\n",
        "        print(f\"   Copied {file_count} images\")\n",
        "        \n",
        "        for fname in [f\"manifest_{HYBRID_VARIANT}.csv\", f\"train_{HYBRID_VARIANT}.csv\",\n",
        "                      f\"val_{HYBRID_VARIANT}.csv\", f\"test_{HYBRID_VARIANT}.csv\"]:\n",
        "            src = DRIVE_ROOT / fname\n",
        "            if src.exists():\n",
        "                shutil.copy2(src, LOCAL_ROOT / fname)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è {HYBRID_VARIANT}: Not found on Drive\")\n",
        "\n",
        "os.environ[\"HYBRID_ROOT_LOCAL\"] = str(Path(\"/content/data/hybrid\"))\n",
        "os.environ[\"DATASET_ROOT\"] = str(Path(\"/content/data/hybrid\"))\n",
        "print(f\"\\n‚úÖ DATASET_ROOT = {os.environ['DATASET_ROOT']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Section 3: Build Modality Bundles\n",
        "\n",
        "Load models, predictions, and Grad-CAM objects for each modality.\n",
        "\n",
        "**5-Run Experimental Plan:**\n",
        "| Run | Name | Tag Example | Mode |\n",
        "|-----|------|-------------|------|\n",
        "| 1 | Full | `effb0_fullframe_v1` | full |\n",
        "| 2 | Face | `effb0_face_v1` | hybrid |\n",
        "| 3 | Face+Hands | `effb0_face_hands_v1` | hybrid |\n",
        "| 4 | Ctrl-FaceSub | `effb0_fullframe_facesubset_v1` | full |\n",
        "| 5 | Ctrl-FHSub | `effb0_fullframe_fhsubset_v1` | full |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== CONFIGURE YOUR RUNS ==============\n",
        "# Update tags to match your experiment naming from 02_training.ipynb\n",
        "# Set any entry to comment-out or remove to skip\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "RUNS = [\n",
        "    # --- Natural Runs (different ID sets) ---\n",
        "    {\"name\": \"Full\",       \"tag\": \"effb0_fullframe_v1\",             \"mode\": \"full\",   \"roi_variant\": None},\n",
        "    {\"name\": \"Face\",       \"tag\": \"effb0_face_v1\",                  \"mode\": \"hybrid\", \"roi_variant\": \"face\"},\n",
        "    {\"name\": \"Face+Hands\", \"tag\": \"effb0_face_hands_v1\",            \"mode\": \"hybrid\", \"roi_variant\": \"face_hands\"},\n",
        "    \n",
        "    # --- Control Runs (same IDs as ROI runs, but full-frame) ---\n",
        "    {\"name\": \"Ctrl-FaceSub\", \"tag\": \"effb0_fullframe_facesubset_v1\", \"mode\": \"full\", \"roi_variant\": None},\n",
        "    {\"name\": \"Ctrl-FHSub\",   \"tag\": \"effb0_fullframe_fhsubset_v1\",   \"mode\": \"full\", \"roi_variant\": None},\n",
        "]\n",
        "\n",
        "# Filter to only runs whose checkpoints/predictions exist (set to True to auto-filter)\n",
        "AUTO_FILTER_AVAILABLE = True\n",
        "\n",
        "MODEL_NAME = \"efficientnet_b0\"\n",
        "SPLIT_TO_ANALYZE = \"test\"\n",
        "IMAGE_SIZE = 224\n",
        "\n",
        "FORCE_CPU = False\n",
        "import torch\n",
        "device = torch.device(\"cpu\" if FORCE_CPU else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "print(f\"üß† Using device: {device}\")\n",
        "\n",
        "if AUTO_FILTER_AVAILABLE:\n",
        "    available_runs = []\n",
        "    for run in RUNS:\n",
        "        ckpt_path = Path(CKPT_ROOT) / run[\"tag\"] / \"best.pt\"\n",
        "        pred_path = Path(OUT_ROOT) / \"preds\" / SPLIT_TO_ANALYZE / f\"{run['tag']}_{SPLIT_TO_ANALYZE}.csv\"\n",
        "        if ckpt_path.exists() and pred_path.exists():\n",
        "            available_runs.append(run)\n",
        "            print(f\"‚úÖ {run['name']}: checkpoint + predictions found\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  {run['name']}: skipping (ckpt={ckpt_path.exists()}, preds={pred_path.exists()})\")\n",
        "    RUNS = available_runs\n",
        "    print(f\"\\nüéØ Analyzing {len(RUNS)} runs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build modality bundles\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from torchvision import transforms\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from ddriver.models.registry import build_model, register_timm_backbone\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def get_target_layers(model, model_name):\n",
        "    base = getattr(model, \"backbone\", model)\n",
        "    if \"efficientnet\" in model_name.lower():\n",
        "        if hasattr(base, \"conv_head\"):\n",
        "            return [base.conv_head]\n",
        "        elif hasattr(base, \"blocks\"):\n",
        "            return [base.blocks[-1]]\n",
        "    for attr in [\"features\", \"blocks\", \"stages\", \"layer4\"]:\n",
        "        if hasattr(base, attr):\n",
        "            layer = getattr(base, attr)\n",
        "            if hasattr(layer, \"__getitem__\"):\n",
        "                return [layer[-1]]\n",
        "    return [list(base.children())[-2]]\n",
        "\n",
        "def find_ckpt_for_tag(run_tag):\n",
        "    run_base = Path(CKPT_ROOT) / \"runs\" / run_tag\n",
        "    all_runs = sorted(run_base.glob(\"*/\"))\n",
        "    if not all_runs:\n",
        "        raise FileNotFoundError(f\"No runs under {run_base}\")\n",
        "    latest = all_runs[-1]\n",
        "    for name in [\"best.pt\", \"last.pt\"]:\n",
        "        if (latest / name).exists():\n",
        "            return latest / name\n",
        "    raise FileNotFoundError(f\"No checkpoint in {latest}\")\n",
        "\n",
        "def find_preds_csv(run_tag, split):\n",
        "    for pattern in [f\"{run_tag}_{split}.csv\", f\"{run_tag}.csv\"]:\n",
        "        p = Path(OUT_ROOT) / \"preds\" / split / pattern\n",
        "        if p.exists():\n",
        "            return p\n",
        "    raise FileNotFoundError(f\"Preds not found for {run_tag}\")\n",
        "\n",
        "def extract_class_from_path(p):\n",
        "    for part in Path(p).parts:\n",
        "        if part.startswith(\"c\") and len(part) == 2 and part[1].isdigit():\n",
        "            return part\n",
        "    return \"\"\n",
        "\n",
        "def class_to_int(class_id):\n",
        "    if pd.isna(class_id):\n",
        "        return -1\n",
        "    if isinstance(class_id, str) and class_id.startswith(\"c\"):\n",
        "        return int(class_id[1:])\n",
        "    return int(class_id)\n",
        "\n",
        "def load_bundle(run):\n",
        "    run_tag = run[\"tag\"]\n",
        "    mode = run[\"mode\"]\n",
        "    roi_variant = run[\"roi_variant\"]\n",
        "    \n",
        "    print(f\"\\nüì¶ Loading: {run['name']} ({run_tag})\")\n",
        "    \n",
        "    ckpt_path = find_ckpt_for_tag(run_tag)\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    \n",
        "    try:\n",
        "        register_timm_backbone(MODEL_NAME)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    msd = ckpt.get(\"model_state_dict\", {})\n",
        "    num_classes = msd.get(\"classifier.weight\", torch.zeros(10, 1)).shape[0]\n",
        "    \n",
        "    model = build_model(MODEL_NAME, num_classes=num_classes, pretrained=False)\n",
        "    model.load_state_dict(msd)\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    \n",
        "    target_layers = get_target_layers(model, MODEL_NAME)\n",
        "    cam = GradCAM(model=model, target_layers=target_layers)\n",
        "    \n",
        "    preds_csv = find_preds_csv(run_tag, SPLIT_TO_ANALYZE)\n",
        "    preds_df = pd.read_csv(preds_csv)\n",
        "    \n",
        "    if mode == \"hybrid\":\n",
        "        data_root = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", Path(OUT_ROOT) / \"hybrid\"))\n",
        "        manifest_path = data_root / f\"manifest_{roi_variant}.csv\"\n",
        "    else:\n",
        "        data_root = Path(DATASET_ROOT)\n",
        "        manifest_path = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
        "    \n",
        "    manifest_df = pd.read_csv(manifest_path)\n",
        "    \n",
        "    preds_df[\"_class\"] = preds_df[\"path\"].apply(extract_class_from_path)\n",
        "    preds_df[\"_filename\"] = preds_df[\"path\"].apply(lambda p: Path(p).name)\n",
        "    manifest_df[\"_class\"] = manifest_df[\"class_id\"]\n",
        "    manifest_df[\"_filename\"] = manifest_df[\"path\"].apply(lambda p: Path(p).name)\n",
        "    \n",
        "    manifest_for_merge = manifest_df[[\"_class\", \"_filename\", \"path\", \"class_id\"]].drop_duplicates(\n",
        "        subset=[\"_class\", \"_filename\"], keep=\"first\"\n",
        "    ).rename(columns={\"path\": \"crop_path\"})\n",
        "    \n",
        "    preds_df = preds_df.merge(manifest_for_merge, on=[\"_class\", \"_filename\"], how=\"left\")\n",
        "    preds_df = preds_df.dropna(subset=[\"class_id\"]).copy()\n",
        "    preds_df[\"vis_path\"] = preds_df[\"crop_path\"]\n",
        "    preds_df[\"label\"] = preds_df[\"class_id\"].apply(class_to_int)\n",
        "    preds_df[\"pred\"] = preds_df[\"pred_class_id\"].apply(class_to_int)\n",
        "    if \"confidence\" not in preds_df.columns:\n",
        "        preds_df[\"confidence\"] = 1.0\n",
        "    \n",
        "    def generate_gradcam_fn(img_path):\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img_resized = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
        "        img_np = np.array(img_resized) / 255.0\n",
        "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "        grayscale_cam = cam(input_tensor=img_tensor, targets=None)[0, :]\n",
        "        visualization = show_cam_on_image(img_np.astype(np.float32), grayscale_cam, use_rgb=True)\n",
        "        return visualization, grayscale_cam\n",
        "    \n",
        "    print(f\"   ‚úÖ Loaded {len(preds_df)} predictions, checkpoint from {ckpt_path.name}\")\n",
        "    \n",
        "    return {\n",
        "        \"run_name\": run[\"name\"],\n",
        "        \"tag\": run_tag,\n",
        "        \"mode\": mode,\n",
        "        \"roi_variant\": roi_variant,\n",
        "        \"preds_df\": preds_df,\n",
        "        \"model\": model,\n",
        "        \"cam\": cam,\n",
        "        \"device\": device,\n",
        "        \"data_root\": data_root,\n",
        "        \"generate_gradcam\": generate_gradcam_fn,\n",
        "    }\n",
        "\n",
        "MODALITY_BUNDLES = {}\n",
        "for run in RUNS:\n",
        "    try:\n",
        "        key = run[\"name\"].lower().replace(\"+\", \"_\").replace(\" \", \"_\")\n",
        "        MODALITY_BUNDLES[key] = load_bundle(run)\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Skipped {run['name']}: {e}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded {len(MODALITY_BUNDLES)} bundles: {list(MODALITY_BUNDLES.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Section 4: Confidence Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confidence summary per modality\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "HIGH_CONF_THRESHOLD = 0.8\n",
        "results = []\n",
        "\n",
        "for bundle_key, bundle in MODALITY_BUNDLES.items():\n",
        "    preds_df = bundle[\"preds_df\"].copy()\n",
        "    preds_df[\"correct\"] = preds_df[\"pred\"] == preds_df[\"label\"]\n",
        "    \n",
        "    correct_df = preds_df[preds_df[\"correct\"]]\n",
        "    wrong_df = preds_df[~preds_df[\"correct\"]]\n",
        "    \n",
        "    mean_conf_correct = correct_df[\"confidence\"].mean() if len(correct_df) > 0 else np.nan\n",
        "    mean_conf_wrong = wrong_df[\"confidence\"].mean() if len(wrong_df) > 0 else np.nan\n",
        "    \n",
        "    high_conf = preds_df[preds_df[\"confidence\"] >= HIGH_CONF_THRESHOLD]\n",
        "    overconf_rate = (len(high_conf[~high_conf[\"correct\"]]) / len(high_conf) * 100) if len(high_conf) > 0 else 0\n",
        "    \n",
        "    results.append({\n",
        "        \"Modality\": bundle[\"run_name\"],\n",
        "        \"N\": len(preds_df),\n",
        "        \"Accuracy\": (preds_df[\"correct\"].mean() * 100),\n",
        "        \"Conf (Correct)\": mean_conf_correct,\n",
        "        \"Conf (Wrong)\": mean_conf_wrong,\n",
        "        \"Conf Gap\": mean_conf_correct - mean_conf_wrong if not np.isnan(mean_conf_wrong) else np.nan,\n",
        "        \"Overconf Error %\": overconf_rate,\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"=\" * 80)\n",
        "print(\"üìä CONFIDENCE SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî• Section 5: Grad-CAM Gallery\n",
        "\n",
        "Generate example visualizations for correct/wrong predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grad-CAM gallery: sample from each category\n",
        "from PIL import Image\n",
        "\n",
        "CLASS_NAMES = {\n",
        "    0: \"Safe\", 1: \"Txt-R\", 2: \"Ph-R\", 3: \"Txt-L\",\n",
        "    4: \"Ph-L\", 5: \"Radio\", 6: \"Drink\", 7: \"Reach\",\n",
        "    8: \"Hair\", 9: \"Pass\"\n",
        "}\n",
        "\n",
        "BUNDLE_KEY = \"face\"  # face | face_hands | full\n",
        "N_SAMPLES = 3\n",
        "\n",
        "if BUNDLE_KEY not in MODALITY_BUNDLES:\n",
        "    print(f\"‚ö†Ô∏è Bundle '{BUNDLE_KEY}' not loaded\")\n",
        "else:\n",
        "    bundle = MODALITY_BUNDLES[BUNDLE_KEY]\n",
        "    preds_df = bundle[\"preds_df\"].copy()\n",
        "    preds_df[\"correct\"] = preds_df[\"pred\"] == preds_df[\"label\"]\n",
        "    \n",
        "    def find_image_path(vis_path, data_root, roi_variant):\n",
        "        p = Path(vis_path)\n",
        "        if p.is_absolute() and p.exists():\n",
        "            return p\n",
        "        candidate = data_root / vis_path\n",
        "        if candidate.exists():\n",
        "            return candidate\n",
        "        if roi_variant:\n",
        "            candidate = data_root / roi_variant / vis_path\n",
        "            if candidate.exists():\n",
        "                return candidate\n",
        "        return None\n",
        "    \n",
        "    # Sample categories\n",
        "    categories = [\n",
        "        (\"Correct + High Conf\", preds_df[(preds_df[\"correct\"]) & (preds_df[\"confidence\"] >= 0.9)]),\n",
        "        (\"Wrong + High Conf\", preds_df[(~preds_df[\"correct\"]) & (preds_df[\"confidence\"] >= 0.8)]),\n",
        "        (\"Wrong + Low Conf\", preds_df[(~preds_df[\"correct\"]) & (preds_df[\"confidence\"] < 0.5)]),\n",
        "    ]\n",
        "    \n",
        "    for cat_name, cat_df in categories:\n",
        "        if len(cat_df) == 0:\n",
        "            print(f\"\\n‚ö†Ô∏è {cat_name}: No samples\")\n",
        "            continue\n",
        "        \n",
        "        samples = cat_df.sample(min(N_SAMPLES, len(cat_df)))\n",
        "        print(f\"\\nüì∏ {cat_name} ({len(cat_df)} total, showing {len(samples)})\")\n",
        "        \n",
        "        fig, axes = plt.subplots(1, len(samples), figsize=(4*len(samples), 4))\n",
        "        if len(samples) == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for ax, (_, row) in zip(axes, samples.iterrows()):\n",
        "            img_path = find_image_path(row[\"vis_path\"], bundle[\"data_root\"], bundle[\"roi_variant\"])\n",
        "            if img_path and img_path.exists():\n",
        "                viz, _ = bundle[\"generate_gradcam\"](img_path)\n",
        "                ax.imshow(viz)\n",
        "                true_label = CLASS_NAMES.get(row[\"label\"], f\"c{row['label']}\")\n",
        "                pred_label = CLASS_NAMES.get(row[\"pred\"], f\"c{row['pred']}\")\n",
        "                icon = \"‚úÖ\" if row[\"correct\"] else \"‚ùå\"\n",
        "                ax.set_title(f\"True: {true_label}\\nPred: {pred_label} ({row['confidence']:.2f}) {icon}\", fontsize=10)\n",
        "            else:\n",
        "                ax.text(0.5, 0.5, \"Not found\", ha=\"center\", va=\"center\")\n",
        "            ax.axis(\"off\")\n",
        "        \n",
        "        plt.suptitle(f\"{bundle['run_name']}: {cat_name}\", fontweight=\"bold\")\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        out_dir = Path(OUT_ROOT) / \"gradcam\" / bundle[\"tag\"]\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "        out_path = out_dir / f\"{cat_name.replace(' ', '_').lower()}.png\"\n",
        "        plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "        print(f\"   üíæ Saved to {out_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Section 6: Confusion-Pair Grad-CAM\n",
        "\n",
        "Investigate specific confusion pairs to understand why the model fails.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion-pair Grad-CAM\n",
        "CONFUSION_PAIRS = [\n",
        "    (5, 8, \"Radio ‚Üí Hair/Makeup\"),\n",
        "    (9, 0, \"Passenger ‚Üí Safe\"),\n",
        "    (1, 2, \"Texting(R) ‚Üí Phone(R)\"),\n",
        "]\n",
        "\n",
        "N_EXAMPLES = 3\n",
        "BUNDLE_KEY = \"face_hands\"  # Which modality to analyze\n",
        "\n",
        "if BUNDLE_KEY not in MODALITY_BUNDLES:\n",
        "    print(f\"‚ö†Ô∏è Bundle '{BUNDLE_KEY}' not loaded\")\n",
        "else:\n",
        "    bundle = MODALITY_BUNDLES[BUNDLE_KEY]\n",
        "    preds_df = bundle[\"preds_df\"]\n",
        "    \n",
        "    for true_c, pred_c, desc in CONFUSION_PAIRS:\n",
        "        confusion_df = preds_df[(preds_df[\"label\"] == true_c) & (preds_df[\"pred\"] == pred_c)]\n",
        "        \n",
        "        if len(confusion_df) == 0:\n",
        "            print(f\"\\n‚¨ú {desc}: No examples\")\n",
        "            continue\n",
        "        \n",
        "        samples = confusion_df.sample(min(N_EXAMPLES, len(confusion_df)))\n",
        "        print(f\"\\nüîç {desc} ({len(confusion_df)} total, showing {len(samples)})\")\n",
        "        \n",
        "        fig, axes = plt.subplots(1, len(samples), figsize=(4*len(samples), 4))\n",
        "        if len(samples) == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for ax, (_, row) in zip(axes, samples.iterrows()):\n",
        "            img_path = find_image_path(row[\"vis_path\"], bundle[\"data_root\"], bundle[\"roi_variant\"])\n",
        "            if img_path and img_path.exists():\n",
        "                viz, _ = bundle[\"generate_gradcam\"](img_path)\n",
        "                ax.imshow(viz)\n",
        "                ax.set_title(f\"True: {CLASS_NAMES.get(true_c)}\\nPred: {CLASS_NAMES.get(pred_c)} ({row['confidence']:.2f})\", fontsize=10)\n",
        "            else:\n",
        "                ax.text(0.5, 0.5, \"Not found\", ha=\"center\", va=\"center\")\n",
        "            ax.axis(\"off\")\n",
        "        \n",
        "        plt.suptitle(f\"{bundle['run_name']}: {desc}\", fontweight=\"bold\")\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        out_dir = Path(OUT_ROOT) / \"gradcam\" / \"confusions\"\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "        safe_name = desc.replace(\" \", \"_\").replace(\"‚Üí\", \"to\").replace(\"/\", \"_\")\n",
        "        out_path = out_dir / f\"{safe_name}__{BUNDLE_KEY}.png\"\n",
        "        plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "        print(f\"   üíæ Saved to {out_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Section 7: ROI vs Control Side-by-Side Comparison\n",
        "\n",
        "Compare Grad-CAM attention between ROI models and their matched full-frame controls **on the same image IDs**. This shows whether the ROI crop helps the model focus on semantically relevant features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROI vs Control Side-by-Side Comparison\n",
        "# Compare attention on SAME images (matched by original image ID)\n",
        "\n",
        "# Comparisons to make (ROI model, Control model)\n",
        "COMPARISONS = [\n",
        "    (\"Face+Hands\", \"Ctrl-FHSub\"),   # Face+Hands ROI vs Full-frame on same IDs\n",
        "    (\"Face\", \"Ctrl-FaceSub\"),       # Face ROI vs Full-frame on same IDs\n",
        "]\n",
        "\n",
        "N_COMPARISON_SAMPLES = 6  # Per comparison\n",
        "SAMPLE_CATEGORIES = [\"correct\", \"wrong\"]  # Sample from correct and wrong predictions\n",
        "\n",
        "for roi_name, ctrl_name in COMPARISONS:\n",
        "    if roi_name not in MODALITY_BUNDLES or ctrl_name not in MODALITY_BUNDLES:\n",
        "        print(f\"‚ö†Ô∏è Skipping {roi_name} vs {ctrl_name}: one or both not loaded\")\n",
        "        continue\n",
        "    \n",
        "    roi_bundle = MODALITY_BUNDLES[roi_name]\n",
        "    ctrl_bundle = MODALITY_BUNDLES[ctrl_name]\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üéØ COMPARING: {roi_name} (ROI) vs {ctrl_name} (Full-frame control)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Find common image IDs (by extracting original image ID from paths)\n",
        "    roi_df = roi_bundle[\"preds_df\"].copy()\n",
        "    ctrl_df = ctrl_bundle[\"preds_df\"].copy()\n",
        "    \n",
        "    # For ROI predictions, extract original image ID from crop path\n",
        "    # Expected path format: .../face_hands/c0/img_123_driver_uuid.jpg -> img_123_driver\n",
        "    def extract_original_id(path):\n",
        "        fname = Path(path).stem  # e.g., \"img_123_driver_uuid\"\n",
        "        # Remove hybrid-specific suffixes like _uuid\n",
        "        parts = fname.split(\"_\")\n",
        "        if len(parts) >= 3:\n",
        "            return \"_\".join(parts[:3])  # img_123_driver\n",
        "        return fname\n",
        "    \n",
        "    roi_df[\"orig_id\"] = roi_df[\"vis_path\"].apply(extract_original_id)\n",
        "    ctrl_df[\"orig_id\"] = ctrl_df[\"vis_path\"].apply(extract_original_id)\n",
        "    \n",
        "    # Find common IDs\n",
        "    common_ids = set(roi_df[\"orig_id\"]) & set(ctrl_df[\"orig_id\"])\n",
        "    print(f\"üìä Common image IDs: {len(common_ids)}\")\n",
        "    \n",
        "    if len(common_ids) < N_COMPARISON_SAMPLES:\n",
        "        print(f\"‚ö†Ô∏è Not enough common IDs for comparison\")\n",
        "        continue\n",
        "    \n",
        "    # Filter to common IDs\n",
        "    roi_common = roi_df[roi_df[\"orig_id\"].isin(common_ids)]\n",
        "    ctrl_common = ctrl_df[ctrl_df[\"orig_id\"].isin(common_ids)]\n",
        "    \n",
        "    # Add correctness\n",
        "    roi_common[\"correct\"] = roi_common[\"pred\"] == roi_common[\"label\"]\n",
        "    ctrl_common[\"correct\"] = ctrl_common[\"pred\"] == ctrl_common[\"label\"]\n",
        "    \n",
        "    # Sample images for comparison\n",
        "    for cat in SAMPLE_CATEGORIES:\n",
        "        is_correct = (cat == \"correct\")\n",
        "        \n",
        "        # Get IDs where ROI model was correct/wrong\n",
        "        roi_filtered = roi_common[roi_common[\"correct\"] == is_correct]\n",
        "        if len(roi_filtered) < N_COMPARISON_SAMPLES:\n",
        "            print(f\"‚ö†Ô∏è Not enough {cat} samples for {roi_name}\")\n",
        "            continue\n",
        "        \n",
        "        sample_ids = roi_filtered.sample(min(N_COMPARISON_SAMPLES, len(roi_filtered)))[\"orig_id\"].tolist()\n",
        "        \n",
        "        # Create side-by-side figure\n",
        "        fig, axes = plt.subplots(2, len(sample_ids), figsize=(4*len(sample_ids), 8))\n",
        "        \n",
        "        for col_idx, orig_id in enumerate(sample_ids):\n",
        "            # Get ROI row\n",
        "            roi_row = roi_common[roi_common[\"orig_id\"] == orig_id].iloc[0]\n",
        "            # Get Control row with same orig_id\n",
        "            ctrl_rows = ctrl_common[ctrl_common[\"orig_id\"] == orig_id]\n",
        "            if len(ctrl_rows) == 0:\n",
        "                continue\n",
        "            ctrl_row = ctrl_rows.iloc[0]\n",
        "            \n",
        "            # ROI Grad-CAM (top row)\n",
        "            ax_roi = axes[0, col_idx] if len(sample_ids) > 1 else axes[0]\n",
        "            roi_img_path = Path(roi_bundle[\"data_root\"]) / roi_bundle[\"roi_variant\"] / roi_row[\"vis_path\"]\n",
        "            if not roi_img_path.exists():\n",
        "                roi_img_path = Path(roi_row[\"vis_path\"])\n",
        "            \n",
        "            if roi_img_path.exists():\n",
        "                viz_roi, _ = roi_bundle[\"generate_gradcam\"](roi_img_path)\n",
        "                ax_roi.imshow(viz_roi)\n",
        "            ax_roi.set_title(f\"{roi_name}\\nP:{CLASS_NAMES.get(roi_row['pred'], '?')[:6]} ({roi_row['confidence']:.2f})\", fontsize=9)\n",
        "            ax_roi.axis(\"off\")\n",
        "            \n",
        "            # Control Grad-CAM (bottom row) - uses full-frame image\n",
        "            ax_ctrl = axes[1, col_idx] if len(sample_ids) > 1 else axes[1]\n",
        "            ctrl_img_path = Path(ctrl_bundle[\"data_root\"]) / ctrl_row[\"vis_path\"]\n",
        "            if not ctrl_img_path.exists():\n",
        "                ctrl_img_path = Path(ctrl_row[\"vis_path\"])\n",
        "            \n",
        "            if ctrl_img_path.exists():\n",
        "                viz_ctrl, _ = ctrl_bundle[\"generate_gradcam\"](ctrl_img_path)\n",
        "                ax_ctrl.imshow(viz_ctrl)\n",
        "            ax_ctrl.set_title(f\"{ctrl_name}\\nP:{CLASS_NAMES.get(ctrl_row['pred'], '?')[:6]} ({ctrl_row['confidence']:.2f})\", fontsize=9)\n",
        "            ax_ctrl.axis(\"off\")\n",
        "        \n",
        "        # Super title with category\n",
        "        true_class = CLASS_NAMES.get(roi_row[\"label\"], f\"c{roi_row['label']}\")\n",
        "        status = \"‚úÖ ROI Correct\" if is_correct else \"‚ùå ROI Wrong\"\n",
        "        fig.suptitle(f\"{roi_name} vs {ctrl_name} | {status} | True: {true_class}\", fontweight=\"bold\", fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Save\n",
        "        out_dir = Path(OUT_ROOT) / \"gradcam\" / \"control_comparison\"\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "        out_path = out_dir / f\"{roi_name}_vs_{ctrl_name}_{cat}.png\"\n",
        "        plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "        print(f\"üíæ Saved to {out_path}\")\n",
        "\n",
        "print(\"\\n‚úÖ Control comparison complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Grad-CAM Complete!\n",
        "\n",
        "**Outputs saved to Drive:**\n",
        "- `OUT_ROOT/gradcam/{model_tag}/` ‚Äî Per-category Grad-CAM galleries\n",
        "- `OUT_ROOT/gradcam/confusions/` ‚Äî Confusion-pair visualizations\n",
        "- `OUT_ROOT/gradcam/control_comparison/` ‚Äî ROI vs Control side-by-side comparisons\n",
        "\n",
        "**Use these figures in your thesis to:**\n",
        "- Show what the model focuses on when correct (face, hands, posture)\n",
        "- Identify shortcuts (looking at background, identity features)\n",
        "- Explain why certain confusions happen\n",
        "- **Demonstrate ROI effect:** Control comparison figures show attention on the same image IDs\n",
        "\n",
        "**5-Run Control Analysis Key Findings:**\n",
        "- Compare `Face+Hands` vs `Ctrl-FHSub` on same IDs ‚Üí isolates ROI cropping effect\n",
        "- Full-frame controls may attend to background/distractors\n",
        "- ROI crops should attend more to face/hands regions\n",
        "- Use these comparisons to justify (or question) ROI preprocessing\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
