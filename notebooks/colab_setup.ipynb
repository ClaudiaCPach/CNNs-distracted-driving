{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7296b001",
   "metadata": {},
   "source": [
    "\n",
    "# üöÄ Colab Setup ‚Äî **CNNs-distracted-driving** (hardcoded + config-aware)\n",
    "\n",
    "This version is **simplified and hardcoded** for your repo and URL, and it **respects your `src/ddriver/config.py`**.\n",
    "- Repo name fixed to **`CNNs-distracted-driving`**\n",
    "- Repo URL fixed to **`https://github.com/ClaudiaCPach/CNNs-distracted-driving`**\n",
    "- Uses your `config.py` convention: when running in Colab, we **set env vars** (`DRIVE_PATH`, `DATASET_ROOT`, `OUT_ROOT`, `CKPT_ROOT`, `FAST_DATA`) so your code reads correct paths via `ddriver.config`.\n",
    "- Optional `FAST_DATA` at `/content/data` for faster I/O (if you later copy data there).\n",
    "\n",
    "> Run cells **top ‚Üí bottom** the first time. Re-run **Update repo** to pull new commits after you push.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0fd729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîß 0) (Optional) quick GPU check\n",
    "!nvidia-smi || echo \"No GPU detected ‚Äî CPU runtime is okay for setup steps.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîß 1) Fixed config for your repo + Drive layout\n",
    "import os\n",
    "\n",
    "REPO_URL       = \"https://github.com/ClaudiaCPach/CNNs-distracted-driving\"\n",
    "REPO_DIRNAME   = \"CNNs-distracted-driving\"   # hardcoded\n",
    "BRANCH         = \"main\"\n",
    "PROJECT_ROOT   = f\"/content/{REPO_DIRNAME}\"  # where the repo will live in Colab\n",
    "\n",
    "# Your persistent Google Drive base folder (matches your project docs):\n",
    "DRIVE_PATH       = \"/content/drive/MyDrive/TFM\"\n",
    "DRIVE_DATA_ROOT  = f\"{DRIVE_PATH}/data\"      # contains auc.distracted.driver.dataset_v2\n",
    "\n",
    "# Optional: a fast, ephemeral workspace inside the VM\n",
    "FAST_DATA        = \"/content/data\"           # rsync target for faster I/O (lives on the VM SSD)\n",
    "\n",
    "# Start with Drive as the canonical dataset root; later cells can switch to FAST_DATA\n",
    "DATASET_ROOT     = DRIVE_DATA_ROOT\n",
    "OUT_ROOT         = f\"{DRIVE_PATH}/outputs\"\n",
    "CKPT_ROOT        = f\"{DRIVE_PATH}/checkpoints\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîå 2) Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "print(\"‚úÖ Drive mounted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df094a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìÅ 3) Clone or update the repo (no name inference ‚Äî all hardcoded)\n",
    "import os, subprocess\n",
    "\n",
    "def sh(cmd):\n",
    "    print(f\"\\n$ {cmd}\")\n",
    "    rc = subprocess.call(cmd, shell=True, executable=\"/bin/bash\")\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {rc}: {cmd}\")\n",
    "\n",
    "if os.path.isdir(PROJECT_ROOT):\n",
    "    print(f\"üìÅ Repo already present at {PROJECT_ROOT}. Pulling latest on branch {BRANCH}...\")\n",
    "    sh(f\"cd {PROJECT_ROOT} && git fetch origin {BRANCH} && git checkout {BRANCH} && git pull --rebase origin {BRANCH}\")\n",
    "else:\n",
    "    print(f\"‚¨áÔ∏è Cloning {REPO_URL} ‚Üí {PROJECT_ROOT}\")\n",
    "    sh(f\"git clone --branch {BRANCH} {REPO_URL} {PROJECT_ROOT}\")\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üì¶ 4) Install the repo (editable) + requirements (uses pyproject.toml if present)\n",
    "import os, subprocess\n",
    "\n",
    "def sh(cmd):\n",
    "    print(f\"\\n$ {cmd}\")\n",
    "    rc = subprocess.call(cmd, shell=True, executable=\"/bin/bash\")\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {rc}: {cmd}\")\n",
    "\n",
    "print(\"üîÑ Upgrading pip/setuptools/wheel...\")\n",
    "sh(\"python -m pip install --upgrade pip setuptools wheel\")\n",
    "\n",
    "has_pyproject = os.path.exists(os.path.join(PROJECT_ROOT, \"pyproject.toml\"))\n",
    "if has_pyproject:\n",
    "    print(\"üì¶ Editable install from pyproject.toml ...\")\n",
    "    sh(f\"cd {PROJECT_ROOT} && pip install -e .\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No pyproject.toml found. Skipping editable install.\")\n",
    "\n",
    "req_path = os.path.join(PROJECT_ROOT, \"requirements.txt\")\n",
    "if os.path.exists(req_path):\n",
    "    print(\"üìù Installing requirements.txt...\")\n",
    "    sh(f\"pip install -r {req_path}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No requirements.txt found ‚Äî continuing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accfc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üå≥ 5) Configure environment for your ddriver.config (Colab branch)\n",
    "# Your config.py reads env vars and falls back to sensible defaults when in Colab.\n",
    "import os\n",
    "\n",
    "os.environ[\"DRIVE_PATH\"]   = DRIVE_PATH\n",
    "os.environ[\"DATASET_ROOT\"] = DATASET_ROOT\n",
    "os.environ[\"OUT_ROOT\"]     = OUT_ROOT\n",
    "os.environ[\"CKPT_ROOT\"]    = CKPT_ROOT\n",
    "os.environ[\"FAST_DATA\"]    = FAST_DATA\n",
    "\n",
    "# Also write a .env (harmless in Colab; helpful if code calls load_dotenv())\n",
    "env_text = f\"\"\"DRIVE_PATH={DRIVE_PATH}\n",
    "DATASET_ROOT={DATASET_ROOT}\n",
    "OUT_ROOT={OUT_ROOT}\n",
    "CKPT_ROOT={CKPT_ROOT}\n",
    "FAST_DATA={FAST_DATA}\n",
    "\"\"\"\n",
    "with open(os.path.join(PROJECT_ROOT, \".env\"), \"w\") as f:\n",
    "    f.write(env_text)\n",
    "\n",
    "print(\"‚úÖ Environment variables set for ddriver.config\")\n",
    "print(\"\\nSummary:\")\n",
    "for k in [\"DRIVE_PATH\",\"DATASET_ROOT\",\"OUT_ROOT\",\"CKPT_ROOT\",\"FAST_DATA\"]:\n",
    "    print(f\"{k} = {os.environ[k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ 8) Import smoke test (uses your package + config.py)\n",
    "import sys, os\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "sys.path.append(os.path.join(PROJECT_ROOT, \"src\"))  # <‚Äî lets Python find src/ddriver\n",
    "\n",
    "try:\n",
    "    import ddriver\n",
    "    print(\"ddriver imported OK from:\", ddriver.__file__)\n",
    "    # Confirm config picks up Colab env:\n",
    "    try:\n",
    "        from ddriver import config\n",
    "        print(\"Loaded ddriver.config successfully.\")\n",
    "        # Echo the resolved paths from config (they are pathlib.Path objects)\n",
    "        print(\"config.DATASET_ROOT =\", config.DATASET_ROOT)\n",
    "        print(\"config.OUT_ROOT     =\", config.OUT_ROOT)\n",
    "        print(\"config.CKPT_ROOT    =\", config.CKPT_ROOT)\n",
    "        print(\"config.FAST_DATA    =\", config.FAST_DATA)\n",
    "    except Exception as e:\n",
    "        print(\"Note: ddriver.config not imported:\", e)\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Import failed ‚Äî check package name/setup.\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374139f5",
   "metadata": {},
   "source": [
    "# üìã 9) Generate Manifest and Split CSVs\n",
    "\n",
    "This step creates the CSV files that tell your code where all the images are and which ones go to train/val/test.\n",
    "\n",
    "**What this does:**\n",
    "- Scans all your images in the dataset folder\n",
    "- Creates a big list (manifest.csv) with info about every image\n",
    "- Creates three smaller lists (train.csv, val.csv, test.csv) that say which images belong where\n",
    "- Saves everything to your Google Drive so it's permanent\n",
    "\n",
    "**Why we need this:**\n",
    "- Your training code needs to know which images to use\n",
    "- The manifest remembers which driver each image belongs to (for VAL split)\n",
    "- The split CSVs organize images into train/val/test groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8883386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the manifest generator\n",
    "# This is like asking a librarian to catalog all your books and create reading lists\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Make sure we can import ddriver\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Run the manifest script\n",
    "# --write-split-lists means \"also create train.csv, val.csv, test.csv files\"\n",
    "manifest_cmd = f\"cd {PROJECT_ROOT} && python -m ddriver.data.manifest --write-split-lists\"\n",
    "\n",
    "print(\"üî® Generating manifest and split CSVs...\")\n",
    "print(f\"Running: {manifest_cmd}\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    manifest_cmd,\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Show what happened\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Warnings/Errors:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n‚úÖ Manifest and split CSVs generated successfully!\")\n",
    "    print(f\"   Manifest: {os.environ['OUT_ROOT']}/manifests/manifest.csv\")\n",
    "    print(f\"   Train split: {os.environ['OUT_ROOT']}/splits/train.csv\")\n",
    "    print(f\"   Val split: {os.environ['OUT_ROOT']}/splits/val.csv\")\n",
    "    print(f\"   Test split: {os.environ['OUT_ROOT']}/splits/test.csv\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Error generating manifest (exit code {result.returncode})\")\n",
    "    raise RuntimeError(\"Manifest generation failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: Did the CSVs get created?\n",
    "# This is like checking that the librarian actually wrote down all the book lists\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "manifest_path = Path(os.environ['OUT_ROOT']) / \"manifests\" / \"manifest.csv\"\n",
    "train_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"train.csv\"\n",
    "val_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"val.csv\"\n",
    "test_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"test.csv\"\n",
    "\n",
    "print(\"üìä Checking CSV files...\\n\")\n",
    "\n",
    "for name, path in [(\"Manifest\", manifest_path), (\"Train\", train_path), (\"Val\", val_path), (\"Test\", test_path)]:\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"‚úÖ {name}: {len(df)} rows, columns: {list(df.columns)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name}: File not found at {path}\")\n",
    "\n",
    "# Show a sample from the manifest\n",
    "if manifest_path.exists():\n",
    "    print(\"\\nüìÑ Sample from manifest (first 3 rows):\")\n",
    "    sample = pd.read_csv(manifest_path).head(3)\n",
    "    print(sample[['path', 'class_id', 'driver_id', 'camera', 'split']].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc2b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny balanced subset for quick testing\n",
    "# Run this cell ONCE to create train_small.csv, then use it for fast experiments\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from ddriver import config\n",
    "\n",
    "train_csv = Path(config.OUT_ROOT) / \"splits\" / \"train.csv\"\n",
    "train_small_csv = Path(config.OUT_ROOT) / \"splits\" / \"train_small.csv\"\n",
    "\n",
    "print(f\"Reading {train_csv}...\")\n",
    "df = pd.read_csv(train_csv)\n",
    "\n",
    "# Get 20 images per class (balanced)\n",
    "small = df.groupby(\"class_id\").head(20)\n",
    "\n",
    "print(f\"Original train.csv: {len(df)} images\")\n",
    "print(f\"Small subset: {len(small)} images ({len(small) // 10} per class)\")\n",
    "print(f\"\\nClass distribution in small subset:\")\n",
    "print(small[\"class_id\"].value_counts().sort_index())\n",
    "\n",
    "small.to_csv(train_small_csv, index=False)\n",
    "print(f\"\\n‚úÖ Saved to {train_small_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c222e9",
   "metadata": {},
   "source": [
    "### ‚ö°Ô∏è Tiny-train option\n",
    "\n",
    "Set `USE_TINY_SPLIT = True` in the training cell below to replace the heavy\n",
    "`train.csv` with the quick `train_small.csv` (20 images per class). Validation\n",
    "and test splits stay full so you still see realistic metrics.\n",
    "\n",
    "Run the \"Create a tiny balanced subset\" cell once per Drive setup before\n",
    "enabling this flag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041552b4",
   "metadata": {},
   "source": [
    "# üß™ 10) Test dataset.py and datamod.py\n",
    "\n",
    "Now let's make sure the code that loads images actually works!\n",
    "\n",
    "**What we're testing:**\n",
    "1. **dataset.py** - Can it load a single image and give us the right info?\n",
    "2. **datamod.py** - Can it create data loaders that give us batches of images?\n",
    "\n",
    "**Why test this:**\n",
    "- If these don't work, training will fail\n",
    "- Better to catch problems now than later\n",
    "- We want to see that images load correctly and labels are right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501a4c7",
   "metadata": {},
   "source": [
    "## üîç MediaPipe Crop Quality Audit\n",
    "\n",
    "**How to run a FAST audit (recommended):**\n",
    "1. Run the \"Copy crops to /content\" cell 32 below (copies from Drive to fast local SSD)\n",
    "2. Run the audit cell (it auto-detects the local copy and uses it)\n",
    "\n",
    "**Two modes:**\n",
    "- **Full mode**: Uses `detection_metadata_{variant}.csv` (has face/hand detection info)\n",
    "- **Lite mode**: Uses `manifest_{variant}.csv` (infers fallback from crop dimensions)\n",
    "\n",
    "**What you get:**\n",
    "- Numeric stats: fallback rates, ROI area/aspect distributions\n",
    "- Breakdowns by class/camera/split\n",
    "- Visual grids: \"worst suspects\" (tiny crops, fallbacks, extreme aspects)\n",
    "- Per-class sample grids\n",
    "\n",
    "**Path conventions:** All CSVs store relative paths. At runtime, paths are resolved using `config.OUT_ROOT` or `config.FAST_DATA` depending on where the crops are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Run MediaPipe Crop Quality Audit\n",
    "# Auto-detects whether crops are in /content (fast) or Drive, and which mode to use.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ddriver import config\n",
    "from ddriver.data.mediapipe_audit import generate_audit_report, get_crop_root\n",
    "\n",
    "VARIANT = \"face_hands\"  # must match the variant you extracted\n",
    "\n",
    "# Auto-detect crop root (prefers FAST_DATA if available)\n",
    "crop_root = get_crop_root(prefer_fast=True)\n",
    "print(f\"üìÅ Using crop root: {crop_root}\")\n",
    "\n",
    "# Look for metadata/manifest CSVs in the same location\n",
    "metadata_csv = crop_root.parent / f\"detection_metadata_{VARIANT}.csv\"\n",
    "manifest_csv = crop_root.parent / f\"manifest_{VARIANT}.csv\"\n",
    "\n",
    "# Fall back to OUT_ROOT if not found in FAST_DATA\n",
    "if not metadata_csv.exists() and not manifest_csv.exists():\n",
    "    metadata_csv = config.OUT_ROOT / \"mediapipe\" / f\"detection_metadata_{VARIANT}.csv\"\n",
    "    manifest_csv = config.OUT_ROOT / \"mediapipe\" / f\"manifest_{VARIANT}.csv\"\n",
    "\n",
    "# Output directory (always on Drive for persistence)\n",
    "audit_output = config.OUT_ROOT / \"mediapipe\" / \"audit\" / VARIANT\n",
    "\n",
    "# Run the audit\n",
    "if metadata_csv.exists():\n",
    "    print(f\"‚úÖ Found detection metadata: {metadata_csv}\")\n",
    "    audit_result = generate_audit_report(\n",
    "        metadata_csv=metadata_csv,\n",
    "        crop_root=crop_root,\n",
    "        output_dir=audit_output,\n",
    "        variant=VARIANT,\n",
    "        n_samples=25,\n",
    "        save_figures=True,\n",
    "        show_figures=True,\n",
    "    )\n",
    "elif manifest_csv.exists():\n",
    "    print(f\"‚ö†Ô∏è Using manifest (lite mode): {manifest_csv}\")\n",
    "    audit_result = generate_audit_report(\n",
    "        manifest_csv=manifest_csv,\n",
    "        crop_root=crop_root,\n",
    "        output_dir=audit_output,\n",
    "        variant=VARIANT,\n",
    "        n_samples=25,\n",
    "        save_figures=True,\n",
    "        show_figures=True,\n",
    "    )\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Neither metadata nor manifest found. Run extraction first.\\n\"\n",
    "        f\"  Checked: {metadata_csv}\\n\"\n",
    "        f\"  Checked: {manifest_csv}\"\n",
    "    )\n",
    "\n",
    "# Store results for later cells\n",
    "stats = audit_result[\"stats\"]\n",
    "breakdowns = audit_result[\"breakdowns\"]\n",
    "lite_mode = audit_result[\"lite_mode\"]\n",
    "crop_root = audit_result[\"crop_root\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Audit complete! Outputs saved to: {audit_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec6bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Display Audit Summary Stats (uses results from previous cell)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä DETECTION SUMMARY STATS\")\n",
    "if lite_mode:\n",
    "    print(\"   [LITE MODE - face/hand detection info not available]\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total images processed: {stats['total_images']}\")\n",
    "\n",
    "# Face/hand detection (full mode only)\n",
    "if not lite_mode and \"face_detected_pct\" in stats:\n",
    "    print(f\"\\nüéØ Detection rates:\")\n",
    "    print(f\"   Face detected: {stats['face_detected_pct']:.1f}%\")\n",
    "    print(f\"   Hands: 0={stats['hands_0_pct']:.1f}%, 1={stats['hands_1_pct']:.1f}%, 2={stats['hands_2_pct']:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Fallback to full frame: {stats['fallback_count']} ({stats['fallback_pct']:.1f}%)\")\n",
    "print(f\"   Fallback reasons: {stats['fallback_reasons']}\")\n",
    "\n",
    "print(f\"\\nüìê ROI statistics:\")\n",
    "print(f\"   Area fraction: mean={stats['roi_area_frac_mean']:.3f}, std={stats['roi_area_frac_std']:.3f}\")\n",
    "print(f\"   Area percentiles: 5%={stats['roi_area_frac_p5']:.3f}, 25%={stats['roi_area_frac_p25']:.3f}, 50%={stats['roi_area_frac_median']:.3f}\")\n",
    "print(f\"   Aspect ratio: mean={stats['roi_aspect_mean']:.3f}, min={stats['roi_aspect_min']:.3f}, max={stats['roi_aspect_max']:.3f}\")\n",
    "\n",
    "# Detection types (full mode only)\n",
    "if not lite_mode and \"detection_used_distribution\" in stats:\n",
    "    print(f\"\\nüè∑Ô∏è  Detection types used:\")\n",
    "    for dtype, count in stats['detection_used_distribution'].items():\n",
    "        pct = 100 * count / stats['total_images']\n",
    "        print(f\"   {dtype}: {count} ({pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e75359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Display Breakdown by Class and Camera (uses results from audit cell)\n",
    "import pandas as pd\n",
    "\n",
    "# Class breakdown\n",
    "if \"class_id\" in breakdowns:\n",
    "    print(\"üìã BREAKDOWN BY CLASS\")\n",
    "    print(\"-\" * 100)\n",
    "    print(breakdowns[\"class_id\"].to_string(index=False))\n",
    "    print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Class breakdown not available\")\n",
    "\n",
    "# Camera breakdown\n",
    "if \"camera\" in breakdowns:\n",
    "    print(\"üìã BREAKDOWN BY CAMERA\")\n",
    "    print(\"-\" * 100)\n",
    "    print(breakdowns[\"camera\"].to_string(index=False))\n",
    "    print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Camera breakdown not available\")\n",
    "\n",
    "# Split breakdown (train/val/test)\n",
    "if \"split\" in breakdowns:\n",
    "    print(\"üìã BREAKDOWN BY SPLIT\")\n",
    "    print(\"-\" * 100)\n",
    "    print(breakdowns[\"split\"].to_string(index=False))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Split breakdown not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55662c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñºÔ∏è Re-display saved grids from disk (if you need to see them again)\n",
    "# Note: Grids were already shown inline when you ran the audit cell above!\n",
    "\n",
    "from IPython.display import display, Image as IPImage\n",
    "from ddriver import config\n",
    "\n",
    "VARIANT = \"face_hands\"\n",
    "audit_output = config.OUT_ROOT / \"mediapipe\" / \"audit\" / VARIANT\n",
    "\n",
    "print(\"üìÅ Saved grids location:\", audit_output)\n",
    "print(\"   (These were already displayed inline during the audit)\\n\")\n",
    "\n",
    "# List what's available\n",
    "grids = [\n",
    "    (\"grid_area_small.png\", \"üî¨ Smallest ROI crops\"),\n",
    "    (\"grid_fallback.png\", \"‚ö†Ô∏è Fallback to full frame\"),\n",
    "    (\"grid_aspect_extreme.png\", \"üìê Extreme aspect ratios\"),\n",
    "]\n",
    "# Add full-mode only grids\n",
    "if not lite_mode:\n",
    "    grids.extend([\n",
    "        (\"grid_no_hands.png\", \"üë§ Face detected but no hands\"),\n",
    "        (\"grid_one_hand.png\", \"‚úã Only one hand detected\"),\n",
    "    ])\n",
    "\n",
    "for filename, title in grids:\n",
    "    grid_path = audit_output / filename\n",
    "    if grid_path.exists():\n",
    "        print(f\"‚úÖ {title}: {grid_path.name}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {title}: not found\")\n",
    "\n",
    "# Uncomment below to re-display a specific grid:\n",
    "# display(IPImage(filename=str(audit_output / \"grid_area_small.png\"), width=900))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec581c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üè∑Ô∏è Per-Class Sample Grids info\n",
    "# Note: These were already displayed inline during the audit!\n",
    "\n",
    "from IPython.display import display, Image as IPImage\n",
    "from ddriver import config\n",
    "\n",
    "VARIANT = \"face_hands\"\n",
    "audit_output = config.OUT_ROOT / \"mediapipe\" / \"audit\" / VARIANT\n",
    "\n",
    "# Class labels for reference\n",
    "CLASS_LABELS = {\n",
    "    0: \"c0 - Safe driving\",\n",
    "    1: \"c1 - Texting (right)\",\n",
    "    2: \"c2 - Phone (right)\",\n",
    "    3: \"c3 - Texting (left)\",\n",
    "    4: \"c4 - Phone (left)\",\n",
    "    5: \"c5 - Radio\",\n",
    "    6: \"c6 - Drinking\",\n",
    "    7: \"c7 - Reaching behind\",\n",
    "    8: \"c8 - Hair/makeup\",\n",
    "    9: \"c9 - Talking to passenger\",\n",
    "}\n",
    "\n",
    "print(\"üìÅ Per-class grids saved at:\", audit_output)\n",
    "print(\"   (Already displayed inline during audit)\\n\")\n",
    "\n",
    "# Check what's available\n",
    "for class_id in range(10):\n",
    "    grid_path = audit_output / f\"grid_class_{class_id}.png\"\n",
    "    label = CLASS_LABELS.get(class_id, f\"Class {class_id}\")\n",
    "    if grid_path.exists():\n",
    "        print(f\"‚úÖ {label}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {label}: not found\")\n",
    "\n",
    "# Uncomment to re-display specific class grids:\n",
    "# for class_id in [0, 1, 7]:  # Adjust class IDs as needed\n",
    "#     grid_path = audit_output / f\"grid_class_{class_id}.png\"\n",
    "#     if grid_path.exists():\n",
    "#         print(f\"\\nüè∑Ô∏è {CLASS_LABELS.get(class_id, f'Class {class_id}')}\")\n",
    "#         display(IPImage(filename=str(grid_path), width=900))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6044f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Can dataset.py load a single image?\n",
    "# This is like testing if a worker can fetch one book from the library\n",
    "\n",
    "from ddriver.data.dataset import AucDriverDataset\n",
    "from torchvision import transforms as T\n",
    "from pathlib import Path\n",
    "\n",
    "# Get paths from config\n",
    "manifest_csv = Path(os.environ['OUT_ROOT']) / \"manifests\" / \"manifest.csv\"\n",
    "val_split_csv = Path(os.environ['OUT_ROOT']) / \"splits\" / \"val.csv\"\n",
    "\n",
    "print(\"üß™ Test 1: Testing AucDriverDataset (dataset.py)\")\n",
    "print(f\"   Manifest: {manifest_csv}\")\n",
    "print(f\"   Using Val split: {val_split_csv}\\n\")\n",
    "\n",
    "try:\n",
    "    # Create a simple dataset (no fancy transforms, just load the image)\n",
    "    simple_transforms = T.ToTensor()  # Just convert to tensor, no augmentation\n",
    "    \n",
    "    val_dataset = AucDriverDataset(\n",
    "        manifest_csv=manifest_csv,\n",
    "        split_csv=val_split_csv,\n",
    "        transforms=simple_transforms\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset created! It has {len(val_dataset)} images in VAL split\")\n",
    "    \n",
    "    # Try to load the first image\n",
    "    print(\"\\nüìñ Loading first image from VAL split...\")\n",
    "    sample = val_dataset[0]\n",
    "    \n",
    "    print(f\"‚úÖ Image loaded successfully!\")\n",
    "    print(f\"   Image shape: {sample['image'].shape} (should be [3, height, width])\")\n",
    "    print(f\"   Label: {sample['label']} (should be 0-9)\")\n",
    "    print(f\"   Driver ID: {sample['driver_id']} (VAL should have driver IDs)\")\n",
    "    print(f\"   Camera: {sample['camera']} (should be 'cam1' or 'cam2')\")\n",
    "    print(f\"   Path: {sample['path'][:80]}...\")  # Show first 80 chars\n",
    "    \n",
    "    # Check that label is valid (0-9)\n",
    "    if 0 <= sample['label'] <= 9:\n",
    "        print(f\"   ‚úÖ Label is valid (0-9)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Label {sample['label']} is NOT in range 0-9!\")\n",
    "    \n",
    "    # Check that VAL has driver IDs\n",
    "    if sample['driver_id'] is not None:\n",
    "        print(f\"   ‚úÖ VAL split has driver ID (as expected)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  VAL split missing driver ID (might be okay if this image wasn't in your DRIVER_RANGES)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Test 1 PASSED: dataset.py works!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 1 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286fdbe",
   "metadata": {},
   "source": [
    "# üßµ 11) Full pipeline (train ‚Üí predict ‚Üí metrics)\n",
    "\n",
    "Now that data loading is working, these next cells show how to:\n",
    "1. Register the model you want (e.g., `resnet18` from timm)\n",
    "2. Run training from the command line helper\n",
    "3. Generate predictions from a checkpoint\n",
    "4. Evaluate metrics and save all logs to Drive\n",
    "\n",
    "> You can change the `RUN_TAG`, model name, epochs, etc. in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register models you want to use (run once per runtime)\n",
    "# This example uses timm's convnext_tiny.\n",
    "\n",
    "!pip -q install timm\n",
    "\n",
    "from ddriver.models import registry\n",
    "\n",
    "registry.register_timm_backbone(\"efficientnet_b0\")\n",
    "print(\"Available models:\", registry.available_models()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"mediapipe==0.10.14\" \"protobuf<5\" \"opencv-python-headless<4.11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß≠ Generate MediaPipe ROI crops (face, hands, face+hands)\n",
    "# Run once per runtime/variant. Produces new manifest/splits under OUT_ROOT/mediapipe.\n",
    "!pip -q install mediapipe opencv-python-headless\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "VARIANT = \"face_hands\"  # choose: face | hands | face_hands\n",
    "\n",
    "# ===== OUTPUT LOCATION (FAST vs PERSISTENT) =====\n",
    "# Option 1: Write to /content (FAST - recommended for extraction)\n",
    "OUTPUT_ROOT = Path(\"/content/data/mediapipe\")\n",
    "# Option 2: Write directly to Drive (SLOW but persistent)\n",
    "# OUTPUT_ROOT = Path(OUT_ROOT) / \"mediapipe\"\n",
    "\n",
    "manifest_csv = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "splits_root = Path(OUT_ROOT) / \"splits\"\n",
    "\n",
    "print(f\"üìÅ Extraction will write to: {OUTPUT_ROOT}\")\n",
    "if \"/content/data\" in str(OUTPUT_ROOT):\n",
    "    print(\"   ‚ö° Using fast local SSD - remember to copy back to Drive when done!\")\n",
    "else:\n",
    "    print(\"   üíæ Writing directly to Drive (slower but persistent)\")\n",
    "\n",
    "extract_cmd = f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.data.mediapipe_extract \\\n",
    "  --manifest {manifest_csv} \\\n",
    "  --splits-root {splits_root} \\\n",
    "  --dataset-root {DATASET_ROOT} \\\n",
    "  --output-root {OUTPUT_ROOT} \\\n",
    "  --variant {VARIANT} \\\n",
    "  --max-side 720 \\\n",
    "  --model-complexity 2 \\\n",
    "  --min-detection-area-frac 0.05 \\\n",
    "  --min-area-frac 0.10 \\\n",
    "  --min-aspect 0.20 \\\n",
    "  --pad-frac 0.20 \\\n",
    "  --face-extra-down-frac 0.35 \\\n",
    "  --overwrite\n",
    "\"\"\"\n",
    "\n",
    "print(\"Running MediaPipe extraction for variant:\", VARIANT)\n",
    "print(extract_cmd)\n",
    "proc = subprocess.Popen(\n",
    "    extract_cmd,\n",
    "    shell=True,\n",
    "    text=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "if proc.stdout is None:\n",
    "    raise RuntimeError(\"Extraction process has no stdout pipe.\")\n",
    "for line in proc.stdout:\n",
    "    print(line, end=\"\")\n",
    "proc.wait()\n",
    "if proc.returncode != 0:\n",
    "    raise RuntimeError(\"MediaPipe extraction failed. Check logs above.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76811eb",
   "metadata": {},
   "source": [
    "## üéØ YOLO-World ROI Extraction (Alternative to MediaPipe)\n",
    "\n",
    "YOLO-World uses open-vocabulary detection to find faces and hands without custom training.\n",
    "This is an alternative to the MediaPipe pipeline above.\n",
    "\n",
    "**Advantages over MediaPipe:**\n",
    "- Better detection accuracy for occluded/partial views\n",
    "- Confidence scores for filtering\n",
    "- Faster inference on GPU\n",
    "\n",
    "**Choose ONE pipeline:** Either run MediaPipe extraction OR YOLO extraction, not both.\n",
    "The training cell below lets you pick which pipeline's crops to use (`USE_MEDIAPIPE` vs `USE_YOLO`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Generate YOLO-World ROI crops (face, hands, face+hands)\n",
    "# Alternative to MediaPipe - uses open-vocabulary detection.\n",
    "# Run once per runtime/variant. Produces new manifest/splits under OUT_ROOT/yolo.\n",
    "\n",
    "!pip -q install ultralytics\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "VARIANT = \"face_hands\"  # choose: face | hands | face_hands\n",
    "\n",
    "# ===== OUTPUT LOCATION (FAST vs PERSISTENT) =====\n",
    "# Option 1: Write to /content (FAST - recommended for extraction)\n",
    "YOLO_OUTPUT_ROOT = Path(\"/content/data/yolo\")\n",
    "# Option 2: Write directly to Drive (SLOW but persistent)\n",
    "# YOLO_OUTPUT_ROOT = Path(OUT_ROOT) / \"yolo\"\n",
    "\n",
    "manifest_csv = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "splits_root = Path(OUT_ROOT) / \"splits\"\n",
    "\n",
    "print(f\"üìÅ Extraction will write to: {YOLO_OUTPUT_ROOT}\")\n",
    "if \"/content/data\" in str(YOLO_OUTPUT_ROOT):\n",
    "    print(\"   ‚ö° Using fast local SSD - remember to copy back to Drive when done!\")\n",
    "else:\n",
    "    print(\"   üíæ Writing directly to Drive (slower but persistent)\")\n",
    "\n",
    "# ===== TEST MODE OPTIONS (toggle these!) =====\n",
    "# Option 1: Use train_small.csv for quick testing (~200 images)\n",
    "TEST_MODE = True  # Set False for full extraction\n",
    "SAMPLE_CSV = Path(OUT_ROOT) / \"splits\" / \"train_small.csv\"  # Small balanced subset\n",
    "\n",
    "# Option 2: Limit to first N images (even faster for debugging)\n",
    "LIMIT = None  # Set to e.g. 50 for super quick test, None for no limit\n",
    "\n",
    "# Build command\n",
    "sample_flag = f\"--sample-csv {SAMPLE_CSV}\" if TEST_MODE and SAMPLE_CSV.exists() else \"\"\n",
    "limit_flag = f\"--limit {LIMIT}\" if LIMIT else \"\"\n",
    "\n",
    "extract_cmd = f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.data.yolo_extract \\\n",
    "  --manifest {manifest_csv} \\\n",
    "  --splits-root {splits_root} \\\n",
    "  --dataset-root {DATASET_ROOT} \\\n",
    "  --output-root {YOLO_OUTPUT_ROOT} \\\n",
    "  --variant {VARIANT} \\\n",
    "  --model-size m \\\n",
    "  --confidence 0.15 \\\n",
    "  --min-detection-area-frac 0.03 \\\n",
    "  --min-area-frac 0.08 \\\n",
    "  --min-aspect 0.20 \\\n",
    "  --pad-frac 0.20 \\\n",
    "  {sample_flag} \\\n",
    "  {limit_flag} \\\n",
    "  --overwrite\n",
    "\"\"\"\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\"‚ö° TEST MODE: Using small sample for quick testing\")\n",
    "    print(f\"   Sample CSV: {SAMPLE_CSV}\")\n",
    "    if LIMIT:\n",
    "        print(f\"   Limit: {LIMIT} images\")\n",
    "else:\n",
    "    print(\"ü™µ FULL MODE: Processing all images\")\n",
    "\n",
    "print(f\"\\nRunning YOLO-World extraction for variant: {VARIANT}\")\n",
    "print(extract_cmd)\n",
    "proc = subprocess.Popen(\n",
    "    extract_cmd,\n",
    "    shell=True,\n",
    "    text=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "if proc.stdout is None:\n",
    "    raise RuntimeError(\"Extraction process has no stdout pipe.\")\n",
    "for line in proc.stdout:\n",
    "    print(line, end=\"\")\n",
    "proc.wait()\n",
    "if proc.returncode != 0:\n",
    "    raise RuntimeError(\"YOLO extraction failed. Check logs above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5c186",
   "metadata": {},
   "source": [
    "## üöö Copy YOLO Crops (Fast ‚Üî Drive)\n",
    "\n",
    "**Two modes:**\n",
    "1. **After extraction to /content** ‚Üí Copy TO Drive for persistence\n",
    "2. **If extracted to Drive** ‚Üí Copy FROM Drive to /content for faster training\n",
    "\n",
    "Use this if training/audit from Drive is slow. It copies the generated YOLO crops/CSVs into `/content/data/yolo/<variant>` and updates paths for the current runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöö Copy YOLO crops (Fast ‚Üî Drive)\n",
    "import os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your variant to the one you extracted already\n",
    "YOLO_VARIANT = \"face_hands\"  # face | hands | face_hands\n",
    "\n",
    "# Define both locations\n",
    "LOCAL_ROOT = Path(\"/content/data/yolo\")\n",
    "DRIVE_ROOT = Path(OUT_ROOT) / \"yolo\"\n",
    "\n",
    "LOCAL_VARIANT_DIR = LOCAL_ROOT / YOLO_VARIANT\n",
    "DRIVE_VARIANT_DIR = DRIVE_ROOT / YOLO_VARIANT\n",
    "\n",
    "# ===== AUTO-DETECT COPY DIRECTION =====\n",
    "local_exists = LOCAL_VARIANT_DIR.exists() and any(LOCAL_VARIANT_DIR.glob(\"**/*.jpg\"))\n",
    "drive_exists = DRIVE_VARIANT_DIR.exists() and any(DRIVE_VARIANT_DIR.glob(\"**/*.jpg\"))\n",
    "\n",
    "if local_exists and not drive_exists:\n",
    "    COPY_MODE = \"TO_DRIVE\"\n",
    "    SRC_ROOT = LOCAL_ROOT\n",
    "    SRC_VARIANT_DIR = LOCAL_VARIANT_DIR\n",
    "    DST_ROOT = DRIVE_VARIANT_DIR\n",
    "    print(\"üì¶ Mode: Copy FROM /content TO Drive (for persistence)\")\n",
    "elif drive_exists and not local_exists:\n",
    "    COPY_MODE = \"TO_LOCAL\"\n",
    "    SRC_ROOT = DRIVE_ROOT\n",
    "    SRC_VARIANT_DIR = DRIVE_VARIANT_DIR\n",
    "    DST_ROOT = LOCAL_VARIANT_DIR\n",
    "    print(\"üì¶ Mode: Copy FROM Drive TO /content (for faster training)\")\n",
    "elif local_exists and drive_exists:\n",
    "    print(\"‚ö†Ô∏è  Crops exist in BOTH locations!\")\n",
    "    COPY_MODE = \"TO_DRIVE\"\n",
    "    SRC_ROOT = LOCAL_ROOT\n",
    "    SRC_VARIANT_DIR = LOCAL_VARIANT_DIR\n",
    "    DST_ROOT = DRIVE_VARIANT_DIR\n",
    "    print(\"   Defaulting to: Copy TO Drive for persistence\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No crops found in either location!\\n\"\n",
    "        f\"   /content: {LOCAL_VARIANT_DIR}\\n\"\n",
    "        f\"   Drive:    {DRIVE_VARIANT_DIR}\\n\"\n",
    "        f\"Run the YOLO extraction cell first.\"\n",
    "    )\n",
    "\n",
    "print(f\"Copying YOLO crops from {SRC_VARIANT_DIR} -> {DST_ROOT}\")\n",
    "DST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy images directory tree\n",
    "file_count = 0\n",
    "for src_dir, _, files in os.walk(SRC_VARIANT_DIR):\n",
    "    rel_dir = Path(src_dir).relative_to(SRC_VARIANT_DIR)\n",
    "    dst_dir = DST_ROOT / rel_dir\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for fname in files:\n",
    "        src_path = Path(src_dir) / fname\n",
    "        dst_path = dst_dir / fname\n",
    "        if not dst_path.exists():\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            file_count += 1\n",
    "print(f\"   Copied {file_count} image files\")\n",
    "\n",
    "# Copy manifest/split CSVs that live one level above the variant folder\n",
    "csv_names = [\n",
    "    f\"manifest_{YOLO_VARIANT}.csv\",\n",
    "    f\"train_{YOLO_VARIANT}.csv\",\n",
    "    f\"val_{YOLO_VARIANT}.csv\",\n",
    "    f\"test_{YOLO_VARIANT}.csv\",\n",
    "    f\"detection_metadata_{YOLO_VARIANT}.csv\",  # for auditing\n",
    "]\n",
    "\n",
    "# Determine CSV destination\n",
    "dst_csv_root = DST_ROOT.parent if COPY_MODE == \"TO_LOCAL\" else DRIVE_ROOT\n",
    "\n",
    "for fname in csv_names:\n",
    "    src_csv = SRC_ROOT / fname\n",
    "    if not src_csv.exists():\n",
    "        print(f\"   ‚ö†Ô∏è Skipping {fname} (not found)\")\n",
    "        continue\n",
    "    dst_csv = dst_csv_root / fname\n",
    "    shutil.copy2(src_csv, dst_csv)\n",
    "    print(f\"   ‚úÖ Copied {fname}\")\n",
    "\n",
    "# ===== SET ENVIRONMENT VARIABLES =====\n",
    "# Always point to /content for fast training\n",
    "os.environ[\"YOLO_ROOT_LOCAL\"] = str(LOCAL_ROOT)\n",
    "os.environ[\"DATASET_ROOT\"] = str(LOCAL_ROOT)\n",
    "import importlib\n",
    "from ddriver import config as _cfg\n",
    "importlib.reload(_cfg)\n",
    "\n",
    "print(f\"\\n‚úÖ Copy complete!\")\n",
    "if COPY_MODE == \"TO_DRIVE\":\n",
    "    print(f\"   üíæ Crops persisted to Drive: {DRIVE_ROOT}\")\n",
    "    print(f\"   ‚ö° Training will use fast local copy: {LOCAL_ROOT}\")\n",
    "else:\n",
    "    print(f\"   ‚ö° Crops copied to fast local SSD: {LOCAL_ROOT}\")\n",
    "    \n",
    "print(f\"\\n   YOLO_ROOT_LOCAL = {os.environ['YOLO_ROOT_LOCAL']}\")\n",
    "print(f\"   DATASET_ROOT    = {os.environ['DATASET_ROOT']} (images loaded from here)\")\n",
    "print(f\"\\n   Set USE_YOLO=True in training cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223023bd",
   "metadata": {},
   "source": [
    "## üîç YOLO Crop Quality Audit\n",
    "\n",
    "Quick stats and visual inspection of YOLO-World crops. Uses `detection_metadata_{variant}.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç YOLO Crop Quality Audit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ddriver import config\n",
    "\n",
    "VARIANT = \"face_hands\"  # must match the variant you extracted\n",
    "\n",
    "# Auto-detect: prefer local copy if available\n",
    "yolo_root_local = Path(os.environ.get(\"YOLO_ROOT_LOCAL\", \"\"))\n",
    "if yolo_root_local.exists():\n",
    "    yolo_root = yolo_root_local\n",
    "else:\n",
    "    yolo_root = Path(OUT_ROOT) / \"yolo\"\n",
    "\n",
    "metadata_csv = yolo_root / f\"detection_metadata_{VARIANT}.csv\"\n",
    "if not metadata_csv.exists():\n",
    "    raise FileNotFoundError(f\"Detection metadata not found: {metadata_csv}\\nRun YOLO extraction first.\")\n",
    "\n",
    "print(f\"üìÅ Loading metadata from: {metadata_csv}\")\n",
    "df = pd.read_csv(metadata_csv)\n",
    "\n",
    "# Summary stats\n",
    "n_total = len(df)\n",
    "n_fallback = df[\"fallback_to_full\"].sum()\n",
    "n_face = (df[\"face_count\"] > 0).sum()\n",
    "n_hands = (df[\"hand_count\"] > 0).sum()\n",
    "n_face_and_hands = ((df[\"face_count\"] > 0) & (df[\"hand_count\"] > 0)).sum()\n",
    "avg_face_conf = df.loc[df[\"face_confidence\"] > 0, \"face_confidence\"].mean()\n",
    "avg_hand_conf = df.loc[df[\"hand_confidence\"] > 0, \"hand_confidence\"].mean()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä YOLO DETECTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total images: {n_total}\")\n",
    "print(f\"\\nüéØ Detection rates:\")\n",
    "print(f\"   Face detected: {n_face} ({100*n_face/n_total:.1f}%)\")\n",
    "print(f\"   Hands detected: {n_hands} ({100*n_hands/n_total:.1f}%)\")\n",
    "print(f\"   Both face+hands: {n_face_and_hands} ({100*n_face_and_hands/n_total:.1f}%)\")\n",
    "print(f\"\\nüìà Confidence scores:\")\n",
    "print(f\"   Avg face confidence: {avg_face_conf:.3f}\" if not np.isnan(avg_face_conf) else \"   Avg face confidence: N/A\")\n",
    "print(f\"   Avg hand confidence: {avg_hand_conf:.3f}\" if not np.isnan(avg_hand_conf) else \"   Avg hand confidence: N/A\")\n",
    "print(f\"\\n‚ö†Ô∏è  Fallback to full frame: {n_fallback} ({100*n_fallback/n_total:.1f}%)\")\n",
    "\n",
    "# Fallback reason breakdown\n",
    "fallback_df = df[df[\"fallback_to_full\"]]\n",
    "if len(fallback_df) > 0:\n",
    "    print(f\"   Fallback reasons:\")\n",
    "    for reason, count in fallback_df[\"fallback_reason\"].value_counts().items():\n",
    "        print(f\"      - {reason}: {count} ({100*count/n_total:.1f}%)\")\n",
    "\n",
    "# ROI stats (for non-fallback images)\n",
    "non_fallback = df[~df[\"fallback_to_full\"]]\n",
    "if len(non_fallback) > 0:\n",
    "    print(f\"\\nüìê ROI statistics (non-fallback only, n={len(non_fallback)}):\")\n",
    "    print(f\"   Raw detection area: mean={non_fallback['raw_detection_area_frac'].mean():.3f}, \"\n",
    "          f\"std={non_fallback['raw_detection_area_frac'].std():.3f}\")\n",
    "    print(f\"   Final ROI area: mean={non_fallback['roi_area_frac'].mean():.3f}, \"\n",
    "          f\"std={non_fallback['roi_area_frac'].std():.3f}\")\n",
    "    print(f\"   Aspect ratio: mean={non_fallback['roi_aspect'].mean():.3f}, \"\n",
    "          f\"min={non_fallback['roi_aspect'].min():.3f}, max={non_fallback['roi_aspect'].max():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3602de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã YOLO Breakdown by Camera and Class\n",
    "print(\"\\nüìã BREAKDOWN BY CAMERA\")\n",
    "print(\"-\" * 80)\n",
    "camera_stats = df.groupby(\"camera\").agg({\n",
    "    \"fallback_to_full\": [\"sum\", \"mean\"],\n",
    "    \"roi_area_frac\": \"mean\",\n",
    "    \"face_count\": lambda x: (x > 0).mean(),\n",
    "    \"hand_count\": lambda x: (x > 0).mean(),\n",
    "}).round(3)\n",
    "camera_stats.columns = [\"fallback_count\", \"fallback_pct\", \"mean_roi_area\", \"face_rate\", \"hand_rate\"]\n",
    "camera_stats[\"fallback_pct\"] = (camera_stats[\"fallback_pct\"] * 100).round(1)\n",
    "camera_stats[\"face_rate\"] = (camera_stats[\"face_rate\"] * 100).round(1)\n",
    "camera_stats[\"hand_rate\"] = (camera_stats[\"hand_rate\"] * 100).round(1)\n",
    "print(camera_stats.to_string())\n",
    "\n",
    "print(\"\\nüìã BREAKDOWN BY CLASS\")\n",
    "print(\"-\" * 80)\n",
    "class_stats = df.groupby(\"class_id\").agg({\n",
    "    \"fallback_to_full\": [\"sum\", \"mean\"],\n",
    "    \"roi_area_frac\": \"mean\",\n",
    "    \"face_count\": lambda x: (x > 0).mean(),\n",
    "    \"hand_count\": lambda x: (x > 0).mean(),\n",
    "}).round(3)\n",
    "class_stats.columns = [\"fallback_count\", \"fallback_pct\", \"mean_roi_area\", \"face_rate\", \"hand_rate\"]\n",
    "class_stats[\"fallback_pct\"] = (class_stats[\"fallback_pct\"] * 100).round(1)\n",
    "class_stats[\"face_rate\"] = (class_stats[\"face_rate\"] * 100).round(1)\n",
    "class_stats[\"hand_rate\"] = (class_stats[\"hand_rate\"] * 100).round(1)\n",
    "print(class_stats.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ef94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñºÔ∏è Visual Sample Grid - YOLO Crops\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def show_sample_grid(df_subset, title, crop_root, n_samples=12, n_cols=4):\n",
    "    \"\"\"Display a grid of sample crops from a DataFrame subset.\"\"\"\n",
    "    samples = df_subset.sample(n=min(n_samples, len(df_subset)), random_state=42)\n",
    "    n_rows = (len(samples) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]\n",
    "    \n",
    "    for ax, (_, row) in zip(axes, samples.iterrows()):\n",
    "        crop_path = crop_root / row[\"cropped_path\"]\n",
    "        if crop_path.exists():\n",
    "            img = cv2.imread(str(crop_path))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            ax.imshow(img)\n",
    "            label = f\"c{int(row['class_id'])} | area={row['roi_area_frac']:.2f}\"\n",
    "            if row[\"fallback_to_full\"]:\n",
    "                label += \" [FALLBACK]\"\n",
    "            ax.set_title(label, fontsize=9)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Not found\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for ax in axes[len(samples):]:\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show samples: fallbacks, smallest ROIs, by class\n",
    "crop_root = yolo_root  # Use the root determined in the audit cell\n",
    "\n",
    "# Fallback samples\n",
    "fallback_samples = df[df[\"fallback_to_full\"]]\n",
    "if len(fallback_samples) > 0:\n",
    "    show_sample_grid(fallback_samples, \"‚ö†Ô∏è Fallback to Full Frame Samples\", crop_root)\n",
    "\n",
    "# Smallest ROI samples (non-fallback)\n",
    "smallest = non_fallback.nsmallest(12, \"roi_area_frac\")\n",
    "if len(smallest) > 0:\n",
    "    show_sample_grid(smallest, \"üî¨ Smallest ROI Crops (non-fallback)\", crop_root)\n",
    "\n",
    "# Per-class samples (one random per class)\n",
    "print(\"\\nüè∑Ô∏è Sample crop per class:\")\n",
    "for class_id in sorted(df[\"class_id\"].unique()):\n",
    "    class_df = df[df[\"class_id\"] == class_id]\n",
    "    if len(class_df) > 0:\n",
    "        show_sample_grid(class_df, f\"Class {int(class_id)} Samples\", crop_root, n_samples=8, n_cols=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfab60d",
   "metadata": {},
   "source": [
    "## üîÄ Hybrid ROI Extraction (RetinaFace + MediaPipe Hands)\n",
    "\n",
    "**Best accuracy option!** Uses specialized models:\n",
    "- **RetinaFace**: State-of-the-art face detection (handles occlusion, angles)\n",
    "- **MediaPipe Hands**: Google's dedicated hand model (much better than Holistic)\n",
    "\n",
    "This typically gives the lowest fallback rate and best hand detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca39043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÄ Generate Hybrid ROI crops (InsightFace + MediaPipe Hands)\n",
    "# Best accuracy option - uses specialized models for face and hands.\n",
    "# InsightFace uses ONNX (no TensorFlow dependency!)\n",
    "!pip -q install insightface onnxruntime mediapipe\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "VARIANT = \"face_hands\"  # choose: face | hands | face_hands\n",
    "\n",
    "# ===== OUTPUT LOCATION (FAST vs PERSISTENT) =====\n",
    "# Option 1: Write to /content (FAST - recommended for extraction)\n",
    "HYBRID_OUTPUT_ROOT = Path(\"/content/data/hybrid\")\n",
    "# Option 2: Write directly to Drive (SLOW but persistent)\n",
    "# HYBRID_OUTPUT_ROOT = Path(OUT_ROOT) / \"hybrid\"\n",
    "\n",
    "manifest_csv = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "splits_root = Path(OUT_ROOT) / \"splits\"\n",
    "\n",
    "print(f\"üìÅ Extraction will write to: {HYBRID_OUTPUT_ROOT}\")\n",
    "if \"/content/data\" in str(HYBRID_OUTPUT_ROOT):\n",
    "    print(\"   ‚ö° Using fast local SSD - remember to copy back to Drive when done!\")\n",
    "else:\n",
    "    print(\"   üíæ Writing directly to Drive (slower but persistent)\")\n",
    "\n",
    "# ===== AUTO-DETECT LOCAL vs DRIVE IMAGES =====\n",
    "# If you ran the \"copy + compress dataset\" cells (46/47), images will be in /content/data\n",
    "# We auto-detect and use local images for faster I/O, falling back to Drive if not available\n",
    "LOCAL_DATASET_ROOT = Path(\"/content/data/auc.distracted.driver.dataset_v2\")\n",
    "DRIVE_DATASET_ROOT = Path(DATASET_ROOT)\n",
    "\n",
    "if LOCAL_DATASET_ROOT.exists() and any(LOCAL_DATASET_ROOT.iterdir()):\n",
    "    EFFECTIVE_DATASET_ROOT = LOCAL_DATASET_ROOT\n",
    "    print(f\"üöÄ FAST MODE: Using local images from {LOCAL_DATASET_ROOT}\")\n",
    "else:\n",
    "    EFFECTIVE_DATASET_ROOT = DRIVE_DATASET_ROOT\n",
    "    print(f\"üìÅ Using images from Drive: {DRIVE_DATASET_ROOT}\")\n",
    "    print(\"   üí° Tip: Run cells 46/47 first to copy images to /content for faster extraction!\")\n",
    "\n",
    "# ===== TEST MODE OPTIONS (toggle these!) =====\n",
    "# Option 1: Use train_small.csv for quick testing (~200 images)\n",
    "TEST_MODE = True  # Set False for full extraction\n",
    "SAMPLE_CSV = Path(OUT_ROOT) / \"splits\" / \"train_small.csv\"  # Small balanced subset\n",
    "\n",
    "# Option 2: Limit to first N images (even faster for debugging)\n",
    "LIMIT = None  # Set to e.g. 50 for super quick test, None for no limit\n",
    "\n",
    "# Build command\n",
    "sample_flag = f\"--sample-csv {SAMPLE_CSV}\" if TEST_MODE and SAMPLE_CSV.exists() else \"\"\n",
    "limit_flag = f\"--limit {LIMIT}\" if LIMIT else \"\"\n",
    "\n",
    "extract_cmd = f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.data.hybrid_extract \\\n",
    "  --manifest {manifest_csv} \\\n",
    "  --splits-root {splits_root} \\\n",
    "  --dataset-root {EFFECTIVE_DATASET_ROOT} \\\n",
    "  --output-root {HYBRID_OUTPUT_ROOT} \\\n",
    "  --variant {VARIANT} \\\n",
    "  --min-detection-area-frac 0.005 \\\n",
    "  --min-area-frac 0.01 \\\n",
    "  --min-aspect 0.08 \\\n",
    "  --pad-frac 0.35 \\\n",
    "  --max-area-frac 0.40 \\\n",
    "  {sample_flag} \\\n",
    "  {limit_flag} \\\n",
    "  --overwrite\n",
    "\"\"\"\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\"‚ö° TEST MODE: Using small sample for quick testing\")\n",
    "    print(f\"   Sample CSV: {SAMPLE_CSV}\")\n",
    "    if LIMIT:\n",
    "        print(f\"   Limit: {LIMIT} images\")\n",
    "else:\n",
    "    print(\"ü™µ FULL MODE: Processing all images\")\n",
    "\n",
    "print(f\"\\nRunning Hybrid extraction (InsightFace + MediaPipe Hands) for variant: {VARIANT}\")\n",
    "print(extract_cmd)\n",
    "proc = subprocess.Popen(\n",
    "    extract_cmd,\n",
    "    shell=True,\n",
    "    text=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "if proc.stdout is None:\n",
    "    raise RuntimeError(\"Extraction process has no stdout pipe.\")\n",
    "for line in proc.stdout:\n",
    "    print(line, end=\"\")\n",
    "proc.wait()\n",
    "if proc.returncode != 0:\n",
    "    raise RuntimeError(\"Hybrid extraction failed. Check logs above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cfc3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Regenerate Hybrid CSVs (manifest + splits)\n",
    "# Use this after extraction to rebuild CSVs that point to crop paths.\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "VARIANT = VARIANT if 'VARIANT' in globals() else 'face_hands'\n",
    "HYBRID_OUTPUT_ROOT = HYBRID_OUTPUT_ROOT if 'HYBRID_OUTPUT_ROOT' in globals() else Path('/content/data/hybrid')\n",
    "HYBRID_OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "OUT_ROOT = Path(OUT_ROOT) if 'OUT_ROOT' in globals() else Path('/content/drive/MyDrive')\n",
    "\n",
    "manifest_csv = Path(OUT_ROOT) / 'manifests' / 'manifest.csv'\n",
    "splits_root = Path(OUT_ROOT) / 'splits'\n",
    "crop_root = Path(HYBRID_OUTPUT_ROOT) / VARIANT\n",
    "meta_csv = Path(HYBRID_OUTPUT_ROOT) / f'detection_metadata_{VARIANT}.csv'\n",
    "\n",
    "def _extract_class(path_str):\n",
    "    for part in Path(path_str).parts:\n",
    "        if len(part) == 2 and part.startswith('c') and part[1].isdigit():\n",
    "            return part\n",
    "    return None\n",
    "\n",
    "def _extract_camera(path_str):\n",
    "    for part in Path(path_str).parts:\n",
    "        if part.lower().startswith('camera'):\n",
    "            return part\n",
    "    return None\n",
    "\n",
    "def _extract_filename(path_str):\n",
    "    return Path(path_str).name\n",
    "\n",
    "def _coerce_class_id(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    value_str = str(value)\n",
    "    if len(value_str) == 2 and value_str.startswith('c') and value_str[1].isdigit():\n",
    "        return value_str\n",
    "    if value_str.isdigit():\n",
    "        return f'c{int(value_str)}'\n",
    "    return None\n",
    "\n",
    "def _normalize_camera(cam):\n",
    "    \"\"\"Normalize camera values: 'Camera 1' -> 'cam1', 'cam2' -> 'cam2', etc.\"\"\"\n",
    "    if cam is None or pd.isna(cam):\n",
    "        return None\n",
    "    cam_str = str(cam).lower().replace(' ', '')\n",
    "    if cam_str in ['camera1', 'cam1']:\n",
    "        return 'cam1'\n",
    "    if cam_str in ['camera2', 'cam2']:\n",
    "        return 'cam2'\n",
    "    return cam_str\n",
    "\n",
    "print(f'üìÇ Loading original manifest: {manifest_csv}')\n",
    "orig_df = pd.read_csv(manifest_csv)\n",
    "orig_df = orig_df.rename(columns={'path': 'original_path'})\n",
    "orig_df['_filename'] = orig_df['original_path'].astype(str).map(_extract_filename)\n",
    "orig_df['_class'] = orig_df['original_path'].astype(str).map(_extract_class)\n",
    "orig_df['_camera'] = orig_df['original_path'].astype(str).map(_extract_camera).map(_normalize_camera)\n",
    "\n",
    "print(f'üîç Scanning crops: {crop_root}')\n",
    "crop_paths = list(crop_root.rglob('*.jpg'))\n",
    "crop_df = pd.DataFrame({'crop_path': [str(p) for p in crop_paths]})\n",
    "crop_df['path'] = crop_df['crop_path'].map(lambda p: str(Path(p).relative_to(HYBRID_OUTPUT_ROOT)))\n",
    "crop_df['_filename'] = crop_df['crop_path'].map(_extract_filename)\n",
    "crop_df['_class'] = crop_df['crop_path'].map(_extract_class)\n",
    "crop_df['_camera'] = crop_df['crop_path'].map(_extract_camera).map(_normalize_camera)\n",
    "\n",
    "fallback_paths = set()\n",
    "if meta_csv.exists():\n",
    "    meta_df = pd.read_csv(meta_csv)\n",
    "    meta_df = meta_df[meta_df['cropped_path'].astype(str).str.len() > 0]\n",
    "    meta_df['path'] = meta_df['cropped_path'].astype(str)\n",
    "    meta_df['_class'] = meta_df['class_id'].map(_coerce_class_id)\n",
    "    meta_df['_camera'] = meta_df['camera'].map(_normalize_camera)\n",
    "    crop_df = crop_df.merge(\n",
    "        meta_df[['path', '_class', '_camera', 'fallback_to_full']],\n",
    "        on='path',\n",
    "        how='left',\n",
    "        suffixes=('', '_meta'),\n",
    "    )\n",
    "    crop_df['_class'] = crop_df['_class'].fillna(crop_df['_class_meta'])\n",
    "    crop_df['_camera'] = crop_df['_camera'].fillna(crop_df['_camera_meta'])\n",
    "    crop_df = crop_df.drop(columns=['_class_meta', '_camera_meta'], errors='ignore')\n",
    "    fallback_paths = set(meta_df.loc[meta_df['fallback_to_full'] == True, 'path'].dropna().astype(str))\n",
    "    print(f'üö´ Excluding {len(fallback_paths)} fallback crops from splits')\n",
    "\n",
    "crop_df_all = crop_df.copy()\n",
    "crop_df = crop_df[~crop_df['path'].isin(fallback_paths)]\n",
    "\n",
    "merged = crop_df_all.merge(orig_df, on=['_filename', '_class', '_camera'], how='left')\n",
    "manifest_out = merged.drop(columns=['crop_path', '_filename', '_class', '_camera'], errors='ignore')\n",
    "manifest_out_path = Path(HYBRID_OUTPUT_ROOT) / f'manifest_{VARIANT}.csv'\n",
    "manifest_out.to_csv(manifest_out_path, index=False)\n",
    "print(f'‚úÖ Wrote manifest: {manifest_out_path}')\n",
    "\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    split_path = splits_root / f'{split_name}.csv'\n",
    "    split_df = pd.read_csv(split_path)\n",
    "    split_df['path'] = split_df['path'].astype(str)\n",
    "    split_df['_filename'] = split_df['path'].map(_extract_filename)\n",
    "    split_df['_class'] = split_df['path'].map(_extract_class)\n",
    "    split_df['_camera'] = split_df['path'].map(_extract_camera).map(_normalize_camera)\n",
    "\n",
    "    split_merged = split_df.merge(\n",
    "        crop_df,\n",
    "        on=['_filename', '_class', '_camera'],\n",
    "        how='inner',\n",
    "    )\n",
    "    \n",
    "    # FIXED: Explicitly set path to crop path (path_y) and rename path_x to original_path\n",
    "    split_merged['original_path'] = split_merged['path_x']\n",
    "    split_merged['path'] = split_merged['path_y']\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    cols_to_drop = ['path_x', 'path_y', '_filename', '_class', '_camera', 'crop_path', 'fallback_to_full']\n",
    "    split_merged = split_merged.drop(columns=[c for c in cols_to_drop if c in split_merged.columns])\n",
    "\n",
    "    out_split = Path(HYBRID_OUTPUT_ROOT) / f'{split_name}_{VARIANT}.csv'\n",
    "    split_merged.to_csv(out_split, index=False)\n",
    "    print(f'‚úÖ Wrote split: {out_split} ({len(split_merged)} rows)')\n",
    "    \n",
    "    # Verify the path column is correct\n",
    "    print(f\"   path sample: {split_merged['path'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798703bc",
   "metadata": {},
   "source": [
    "## üöö Copy Hybrid Crops (Fast ‚Üî Drive)\n",
    "\n",
    "**Two modes:**\n",
    "1. **After extraction to /content** ‚Üí Copy TO Drive for persistence\n",
    "2. **If extracted to Drive** ‚Üí Copy FROM Drive to /content for faster training\n",
    "\n",
    "Run this cell after extraction is complete.\n",
    "\n",
    "Use this if training/audit from Drive is slow. Copies crops to local SSD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fec68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöö Copy Hybrid crops (Fast ‚Üî Drive)\n",
    "import os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your variant to the one you extracted already\n",
    "HYBRID_VARIANT = \"face_hands\"  # face | hands | face_hands\n",
    "\n",
    "# Define both locations\n",
    "LOCAL_ROOT = Path(\"/content/data/hybrid\")\n",
    "DRIVE_ROOT = Path(OUT_ROOT) / \"hybrid\"\n",
    "\n",
    "LOCAL_VARIANT_DIR = LOCAL_ROOT / HYBRID_VARIANT\n",
    "DRIVE_VARIANT_DIR = DRIVE_ROOT / HYBRID_VARIANT\n",
    "\n",
    "# ===== AUTO-DETECT COPY DIRECTION =====\n",
    "local_exists = LOCAL_VARIANT_DIR.exists() and any(LOCAL_VARIANT_DIR.glob(\"**/*.jpg\"))\n",
    "drive_exists = DRIVE_VARIANT_DIR.exists() and any(DRIVE_VARIANT_DIR.glob(\"**/*.jpg\"))\n",
    "\n",
    "if local_exists and not drive_exists:\n",
    "    # Extracted to /content, need to copy TO Drive for persistence\n",
    "    COPY_MODE = \"TO_DRIVE\"\n",
    "    SRC_ROOT = LOCAL_ROOT\n",
    "    SRC_VARIANT_DIR = LOCAL_VARIANT_DIR\n",
    "    DST_ROOT = DRIVE_VARIANT_DIR\n",
    "    print(\"üì¶ Mode: Copy FROM /content TO Drive (for persistence)\")\n",
    "elif drive_exists and not local_exists:\n",
    "    # Extracted to Drive, copy TO /content for faster training\n",
    "    COPY_MODE = \"TO_LOCAL\"\n",
    "    SRC_ROOT = DRIVE_ROOT\n",
    "    SRC_VARIANT_DIR = DRIVE_VARIANT_DIR\n",
    "    DST_ROOT = LOCAL_VARIANT_DIR\n",
    "    print(\"üì¶ Mode: Copy FROM Drive TO /content (for faster training)\")\n",
    "elif local_exists and drive_exists:\n",
    "    # Both exist - ask user which direction\n",
    "    print(\"‚ö†Ô∏è  Crops exist in BOTH locations!\")\n",
    "    print(f\"   /content: {LOCAL_VARIANT_DIR}\")\n",
    "    print(f\"   Drive:    {DRIVE_VARIANT_DIR}\")\n",
    "    COPY_MODE = \"TO_DRIVE\"  # Default: persist to Drive\n",
    "    SRC_ROOT = LOCAL_ROOT\n",
    "    SRC_VARIANT_DIR = LOCAL_VARIANT_DIR\n",
    "    DST_ROOT = DRIVE_VARIANT_DIR\n",
    "    print(\"   Defaulting to: Copy TO Drive for persistence\")\n",
    "    print(\"   (Change COPY_MODE = 'TO_LOCAL' if you want the other direction)\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No crops found in either location!\\n\"\n",
    "        f\"   /content: {LOCAL_VARIANT_DIR}\\n\"\n",
    "        f\"   Drive:    {DRIVE_VARIANT_DIR}\\n\"\n",
    "        f\"Run the Hybrid extraction cell first.\"\n",
    "    )\n",
    "\n",
    "print(f\"Copying Hybrid crops from {SRC_VARIANT_DIR} -> {DST_ROOT}\")\n",
    "\n",
    "# Delete old destination directory to remove files that were skipped in new extraction\n",
    "if COPY_MODE == \"TO_DRIVE\" and DST_ROOT.exists():\n",
    "    print(f\"   üóëÔ∏è  Deleting old crops in {DST_ROOT} to ensure clean copy...\")\n",
    "    shutil.rmtree(DST_ROOT)\n",
    "\n",
    "DST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy images directory tree\n",
    "file_count = 0\n",
    "for src_dir, _, files in os.walk(SRC_VARIANT_DIR):\n",
    "    rel_dir = Path(src_dir).relative_to(SRC_VARIANT_DIR)\n",
    "    dst_dir = DST_ROOT / rel_dir\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for fname in files:\n",
    "        src_path = Path(src_dir) / fname\n",
    "        dst_path = dst_dir / fname\n",
    "        if not dst_path.exists():\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            file_count += 1\n",
    "print(f\"   Copied {file_count} image files\")\n",
    "\n",
    "# Copy manifest/split CSVs that live one level above the variant folder\n",
    "csv_names = [\n",
    "    f\"manifest_{HYBRID_VARIANT}.csv\",\n",
    "    f\"train_{HYBRID_VARIANT}.csv\",\n",
    "    f\"val_{HYBRID_VARIANT}.csv\",\n",
    "    f\"test_{HYBRID_VARIANT}.csv\",\n",
    "    f\"detection_metadata_{HYBRID_VARIANT}.csv\",  # for auditing\n",
    "]\n",
    "\n",
    "# Determine CSV destination (parent of variant dir)\n",
    "dst_csv_root = DST_ROOT.parent if COPY_MODE == \"TO_LOCAL\" else DRIVE_ROOT\n",
    "\n",
    "for fname in csv_names:\n",
    "    src_csv = SRC_ROOT / fname\n",
    "    if not src_csv.exists():\n",
    "        print(f\"   ‚ö†Ô∏è Skipping {fname} (not found)\")\n",
    "        continue\n",
    "    dst_csv = dst_csv_root / fname\n",
    "    shutil.copy2(src_csv, dst_csv)\n",
    "    print(f\"   ‚úÖ Copied {fname}\")\n",
    "\n",
    "# ===== SET ENVIRONMENT VARIABLES =====\n",
    "# Always point HYBRID_ROOT_LOCAL to /content for fast training\n",
    "os.environ[\"HYBRID_ROOT_LOCAL\"] = str(LOCAL_ROOT)\n",
    "\n",
    "# CRITICAL: Also update DATASET_ROOT so the Dataset class loads images from local copy!\n",
    "os.environ[\"DATASET_ROOT\"] = str(LOCAL_ROOT)\n",
    "import importlib\n",
    "from ddriver import config as _cfg\n",
    "importlib.reload(_cfg)\n",
    "\n",
    "print(f\"\\n‚úÖ Copy complete!\")\n",
    "if COPY_MODE == \"TO_DRIVE\":\n",
    "    print(f\"   üíæ Crops persisted to Drive: {DRIVE_ROOT}\")\n",
    "    print(f\"   ‚ö° Training will use fast local copy: {LOCAL_ROOT}\")\n",
    "else:\n",
    "    print(f\"   ‚ö° Crops copied to fast local SSD: {LOCAL_ROOT}\")\n",
    "    \n",
    "print(f\"\\n   HYBRID_ROOT_LOCAL = {os.environ['HYBRID_ROOT_LOCAL']}\")\n",
    "print(f\"   DATASET_ROOT      = {os.environ['DATASET_ROOT']} (images loaded from here)\")\n",
    "print(f\"\\n   Set USE_HYBRID=True in training cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f546d61d",
   "metadata": {},
   "source": [
    "## üîç Hybrid Crop Quality Audit\n",
    "\n",
    "Quick stats and visual inspection of Hybrid crops (InsightFace + MediaPipe Hands).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa2df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Hybrid Crop Quality Audit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "VARIANT = \"face_hands\"  # must match the variant you extracted\n",
    "\n",
    "# Auto-detect: prefer local copy if available\n",
    "hybrid_root_local = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", \"\"))\n",
    "if hybrid_root_local.exists():\n",
    "    hybrid_root = hybrid_root_local\n",
    "else:\n",
    "    hybrid_root = Path(OUT_ROOT) / \"hybrid\"\n",
    "\n",
    "metadata_csv = hybrid_root / f\"detection_metadata_{VARIANT}.csv\"\n",
    "if not metadata_csv.exists():\n",
    "    raise FileNotFoundError(f\"Detection metadata not found: {metadata_csv}\\nRun Hybrid extraction first.\")\n",
    "\n",
    "print(f\"üìÅ Loading metadata from: {metadata_csv}\")\n",
    "df = pd.read_csv(metadata_csv)\n",
    "\n",
    "# Summary stats\n",
    "n_total = len(df)\n",
    "n_fallback = df[\"fallback_to_full\"].sum()\n",
    "# Handle new 'skipped' column (may not exist in old metadata CSVs)\n",
    "n_skipped = df[\"skipped\"].sum() if \"skipped\" in df.columns else 0\n",
    "n_saved = n_total - n_skipped\n",
    "n_face = (df[\"face_count\"] > 0).sum()\n",
    "n_left_hand = df[\"left_hand_detected\"].sum()\n",
    "n_right_hand = df[\"right_hand_detected\"].sum()\n",
    "n_both_hands = ((df[\"left_hand_detected\"]) & (df[\"right_hand_detected\"])).sum()\n",
    "n_any_hands = ((df[\"left_hand_detected\"]) | (df[\"right_hand_detected\"])).sum()\n",
    "n_face_and_hands = ((df[\"face_count\"] > 0) & (n_any_hands > 0)).sum()\n",
    "avg_face_conf = df.loc[df[\"face_confidence\"] > 0, \"face_confidence\"].mean()\n",
    "avg_left_conf = df.loc[df[\"left_hand_confidence\"] > 0, \"left_hand_confidence\"].mean()\n",
    "avg_right_conf = df.loc[df[\"right_hand_confidence\"] > 0, \"right_hand_confidence\"].mean()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä HYBRID DETECTION SUMMARY (RetinaFace + MediaPipe Hands)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total images processed: {n_total}\")\n",
    "if n_skipped > 0:\n",
    "    print(f\"   Images SAVED: {n_saved} ({100*n_saved/n_total:.1f}%)\")\n",
    "    print(f\"   Images SKIPPED: {n_skipped} ({100*n_skipped/n_total:.1f}%)\")\n",
    "print(f\"\\nüéØ Detection rates:\")\n",
    "print(f\"   Face detected (RetinaFace): {n_face} ({100*n_face/n_total:.1f}%)\")\n",
    "print(f\"   Left hand (MediaPipe): {n_left_hand} ({100*n_left_hand/n_total:.1f}%)\")\n",
    "print(f\"   Right hand (MediaPipe): {n_right_hand} ({100*n_right_hand/n_total:.1f}%)\")\n",
    "print(f\"   Both hands: {n_both_hands} ({100*n_both_hands/n_total:.1f}%)\")\n",
    "print(f\"   Any hand: {n_any_hands} ({100*n_any_hands/n_total:.1f}%)\")\n",
    "print(f\"\\nüìà Confidence scores:\")\n",
    "print(f\"   Avg face confidence: {avg_face_conf:.3f}\" if not np.isnan(avg_face_conf) else \"   Avg face confidence: N/A\")\n",
    "print(f\"   Avg left hand confidence: {avg_left_conf:.3f}\" if not np.isnan(avg_left_conf) else \"   Avg left hand confidence: N/A\")\n",
    "print(f\"   Avg right hand confidence: {avg_right_conf:.3f}\" if not np.isnan(avg_right_conf) else \"   Avg right hand confidence: N/A\")\n",
    "print(f\"\\n‚ö†Ô∏è  Fallback to full frame: {n_fallback} ({100*n_fallback/n_total:.1f}%)\")\n",
    "\n",
    "# Fallback reason breakdown\n",
    "fallback_df = df[df[\"fallback_to_full\"]]\n",
    "if len(fallback_df) > 0:\n",
    "    print(f\"   Fallback reasons:\")\n",
    "    for reason, count in fallback_df[\"fallback_reason\"].value_counts().items():\n",
    "        print(f\"      - {reason}: {count} ({100*count/n_total:.1f}%)\")\n",
    "\n",
    "# ROI stats (for non-fallback images)\n",
    "non_fallback = df[~df[\"fallback_to_full\"]]\n",
    "if len(non_fallback) > 0:\n",
    "    print(f\"\\nüìê ROI statistics (non-fallback only, n={len(non_fallback)}):\")\n",
    "    print(f\"   Raw detection area: mean={non_fallback['raw_detection_area_frac'].mean():.3f}, \"\n",
    "          f\"std={non_fallback['raw_detection_area_frac'].std():.3f}\")\n",
    "    print(f\"   Final ROI area: mean={non_fallback['roi_area_frac'].mean():.3f}, \"\n",
    "          f\"std={non_fallback['roi_area_frac'].std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Hybrid Breakdown by Camera and Class\n",
    "print(\"\\nüìã BREAKDOWN BY CAMERA\")\n",
    "print(\"-\" * 80)\n",
    "camera_stats = df.groupby(\"camera\").agg({\n",
    "    \"fallback_to_full\": [\"sum\", \"mean\"],\n",
    "    \"roi_area_frac\": \"mean\",\n",
    "    \"face_count\": lambda x: (x > 0).mean(),\n",
    "    \"left_hand_detected\": \"mean\",\n",
    "    \"right_hand_detected\": \"mean\",\n",
    "}).round(3)\n",
    "camera_stats.columns = [\"fallback_count\", \"fallback_pct\", \"mean_roi_area\", \"face_rate\", \"left_hand_rate\", \"right_hand_rate\"]\n",
    "camera_stats[\"fallback_pct\"] = (camera_stats[\"fallback_pct\"] * 100).round(1)\n",
    "camera_stats[\"face_rate\"] = (camera_stats[\"face_rate\"] * 100).round(1)\n",
    "camera_stats[\"left_hand_rate\"] = (camera_stats[\"left_hand_rate\"] * 100).round(1)\n",
    "camera_stats[\"right_hand_rate\"] = (camera_stats[\"right_hand_rate\"] * 100).round(1)\n",
    "print(camera_stats.to_string())\n",
    "\n",
    "print(\"\\nüìã BREAKDOWN BY CLASS\")\n",
    "print(\"-\" * 80)\n",
    "class_stats = df.groupby(\"class_id\").agg({\n",
    "    \"fallback_to_full\": [\"sum\", \"mean\"],\n",
    "    \"roi_area_frac\": \"mean\",\n",
    "    \"face_count\": lambda x: (x > 0).mean(),\n",
    "    \"left_hand_detected\": \"mean\",\n",
    "    \"right_hand_detected\": \"mean\",\n",
    "}).round(3)\n",
    "class_stats.columns = [\"fallback_count\", \"fallback_pct\", \"mean_roi_area\", \"face_rate\", \"left_hand_rate\", \"right_hand_rate\"]\n",
    "class_stats[\"fallback_pct\"] = (class_stats[\"fallback_pct\"] * 100).round(1)\n",
    "class_stats[\"face_rate\"] = (class_stats[\"face_rate\"] * 100).round(1)\n",
    "class_stats[\"left_hand_rate\"] = (class_stats[\"left_hand_rate\"] * 100).round(1)\n",
    "class_stats[\"right_hand_rate\"] = (class_stats[\"right_hand_rate\"] * 100).round(1)\n",
    "print(class_stats.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116fa496",
   "metadata": {},
   "source": [
    "## üß™ Face-Only / Hands-Only Ablation Setup\n",
    "\n",
    "**Purpose:** Understand which modality (face vs hands) carries the signal for each class.\n",
    "\n",
    "**This is NOT about \"winning accuracy\"** - these ablations will usually be worse than face+hands combined. The goal is to produce **evidence** about:\n",
    "1. Which modality explains which classes (hands-dominated vs face-dominated)\n",
    "2. What types of errors happen when a modality is removed  \n",
    "3. Whether Grad-CAM shows meaningful cues vs shortcuts within each modality\n",
    "\n",
    "**Workflow:**\n",
    "1. Run **face-only extraction** (change `VARIANT = \"face\"` in cell above, re-run)\n",
    "2. Copy to `/content/data/hybrid/face/` (or run from there directly)\n",
    "3. Train a face-only model\n",
    "4. Repeat for **hands-only** (`VARIANT = \"hands\"`)\n",
    "\n",
    "**NEW: Automatic Fallback Skipping**\n",
    "- `hands` variant: **ONLY saves images where hands were detected** (skips fallbacks)\n",
    "- `face` variant: **ONLY saves images where face was detected** (skips fallbacks)\n",
    "- The split CSVs automatically contain only the saved images\n",
    "- No \"zoomed out\" full-frame images will contaminate your ablation data!\n",
    "\n",
    "**Evaluation is automatic subset-only:**\n",
    "- Face-only models train/eval on images where face detection succeeded\n",
    "- Hands-only models train/eval on images where hand detection succeeded\n",
    "- Check `detection_metadata_*.csv` for coverage stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c69110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Ablation Helper: Check Detection Coverage for Subset-Only Evaluation\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "HYBRID_ROOT = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", Path(OUT_ROOT) / \"hybrid\"))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ ABLATION SETUP: Detection Coverage Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load detection metadata for face_hands (it includes both face and hands detection status)\n",
    "metadata_files = list(HYBRID_ROOT.glob(\"detection_metadata_*.csv\"))\n",
    "if not metadata_files:\n",
    "    print(\"‚ö†Ô∏è  No detection metadata found. Run hybrid extraction first.\")\n",
    "else:\n",
    "    for mf in metadata_files:\n",
    "        variant = mf.stem.replace(\"detection_metadata_\", \"\")\n",
    "        print(f\"\\nüìä Variant: {variant}\")\n",
    "        \n",
    "        df = pd.read_csv(mf)\n",
    "        total = len(df)\n",
    "        \n",
    "        # Check for detection columns\n",
    "        if \"face_detected\" in df.columns:\n",
    "            face_ok = df[\"face_detected\"].sum()\n",
    "            print(f\"   Face detected: {face_ok}/{total} ({100*face_ok/total:.1f}%)\")\n",
    "        \n",
    "        if \"hands_detected\" in df.columns:\n",
    "            hands_ok = df[\"hands_detected\"].sum()\n",
    "            print(f\"   Hands detected: {hands_ok}/{total} ({100*hands_ok/total:.1f}%)\")\n",
    "        elif \"left_hand_detected\" in df.columns and \"right_hand_detected\" in df.columns:\n",
    "            any_hand = (df[\"left_hand_detected\"] | df[\"right_hand_detected\"]).sum()\n",
    "            print(f\"   Any hand detected: {any_hand}/{total} ({100*any_hand/total:.1f}%)\")\n",
    "        \n",
    "        # If using hybrid with face_hands, calculate fallback rate\n",
    "        if \"used_fallback\" in df.columns:\n",
    "            fallback = df[\"used_fallback\"].sum()\n",
    "            print(f\"   Used fallback (full frame): {fallback}/{total} ({100*fallback/total:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã ABLATION VARIANTS TO TRAIN:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "To run face-only or hands-only ablations:\n",
    "\n",
    "1. EXTRACTION (modify cell above):\n",
    "   VARIANT = \"face\"   # or \"hands\" \n",
    "   Re-run the hybrid extraction cell\n",
    "\n",
    "2. COPY to /content:\n",
    "   Change SRC_VARIANT_DIR and DST_ROOT in the copy cell\n",
    "\n",
    "3. TRAINING:\n",
    "   Create a new training run with appropriate tag:\n",
    "   TRAIN_TAG = \"effb0_face_only_V1\"\n",
    "   # or\n",
    "   TRAIN_TAG = \"effb0_hands_only_V1\"\n",
    "\n",
    "4. EVALUATION (subset-only):\n",
    "   Use the detection metadata to filter predictions to only\n",
    "   images where the relevant modality was detected.\n",
    "\"\"\")\n",
    "\n",
    "# Show which ablation datasets are available\n",
    "print(\"\\nüìÅ Available ablation datasets:\")\n",
    "for variant in [\"face\", \"hands\", \"face_hands\"]:\n",
    "    variant_dir = HYBRID_ROOT / variant\n",
    "    if variant_dir.exists():\n",
    "        n_files = len(list(variant_dir.glob(\"**/*.jpg\")))\n",
    "        print(f\"   ‚úÖ {variant}: {n_files} images\")\n",
    "    else:\n",
    "        print(f\"   ‚¨ú {variant}: not extracted yet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e011f7",
   "metadata": {},
   "source": [
    "## üöÇ 11.1 Train a model (adjust these knobs)\n",
    "\n",
    "- Choose a `RUN_TAG` so logs/checkpoints go into `TFM/checkpoints/runs/<tag>/...`\n",
    "- Set epochs/batch size to something small for a dry run (1 epoch, 16 batch)\n",
    "- This command uses the CLI helper (`python -m src.ddriver.cli.train ...`)\n",
    "- Logs + checkpoints are saved automatically to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess, textwrap, json, time, threading\n",
    "from pathlib import Path\n",
    "\n",
    "# ConvNeXt-Tiny baseline run (change RUN_TAG for each experiment)\n",
    "RUN_TAG = \"effb0_noLabelSmoothingCORRECTED\"   # change me for each experiment\n",
    "MODEL_NAME = \"efficientnet_b0\"                  # must be registered above (timm)\n",
    "\n",
    "# Training hyperparameters (EfficientNet-B0)\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_SIZE = 224\n",
    "LR = 3e-4                        # per provided hyperparams\n",
    "LR_DROP_EPOCH = None             # no LR drop\n",
    "LR_DROP_FACTOR = 0.1\n",
    "LABEL_SMOOTHING = 0.0\n",
    "USE_TINY_SPLIT = False\n",
    "\n",
    "# ROI crop pipeline selection (pick ONE, set others to False)\n",
    "USE_MEDIAPIPE = False             # set True to use MediaPipe ROI crops\n",
    "USE_YOLO = False                  # set True to use YOLO-World ROI crops\n",
    "USE_HYBRID = True                 # set True to use Hybrid (RetinaFace + MediaPipe Hands) crops\n",
    "ROI_VARIANT = \"face_hands\"        # face | hands | face_hands\n",
    "\n",
    "# Validate only one pipeline is selected\n",
    "active_pipelines = sum([USE_MEDIAPIPE, USE_YOLO, USE_HYBRID])\n",
    "if active_pipelines > 1:\n",
    "    raise ValueError(\"Pick ONE pipeline: set only one of USE_MEDIAPIPE, USE_YOLO, USE_HYBRID to True.\")\n",
    "\n",
    "if USE_HYBRID:\n",
    "    hybrid_root = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", Path(OUT_ROOT) / \"hybrid\"))\n",
    "    manifest_csv = hybrid_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    train_split = f\"train_{ROI_VARIANT}.csv\" if not USE_TINY_SPLIT else f\"train_small_{ROI_VARIANT}.csv\"\n",
    "    val_csv = hybrid_root / f\"val_{ROI_VARIANT}.csv\"\n",
    "    test_csv = hybrid_root / f\"test_{ROI_VARIANT}.csv\"\n",
    "    train_csv = hybrid_root / train_split\n",
    "    print(f\"üîÄ Using Hybrid (RetinaFace + MediaPipe Hands) ROI variant: {ROI_VARIANT}\")\n",
    "    print(f\"   hybrid_root = {hybrid_root}\")\n",
    "elif USE_YOLO:\n",
    "    yolo_root = Path(os.environ.get(\"YOLO_ROOT_LOCAL\", Path(OUT_ROOT) / \"yolo\"))\n",
    "    manifest_csv = yolo_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    train_split = f\"train_{ROI_VARIANT}.csv\" if not USE_TINY_SPLIT else f\"train_small_{ROI_VARIANT}.csv\"\n",
    "    val_csv = yolo_root / f\"val_{ROI_VARIANT}.csv\"\n",
    "    test_csv = yolo_root / f\"test_{ROI_VARIANT}.csv\"\n",
    "    train_csv = yolo_root / train_split\n",
    "    print(f\"üéØ Using YOLO-World ROI variant: {ROI_VARIANT}\")\n",
    "    print(f\"   yolo_root = {yolo_root}\")\n",
    "elif USE_MEDIAPIPE:\n",
    "    mp_root = Path(os.environ.get(\"MEDIAPIPE_ROOT_LOCAL\", Path(OUT_ROOT) / \"mediapipe\"))\n",
    "    manifest_csv = mp_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    train_split = f\"train_{ROI_VARIANT}.csv\" if not USE_TINY_SPLIT else f\"train_small_{ROI_VARIANT}.csv\"\n",
    "    val_csv = mp_root / f\"val_{ROI_VARIANT}.csv\"\n",
    "    test_csv = mp_root / f\"test_{ROI_VARIANT}.csv\"\n",
    "    train_csv = mp_root / train_split\n",
    "    print(f\"üß≠ Using MediaPipe ROI variant: {ROI_VARIANT}\")\n",
    "    print(f\"   mp_root = {mp_root}\")\n",
    "else:\n",
    "    manifest_csv = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "    train_split = \"train_small.csv\" if USE_TINY_SPLIT else \"train.csv\"\n",
    "    train_csv = Path(OUT_ROOT) / \"splits\" / train_split\n",
    "    val_csv = Path(OUT_ROOT) / \"splits\" / \"val.csv\"\n",
    "    test_csv = Path(OUT_ROOT) / \"splits\" / \"test.csv\"\n",
    "    print(\"üì∑ Using full-frame images (no ROI cropping)\")\n",
    "\n",
    "# CRITICAL: Update DATASET_ROOT when using local ROI crops so images load from fast SSD\n",
    "_roi_root = None\n",
    "if USE_HYBRID:\n",
    "    _roi_root = hybrid_root\n",
    "elif USE_YOLO:\n",
    "    _roi_root = yolo_root\n",
    "elif USE_MEDIAPIPE:\n",
    "    _roi_root = mp_root\n",
    "\n",
    "if _roi_root and str(_roi_root).startswith(\"/content/data\"):\n",
    "    import importlib\n",
    "    os.environ[\"DATASET_ROOT\"] = str(_roi_root)\n",
    "    from ddriver import config as _cfg\n",
    "    importlib.reload(_cfg)\n",
    "    print(f\"   ‚ö° DATASET_ROOT = {os.environ['DATASET_ROOT']} (fast local SSD)\")\n",
    "elif _roi_root:\n",
    "    print(f\"   ‚ö†Ô∏è  DATASET_ROOT = {os.environ.get('DATASET_ROOT', 'not set')} (reading from Drive - may be slow)\")\n",
    "\n",
    "if USE_TINY_SPLIT:\n",
    "    print(\"‚ö° Using train_small.csv (20 imgs/class) for a quick smoke test.\")\n",
    "else:\n",
    "    print(\"ü™µ Using full train.csv for a proper run.\")\n",
    "\n",
    "train_cmd = textwrap.dedent(f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.cli.train \\\n",
    "    --model-name {MODEL_NAME} \\\n",
    "    --epochs {EPOCHS} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --num-workers {NUM_WORKERS} \\\n",
    "    --image-size {IMAGE_SIZE} \\\n",
    "    --lr {LR} \\\n",
    "    --weight-decay 0.0 \\\n",
    "    --optimizer adam \\\n",
    "    --label-smoothing {LABEL_SMOOTHING} \\\n",
    "    --out-tag {RUN_TAG} \\\n",
    "    --manifest-csv {manifest_csv} \\\n",
    "    --train-csv {train_csv} \\\n",
    "    --val-csv {val_csv} \\\n",
    "    --test-csv {test_csv}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Running training command and streaming logs:\\n\", train_cmd)\n",
    "\n",
    "proc = subprocess.Popen(\n",
    "    train_cmd,\n",
    "    shell=True,\n",
    "    text=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "\n",
    "# Background GPU monitor (prints every 5 seconds)\n",
    "def _gpu_monitor():\n",
    "    while proc.poll() is None:\n",
    "        try:\n",
    "            stats = subprocess.check_output(\n",
    "                \"nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total \"\n",
    "                \"--format=csv,nounits,noheader\",\n",
    "                shell=True,\n",
    "            ).decode(\"utf-8\").strip()\n",
    "            print(f\"[GPU] util%, mem_used, mem_total :: {stats}\")\n",
    "        except Exception as exc:\n",
    "            print(\"[GPU] Could not query nvidia-smi:\", exc)\n",
    "        time.sleep(5)\n",
    "\n",
    "monitor_thread = threading.Thread(target=_gpu_monitor, daemon=True)\n",
    "monitor_thread.start()\n",
    "\n",
    "# Stream CLI stdout live\n",
    "if proc.stdout is None:\n",
    "    raise RuntimeError(\"Training process has no stdout pipe.\")\n",
    "\n",
    "for line in proc.stdout:\n",
    "    print(line, end=\"\")\n",
    "\n",
    "proc.wait()\n",
    "monitor_thread.join(timeout=0)\n",
    "\n",
    "print(\"\\n‚úÖ Training run complete!\\n\")\n",
    "\n",
    "# --- Display every epoch's metrics so the notebook shows the learning curve ---\n",
    "run_base = Path(CKPT_ROOT) / \"runs\" / RUN_TAG\n",
    "all_runs = sorted(run_base.glob(\"*/\"))\n",
    "if not all_runs:\n",
    "    raise FileNotFoundError(f\"No run folders found under {run_base}\")\n",
    "latest_run = all_runs[-1]\n",
    "\n",
    "history_path = latest_run / \"history.json\"\n",
    "if not history_path.exists():\n",
    "    raise FileNotFoundError(f\"history.json not found in {latest_run}\")\n",
    "\n",
    "history = json.loads(history_path.read_text()).get(\"history\", [])\n",
    "print(f\"üìä Epoch metrics for run: {latest_run.name}\")\n",
    "for record in history:\n",
    "    train_metrics = record.get(\"train\", {})\n",
    "    val_metrics = record.get(\"val\", {}) or {}\n",
    "    train_loss = train_metrics.get(\"loss\")\n",
    "    train_acc = train_metrics.get(\"accuracy\")\n",
    "    val_loss = val_metrics.get(\"loss\")\n",
    "    val_acc = val_metrics.get(\"accuracy\")\n",
    "    val_str = (\n",
    "        f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\"\n",
    "        if val_loss is not None and val_acc is not None\n",
    "        else \"val_loss=‚Äî val_acc=‚Äî\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Epoch {record['epoch']:>2}: \"\n",
    "        f\"train_loss={train_loss:.4f} acc={train_acc:.4f}  \"\n",
    "        f\"{val_str}\"\n",
    "    )\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b93ef9",
   "metadata": {},
   "source": [
    "## üìù 11.1a Log training summary to Google Sheet\n",
    "Run this right after the training cell finishes. It looks up the newest run under `CKPT_ROOT/runs/<RUN_TAG>`, grabs the best/final train + val accuracies, and logs the model/hyperparams so you can compare experiments before doing predictions or metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75995de",
   "metadata": {},
   "source": [
    "## üöö Copy MediaPipe crops to /content (optional, faster I/O)\n",
    "Use this if training from Drive is slow. It copies the generated MediaPipe crops/CSVs into `/content/data/mediapipe/<variant>` and updates paths for the current runtime. Does not affect the original full-image copy cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your variant to the one you extracted already\n",
    "MEDIAPIPE_VARIANT = \"face_hands\"  # face | hands | face_hands\n",
    "SRC_ROOT = Path(OUT_ROOT) / \"mediapipe\"\n",
    "SRC_VARIANT_DIR = SRC_ROOT / MEDIAPIPE_VARIANT\n",
    "DST_ROOT = Path(\"/content/data/mediapipe\") / MEDIAPIPE_VARIANT\n",
    "\n",
    "if not SRC_VARIANT_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Source mediapipe folder not found: {SRC_VARIANT_DIR}\\nRun the extraction cell first.\")\n",
    "\n",
    "print(f\"Copying MediaPipe crops from {SRC_VARIANT_DIR} -> {DST_ROOT}\")\n",
    "DST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy images directory tree\n",
    "for src_dir, _, files in os.walk(SRC_VARIANT_DIR):\n",
    "    rel_dir = Path(src_dir).relative_to(SRC_VARIANT_DIR)\n",
    "    dst_dir = DST_ROOT / rel_dir\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for fname in files:\n",
    "        src_path = Path(src_dir) / fname\n",
    "        dst_path = dst_dir / fname\n",
    "        if not dst_path.exists():\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "# Copy manifest/split CSVs that live one level above the variant folder\n",
    "csv_names = [\n",
    "    f\"manifest_{MEDIAPIPE_VARIANT}.csv\",\n",
    "    f\"train_{MEDIAPIPE_VARIANT}.csv\",\n",
    "    f\"val_{MEDIAPIPE_VARIANT}.csv\",\n",
    "    f\"test_{MEDIAPIPE_VARIANT}.csv\",\n",
    "]\n",
    "for fname in csv_names:\n",
    "    src_csv = SRC_ROOT / fname\n",
    "    if not src_csv.exists():\n",
    "        raise FileNotFoundError(src_csv)\n",
    "    dst_csv = DST_ROOT.parent / fname\n",
    "    shutil.copy2(src_csv, dst_csv)\n",
    "    print(f\"Copied {src_csv} -> {dst_csv}\")\n",
    "\n",
    "# Point env vars for this runtime to the local mediapipe copy\n",
    "os.environ[\"MEDIAPIPE_ROOT_LOCAL\"] = str(DST_ROOT.parent)\n",
    "os.environ[\"MEDIAPIPE_VARIANT\"] = MEDIAPIPE_VARIANT\n",
    "\n",
    "# CRITICAL: Also update DATASET_ROOT so the Dataset class loads images from local copy!\n",
    "os.environ[\"DATASET_ROOT\"] = str(DST_ROOT.parent)\n",
    "import importlib\n",
    "from ddriver import config as _cfg\n",
    "importlib.reload(_cfg)\n",
    "\n",
    "print(\"\\n‚úÖ Copy complete. Set USE_MEDIAPIPE=True in training cell.\")\n",
    "print(f\"   MEDIAPIPE_ROOT_LOCAL = {os.environ['MEDIAPIPE_ROOT_LOCAL']}\")\n",
    "print(f\"   DATASET_ROOT         = {os.environ['DATASET_ROOT']} (images loaded from here)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Training summary ‚Üí Google Sheet\n",
    "!pip -q install gspread\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import gspread\n",
    "from google.colab import auth\n",
    "import google.auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "creds, _ = google.auth.default()\n",
    "gc = gspread.authorize(creds)\n",
    "\n",
    "TRAIN_SHEET_NAME = \"TFM Train Logs\"   # create this sheet/tab ahead of time\n",
    "TRAIN_WORKSHEET = \"Sheet1\"\n",
    "\n",
    "run_base = Path(CKPT_ROOT) / \"runs\" / RUN_TAG\n",
    "all_runs = sorted(run_base.glob(\"*/\"))\n",
    "if not all_runs:\n",
    "    raise FileNotFoundError(f\"No run folders found under {run_base}\")\n",
    "latest_run = all_runs[-1]\n",
    "print(f\"Logging training summary for run folder: {latest_run}\")\n",
    "\n",
    "history_path = latest_run / \"history.json\"\n",
    "if not history_path.exists():\n",
    "    raise FileNotFoundError(f\"history.json not found under {latest_run}\")\n",
    "\n",
    "history_records = json.loads(history_path.read_text()).get(\"history\", [])\n",
    "if not history_records:\n",
    "    raise ValueError(f\"history.json under {latest_run} has no records.\")\n",
    "\n",
    "params_path = latest_run / \"params.json\"\n",
    "params = json.loads(params_path.read_text()) if params_path.exists() else {}\n",
    "\n",
    "model_name = params.get(\"model_name\", MODEL_NAME)\n",
    "epochs_cfg = params.get(\"epochs\", EPOCHS)\n",
    "batch_cfg = params.get(\"batch_size\", BATCH_SIZE)\n",
    "lr_cfg = params.get(\"lr\", LR)\n",
    "lr_drop_epoch_cfg = params.get(\"lr_drop_epoch\", LR_DROP_EPOCH)\n",
    "lr_drop_factor_cfg = params.get(\"lr_drop_factor\", LR_DROP_FACTOR)\n",
    "image_size_cfg = params.get(\"image_size\", IMAGE_SIZE)\n",
    "num_workers_cfg = params.get(\"num_workers\", NUM_WORKERS)\n",
    "use_tiny_cfg = params.get(\"use_tiny_split\", USE_TINY_SPLIT)\n",
    "\n",
    "\n",
    "def _best_metric(records, split: str) -> tuple[dict, float | None]:\n",
    "    best_epoch = None\n",
    "    best_acc = None\n",
    "    for rec in records:\n",
    "        split_metrics = rec.get(split) or {}\n",
    "        acc = split_metrics.get(\"accuracy\")\n",
    "        if acc is None:\n",
    "            continue\n",
    "        if best_acc is None or acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_epoch = rec.get(\"epoch\")\n",
    "    final_metrics = records[-1].get(split) or {}\n",
    "    final_acc = final_metrics.get(\"accuracy\")\n",
    "    return {\"epoch\": best_epoch, \"accuracy\": best_acc}, final_acc\n",
    "\n",
    "\n",
    "best_train, final_train = _best_metric(history_records, \"train\")\n",
    "best_val, final_val = _best_metric(history_records, \"val\")\n",
    "\n",
    "row = [\n",
    "    RUN_TAG,\n",
    "    latest_run.name,\n",
    "    model_name,\n",
    "    epochs_cfg,\n",
    "    batch_cfg,\n",
    "    lr_cfg,\n",
    "    lr_drop_epoch_cfg,\n",
    "    lr_drop_factor_cfg,\n",
    "    image_size_cfg,\n",
    "    num_workers_cfg,\n",
    "    use_tiny_cfg,\n",
    "    best_train[\"epoch\"] if best_train[\"epoch\"] is not None else \"\",\n",
    "    round(best_train[\"accuracy\"], 4) if best_train[\"accuracy\"] is not None else \"\",\n",
    "    best_val[\"epoch\"] if best_val[\"epoch\"] is not None else \"\",\n",
    "    round(best_val[\"accuracy\"], 4) if best_val[\"accuracy\"] is not None else \"\",\n",
    "    round(final_train, 4) if final_train is not None else \"\",\n",
    "    round(final_val, 4) if final_val is not None else \"\",\n",
    "]\n",
    "\n",
    "ws = gc.open(TRAIN_SHEET_NAME).worksheet(TRAIN_WORKSHEET)\n",
    "ws.append_row(row, value_input_option=\"USER_ENTERED\")\n",
    "print(f\"Appended training summary for {latest_run.name} ‚úÖ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c73e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Optional: copy + compress dataset subset ‚Üí fast local SSD (/content/data)\n",
    "# Re-encodes JPEGs once (quality 80, short side 320px) before landing in /content/data.\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from ddriver.data.fastcopy import CompressionSpec, copy_splits_with_compression\n",
    "\n",
    "SRC_ROOT = Path(DRIVE_DATA_ROOT) / \"auc.distracted.driver.dataset_v2\"\n",
    "DST_ROOT = Path(FAST_DATA) / \"auc.distracted.driver.dataset_v2\"\n",
    "\n",
    "split_csvs = {\n",
    "    \"train\": Path(OUT_ROOT) / \"splits\" / \"train.csv\",\n",
    "    \"val\": Path(OUT_ROOT) / \"splits\" / \"val.csv\",\n",
    "    \"train_small\": Path(OUT_ROOT) / \"splits\" / \"train_small.csv\",\n",
    "}\n",
    "\n",
    "compression_spec = CompressionSpec(\n",
    "    target_short_side=320,  # still >= image_size + resize margin for training\n",
    "    jpeg_quality=80,        # ImageNet-level compression, visually lossless\n",
    ")\n",
    "\n",
    "summary = copy_splits_with_compression(\n",
    "    split_csvs=split_csvs,\n",
    "    src_root=SRC_ROOT,\n",
    "    dst_root=DST_ROOT,\n",
    "    compression=compression_spec,\n",
    "    skip_existing=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nüìâ FAST_DATA copy stats: processed {summary['processed']} of {summary['total']} files \"\n",
    "    f\"(skipped {summary['skipped']} already present).\"\n",
    ")\n",
    "print(f\"Compressed dataset root: {summary['dst_root']}\")\n",
    "\n",
    "DATASET_ROOT = FAST_DATA\n",
    "os.environ[\"DATASET_ROOT\"] = str(DATASET_ROOT)\n",
    "try:\n",
    "    from ddriver import config as _ddriver_config\n",
    "    importlib.reload(_ddriver_config)\n",
    "    print(\"\\n‚ö° Copy complete. DATASET_ROOT now points to the local FAST_DATA copy for this runtime:\")\n",
    "    print(\"   ddriver.config.DATASET_ROOT =\", _ddriver_config.DATASET_ROOT)\n",
    "except Exception as exc:\n",
    "    print(\"\\n‚ö° Copy complete and DATASET_ROOT env updated, but could not reload ddriver.config:\", exc)\n",
    "print(\"   (Re-run env summary if you want to rewrite .env, but training now uses /content/data.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb9779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Optional: copy + compress TEST split ‚Üí /content/data (same settings as train/val)\n",
    "\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "from ddriver.data.fastcopy import CompressionSpec, copy_splits_with_compression\n",
    "\n",
    "SRC_ROOT = Path(DRIVE_DATA_ROOT) / \"auc.distracted.driver.dataset_v2\"\n",
    "DST_ROOT = Path(FAST_DATA) / \"auc.distracted.driver.dataset_v2\"\n",
    "\n",
    "split_csvs = {\n",
    "    \"test\": Path(OUT_ROOT) / \"splits\" / \"test.csv\",\n",
    "}\n",
    "\n",
    "compression_spec = CompressionSpec(\n",
    "    target_short_side=320,\n",
    "    jpeg_quality=80,\n",
    ")\n",
    "\n",
    "summary = copy_splits_with_compression(\n",
    "    split_csvs=split_csvs,\n",
    "    src_root=SRC_ROOT,\n",
    "    dst_root=DST_ROOT,\n",
    "    compression=compression_spec,\n",
    "    skip_existing=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nüìâ FAST_DATA test copy stats: processed {summary['processed']} of {summary['total']} \"\n",
    "    f\"(skipped {summary['skipped']} already present).\"\n",
    ")\n",
    "print(f\"Compressed dataset root: {summary['dst_root']}\")\n",
    "\n",
    "# DATASET_ROOT is already pointing at FAST_DATA from the earlier cell, but reload config just in case\n",
    "try:\n",
    "    from ddriver import config as _ddriver_config\n",
    "    importlib.reload(_ddriver_config)\n",
    "    print(\"\\n‚ö° Test copy complete. ddriver.config now sees:\")\n",
    "    print(\"   ddriver.config.DATASET_ROOT =\", _ddriver_config.DATASET_ROOT)\n",
    "except Exception as exc:\n",
    "    print(\"\\n‚ö° Test copy complete; config reload optional:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c429a0e",
   "metadata": {},
   "source": [
    "## üì¶ 11.2 Pick the latest checkpoint file\n",
    "\n",
    "This cell looks inside `CKPT_ROOT/runs/<RUN_TAG>/` and grabs the newest `epoch_*.pt`. Use this path in the prediction step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RUN_TAG = \"convnext_tiny_full_v1\"  # pick the tag you want to inspect\n",
    "#RUN_TAG = globals().get(\"RUN_TAG\", \"convnext_tiny_full_v1\")  # reuse your latest training tag by default\n",
    "\n",
    "run_base = Path(CKPT_ROOT) / \"runs\" / RUN_TAG\n",
    "runs = sorted(run_base.glob(\"*/\"))\n",
    "if not runs:\n",
    "    raise FileNotFoundError(f\"No run folders found under {run_base}\")\n",
    "\n",
    "# ---- choose which run folder to use ----\n",
    "RUN_IDX = -1          # -1 = newest, 0 = oldest, or any index from the printout below\n",
    "print(\"Available runs:\")\n",
    "for idx, run_dir in enumerate(runs):\n",
    "    print(f\"  [{idx}] {run_dir.name}\")\n",
    "target_run = runs[RUN_IDX]\n",
    "print(f\"\\nSelected run: {target_run}\\n\")\n",
    "\n",
    "# ---- choose which checkpoint (epoch) inside that run ----\n",
    "checkpoint_patterns = [\"epoch_*.pt\", \"best.pt\", \"last.pt\"]\n",
    "checkpoints = []\n",
    "for pattern in checkpoint_patterns:\n",
    "       matches = sorted(target_run.glob(pattern))\n",
    "       if matches:\n",
    "           checkpoints.extend(matches)\n",
    "\n",
    "if not checkpoints:\n",
    "       raise FileNotFoundError(f\"No checkpoints found under {target_run}\")\n",
    "\n",
    "CHECKPOINT_NAME = \"best.pt\"  # or \"last.pt\", or None to take the last match\n",
    "if CHECKPOINT_NAME:\n",
    "       chosen_ckpt = target_run / CHECKPOINT_NAME\n",
    "       if not chosen_ckpt.exists():\n",
    "           raise FileNotFoundError(chosen_ckpt)\n",
    "else:\n",
    "       chosen_ckpt = checkpoints[-1]\n",
    "\n",
    "LATEST_CKPT = chosen_ckpt\n",
    "print(\"Using checkpoint:\", LATEST_CKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d7e3f8",
   "metadata": {},
   "source": [
    "## üîÆ 11.3 Generate predictions CSV\n",
    "\n",
    "- Uses the checkpoint above\n",
    "- Choose which split to predict on (`val` or `test`)\n",
    "- Saves CSV under `OUT_ROOT/preds/<split>/<out_tag>.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# üîß PREDICTION SETTINGS - Configure these for inference-only runs\n",
    "# ============================================================================\n",
    "PRED_SPLIT = \"test\"           # or \"val\"\n",
    "\n",
    "# Set to True and specify variant if using ROI-cropped data:\n",
    "USE_HYBRID = True             # Set False for full-image models\n",
    "# USE_YOLO = False            # Uncomment if using YOLO crops\n",
    "# USE_MEDIAPIPE = False       # Uncomment if using MediaPipe crops\n",
    "ROI_VARIANT = \"face_hands\"    # e.g., \"face_hands\", \"face\", \"hands\"\n",
    "\n",
    "# Hyperparameters (can be different from training for inference):\n",
    "BATCH_SIZE = 32               # Can be larger for inference (no gradients)\n",
    "NUM_WORKERS = 2               # Adjust based on your machine\n",
    "IMAGE_SIZE = 224              # Must match what the model was trained with\n",
    "\n",
    "# Model name - must match the architecture in your checkpoint:\n",
    "MODEL_NAME = \"efficientnet_b0\"  # e.g., \"convnext_tiny\", \"resnet50\", etc.\n",
    "\n",
    "# Infer RUN_TAG from checkpoint path if not already set:\n",
    "if \"RUN_TAG\" not in dir() or RUN_TAG is None:\n",
    "    # e.g., .../runs/convnext_tiny_full_v1/2024-12-14/best.pt -> convnext_tiny_full_v1\n",
    "    RUN_TAG = LATEST_CKPT.parent.parent.name\n",
    "    print(f\"Inferred RUN_TAG from checkpoint: {RUN_TAG}\")\n",
    "\n",
    "PRED_TAG = f\"{RUN_TAG}_{PRED_SPLIT}\"\n",
    "\n",
    "# ============================================================================\n",
    "# Build manifest/split paths based on data source\n",
    "# ============================================================================\n",
    "if \"USE_HYBRID\" in globals() and USE_HYBRID:\n",
    "    hybrid_root = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", Path(OUT_ROOT) / \"hybrid\"))\n",
    "    manifest_pred = hybrid_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    train_pred = hybrid_root / f\"train_{ROI_VARIANT}.csv\"\n",
    "    val_pred = hybrid_root / f\"val_{ROI_VARIANT}.csv\"\n",
    "    test_pred = hybrid_root / f\"test_{ROI_VARIANT}.csv\"\n",
    "    manifest_flag = f\"--manifest-csv {manifest_pred}\"\n",
    "    split_flag_str = f\"--train-csv {train_pred} --val-csv {val_pred} --test-csv {test_pred}\"\n",
    "    print(f\"üì¶ Using HYBRID crops: {ROI_VARIANT}\")\n",
    "elif \"USE_YOLO\" in globals() and USE_YOLO:\n",
    "    yolo_root = Path(os.environ.get(\"YOLO_ROOT_LOCAL\", Path(OUT_ROOT) / \"yolo\"))\n",
    "    manifest_pred = yolo_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    train_pred = yolo_root / f\"train_{ROI_VARIANT}.csv\"\n",
    "    val_pred = yolo_root / f\"val_{ROI_VARIANT}.csv\"\n",
    "    test_pred = yolo_root / f\"test_{ROI_VARIANT}.csv\"\n",
    "    manifest_flag = f\"--manifest-csv {manifest_pred}\"\n",
    "    split_flag_str = f\"--train-csv {train_pred} --val-csv {val_pred} --test-csv {test_pred}\"\n",
    "    print(f\"üì¶ Using YOLO crops: {ROI_VARIANT}\")\n",
    "elif \"USE_MEDIAPIPE\" in globals() and USE_MEDIAPIPE:\n",
    "    mp_root = Path(os.environ.get(\"MEDIAPIPE_ROOT_LOCAL\", Path(OUT_ROOT) / \"mediapipe\"))\n",
    "    manifest_pred = mp_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    train_pred = mp_root / f\"train_{ROI_VARIANT}.csv\"\n",
    "    val_pred = mp_root / f\"val_{ROI_VARIANT}.csv\"\n",
    "    test_pred = mp_root / f\"test_{ROI_VARIANT}.csv\"\n",
    "    manifest_flag = f\"--manifest-csv {manifest_pred}\"\n",
    "    split_flag_str = f\"--train-csv {train_pred} --val-csv {val_pred} --test-csv {test_pred}\"\n",
    "    print(f\"üì¶ Using MEDIAPIPE crops: {ROI_VARIANT}\")\n",
    "else:\n",
    "    manifest_pred = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "    train_pred = Path(OUT_ROOT) / \"splits\" / \"train.csv\"\n",
    "    val_pred = Path(OUT_ROOT) / \"splits\" / \"val.csv\"\n",
    "    test_pred = Path(OUT_ROOT) / \"splits\" / \"test.csv\"\n",
    "    manifest_flag = f\"--manifest-csv {manifest_pred}\"\n",
    "    split_flag_str = f\"--train-csv {train_pred} --val-csv {val_pred} --test-csv {test_pred}\"\n",
    "    print(\"üì¶ Using FULL images (no ROI cropping)\")\n",
    "\n",
    "# ============================================================================\n",
    "# CRITICAL: Set DATASET_ROOT to point to the correct data folder\n",
    "# For hybrid/YOLO/MediaPipe, images are in the crop output folder, not original data\n",
    "# ============================================================================\n",
    "import importlib\n",
    "from ddriver import config as _cfg\n",
    "\n",
    "if \"USE_HYBRID\" in globals() and USE_HYBRID:\n",
    "    # Use fast local copy if available (HYBRID_ROOT_LOCAL), otherwise fall back to Drive\n",
    "    _new_dataset_root = os.environ.get(\"HYBRID_ROOT_LOCAL\", str(Path(OUT_ROOT) / \"hybrid\"))\n",
    "    os.environ[\"DATASET_ROOT\"] = _new_dataset_root\n",
    "    importlib.reload(_cfg)\n",
    "    print(f\"üìÇ DATASET_ROOT ‚Üí {_new_dataset_root} (hybrid crops)\")\n",
    "elif \"USE_YOLO\" in globals() and USE_YOLO:\n",
    "    # Use fast local copy if available\n",
    "    _new_dataset_root = os.environ.get(\"YOLO_ROOT_LOCAL\", str(Path(OUT_ROOT) / \"yolo\"))\n",
    "    os.environ[\"DATASET_ROOT\"] = _new_dataset_root\n",
    "    importlib.reload(_cfg)\n",
    "    print(f\"üìÇ DATASET_ROOT ‚Üí {_new_dataset_root} (YOLO crops)\")\n",
    "elif \"USE_MEDIAPIPE\" in globals() and USE_MEDIAPIPE:\n",
    "    # Use fast local copy if available\n",
    "    _new_dataset_root = os.environ.get(\"MEDIAPIPE_ROOT_LOCAL\", str(Path(OUT_ROOT) / \"mediapipe\"))\n",
    "    os.environ[\"DATASET_ROOT\"] = _new_dataset_root\n",
    "    importlib.reload(_cfg)\n",
    "    print(f\"üìÇ DATASET_ROOT ‚Üí {_new_dataset_root} (MediaPipe crops)\")\n",
    "else:\n",
    "    print(f\"üìÇ DATASET_ROOT = {os.environ.get('DATASET_ROOT', 'not set')} (original images)\")\n",
    "\n",
    "predict_cmd = textwrap.dedent(f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.cli.predict \\\n",
    "    --model-name {MODEL_NAME} \\\n",
    "    --checkpoint {LATEST_CKPT} \\\n",
    "    --split {PRED_SPLIT} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --num-workers {NUM_WORKERS} \\\n",
    "    --image-size {IMAGE_SIZE} \\\n",
    "    --out-tag {PRED_TAG} \\\n",
    "    {manifest_flag} \\\n",
    "    {split_flag_str}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Running prediction command:\\n\", predict_cmd)\n",
    "result = subprocess.run(predict_cmd, shell=True, text=True, capture_output=True)\n",
    "if result.returncode != 0:\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "    raise RuntimeError(\"Prediction command failed. See logs above.\")\n",
    "print(result.stdout)\n",
    "print(\"\\n‚úÖ Predictions completed! Check OUT_ROOT/preds/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ca361",
   "metadata": {},
   "source": [
    "## üìä 11.4 Evaluate metrics\n",
    "\n",
    "- Uses `src/ddriver/metrics.py`\n",
    "- Reads the manifest + split CSV + predictions CSV\n",
    "- Saves results under `OUT_ROOT/metrics/<tag>/<timestamp>/`\n",
    "- Shows accuracy + macro F1 + per-driver/camera (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "# Uses settings from prediction cell (PRED_SPLIT, PRED_TAG, USE_HYBRID, ROI_VARIANT, etc.)\n",
    "# If you skipped prediction cell, set them here:\n",
    "# PRED_SPLIT = \"test\"\n",
    "# PRED_TAG = \"your_run_tag_test\"\n",
    "# USE_HYBRID = True\n",
    "# ROI_VARIANT = \"face_hands\"\n",
    "\n",
    "if \"USE_HYBRID\" in globals() and USE_HYBRID:\n",
    "    hybrid_root = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", Path(OUT_ROOT) / \"hybrid\"))\n",
    "    manifest_path = hybrid_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    split_csv_path = hybrid_root / f\"{PRED_SPLIT}_{ROI_VARIANT}.csv\"\n",
    "    preds_csv_path = Path(OUT_ROOT) / \"preds\" / PRED_SPLIT / f\"{PRED_TAG}.csv\"\n",
    "elif \"USE_YOLO\" in globals() and USE_YOLO:\n",
    "    yolo_root = Path(os.environ.get(\"YOLO_ROOT_LOCAL\", Path(OUT_ROOT) / \"yolo\"))\n",
    "    manifest_path = yolo_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    split_csv_path = yolo_root / f\"{PRED_SPLIT}_{ROI_VARIANT}.csv\"\n",
    "    preds_csv_path = Path(OUT_ROOT) / \"preds\" / PRED_SPLIT / f\"{PRED_TAG}.csv\"\n",
    "elif \"USE_MEDIAPIPE\" in globals() and USE_MEDIAPIPE:\n",
    "    mp_root = Path(os.environ.get(\"MEDIAPIPE_ROOT_LOCAL\", Path(OUT_ROOT) / \"mediapipe\"))\n",
    "    manifest_path = mp_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    split_csv_path = mp_root / f\"{PRED_SPLIT}_{ROI_VARIANT}.csv\"\n",
    "    preds_csv_path = Path(OUT_ROOT) / \"preds\" / PRED_SPLIT / f\"{PRED_TAG}.csv\"\n",
    "else:\n",
    "    manifest_path = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "    split_csv_path = Path(OUT_ROOT) / \"splits\" / f\"{PRED_SPLIT}.csv\"\n",
    "    preds_csv_path = Path(OUT_ROOT) / \"preds\" / PRED_SPLIT / f\"{PRED_TAG}.csv\"\n",
    "METRICS_TAG = PRED_TAG\n",
    "\n",
    "print(f\"üìä Evaluating: {preds_csv_path}\")\n",
    "\n",
    "metrics_cmd = textwrap.dedent(f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.eval.metrics \\\n",
    "    --manifest {manifest_path} \\\n",
    "    --split-csv {split_csv_path} \\\n",
    "    --predictions {preds_csv_path} \\\n",
    "    --out-tag {METRICS_TAG} \\\n",
    "    --per-driver \\\n",
    "    --per-camera\n",
    "\"\"\")\n",
    "\n",
    "print(\"Running metrics command:\\n\", metrics_cmd)\n",
    "result = subprocess.run(metrics_cmd, shell=True, text=True, capture_output=True)\n",
    "if result.returncode != 0:\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "    raise RuntimeError(\"Metrics command failed. See logs above.\")\n",
    "print(result.stdout)\n",
    "print(\"\\n‚úÖ Metrics saved under OUT_ROOT/metrics/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1047bf",
   "metadata": {},
   "source": [
    "### ‚úÖ You're all set!\n",
    "\n",
    "**What just happened:**\n",
    "1. ‚úÖ Mounted Google Drive\n",
    "2. ‚úÖ Cloned/updated your repo\n",
    "3. ‚úÖ Installed the package\n",
    "4. ‚úÖ Set up paths (works on Colab and Mac!)\n",
    "5. ‚úÖ Generated manifest.csv and train/val/test split CSVs\n",
    "6. ‚úÖ Tested that dataset.py can load images\n",
    "7. ‚úÖ Tested that datamod.py can create data loaders\n",
    "8. ‚úÖ (Optional) Registered a model + ran training ‚Üí prediction ‚Üí metrics pipeline\n",
    "\n",
    "**Your CSVs are saved in Google Drive:**\n",
    "- `OUT_ROOT/manifests/manifest.csv` - Big list of all images\n",
    "- `OUT_ROOT/splits/train.csv` - Training images\n",
    "- `OUT_ROOT/splits/val.csv` - Validation images (with driver IDs!)\n",
    "- `OUT_ROOT/splits/test.csv` - Test images\n",
    "\n",
    "**Next steps:**\n",
    "- Adjust the training/prediction cells (epochs, batch size, tags) to run bigger experiments\n",
    "- All paths use `ddriver.config` so it works on Colab and Mac\n",
    "- Re-run **Clone/Update** cell after pushing new commits\n",
    "- Optional: copy some data into `/content/data` to use `FAST_DATA` for speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Colab cell: append metrics + params to Google Sheet ----\n",
    "!pip -q install gspread\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import gspread\n",
    "from google.colab import auth\n",
    "import google.auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "creds, _ = google.auth.default()\n",
    "gc = gspread.authorize(creds)\n",
    "\n",
    "EVAL_SHEET_NAME = \"TFM Eval Logs\"   # create this sheet/tab ahead of time\n",
    "EVAL_WORKSHEET = \"Sheet1\"\n",
    "\n",
    "METRICS_TAG = (\n",
    "    globals().get(\"METRICS_TAG\")\n",
    "    or globals().get(\"PRED_TAG\")\n",
    "    or \"convnext_tiny_full_v1_val\"\n",
    ")  # match the --out-tag you used\n",
    "metrics_root = Path(OUT_ROOT) / \"metrics\" / METRICS_TAG\n",
    "runs = sorted(metrics_root.glob(\"*/\"))\n",
    "if not runs:\n",
    "    raise FileNotFoundError(f\"No metrics runs found under {metrics_root}\")\n",
    "latest_metrics = runs[-1]\n",
    "print(\"Logging metrics folder:\", latest_metrics)\n",
    "\n",
    "def _read_json(path: Path, *, required: bool = True) -> dict:\n",
    "    if not path.exists():\n",
    "        if required:\n",
    "            raise FileNotFoundError(f\"Expected file missing: {path}\")\n",
    "        return {}\n",
    "    return json.loads(path.read_text())\n",
    "\n",
    "metrics = _read_json(latest_metrics / \"metrics.json\")\n",
    "inputs = _read_json(latest_metrics / \"inputs.json\", required=False)\n",
    "params = _read_json(latest_metrics / \"params.json\", required=False)\n",
    "\n",
    "overall = metrics.get(\"overall\", {})\n",
    "macro = overall.get(\"macro_avg\", {})\n",
    "\n",
    "row = [\n",
    "    str(latest_metrics),\n",
    "    inputs.get(\"predictions\", \"\"),\n",
    "    inputs.get(\"split_source\", \"\"),\n",
    "    metrics.get(\"num_examples\", \"\"),\n",
    "    round(overall.get(\"accuracy\", 0.0), 4),\n",
    "    round(macro.get(\"f1\", 0.0), 4),\n",
    "    json.dumps(params, sort_keys=True)[:500],\n",
    "]\n",
    "\n",
    "ws = gc.open(EVAL_SHEET_NAME).worksheet(EVAL_WORKSHEET)\n",
    "ws.append_row(row, value_input_option=\"USER_ENTERED\")\n",
    "print(f\"Appended metrics run {latest_metrics.name} to {EVAL_SHEET_NAME}/{EVAL_WORKSHEET} ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ddb94",
   "metadata": {},
   "source": [
    "### üìä 11.4a Visualize Confusion Matrix\n",
    "\n",
    "Quick peek at where the model confuses classes using the most recent metrics run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c65d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "METRICS_TAG = (\n",
    "    globals().get(\"METRICS_TAG\")\n",
    "    or globals().get(\"PRED_TAG\")\n",
    "    or \"convnext_tiny_full_v1_val\"\n",
    ")  # change if you used a different --out-tag\n",
    "metrics_root = Path(OUT_ROOT) / \"metrics\" / METRICS_TAG\n",
    "runs = sorted(metrics_root.glob(\"*/\"))\n",
    "if not runs:\n",
    "    raise FileNotFoundError(f\"No metrics runs found under {metrics_root}\")\n",
    "latest_metrics = runs[-1]\n",
    "print(\"Reading confusion matrix from:\", latest_metrics)\n",
    "\n",
    "metrics = json.loads((latest_metrics / \"metrics.json\").read_text())\n",
    "cm_info = metrics.get(\"confusion_matrix\")\n",
    "if not cm_info:\n",
    "    raise ValueError(\"confusion_matrix missing from metrics.json\")\n",
    "\n",
    "labels = cm_info[\"rows_cols_labels\"]\n",
    "cm_df = pd.DataFrame(cm_info[\"matrix\"], index=labels, columns=labels)\n",
    "\n",
    "counts_path = latest_metrics / \"confusion_matrix_counts.png\"\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(f\"Confusion matrix ‚Äì {METRICS_TAG}\")\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(counts_path)\n",
    "plt.show()\n",
    "print(\"Saved counts heatmap to\", counts_path)\n",
    "\n",
    "cm_norm = cm_df.div(cm_df.sum(axis=1).replace(0, 1), axis=0)\n",
    "norm_path = latest_metrics / \"confusion_matrix_normalized.png\"\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "plt.title(f\"Normalized confusion matrix ‚Äì {METRICS_TAG}\")\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(norm_path)\n",
    "plt.show()\n",
    "print(\"Saved normalized heatmap to\", norm_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb470c46",
   "metadata": {},
   "source": [
    "## üß™ 11.4b Subset-Only Evaluation (For Ablations)\n",
    "\n",
    "For face-only or hands-only models, evaluate **only on images where detection succeeded**.\n",
    "\n",
    "This gives a fair comparison: \"What can face-only do on the 90% of images where face detection worked?\"\n",
    "\n",
    "Use this cell after running predictions on a face-only or hands-only model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb999cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Subset-Only Evaluation for Ablation Models\n",
    "# Filters predictions to only images where the relevant modality was detected\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Set these based on which ablation you're evaluating\n",
    "# ============================================================================\n",
    "ABLATION_TYPE = \"face\"  # \"face\" or \"hands\" - which modality this model uses\n",
    "PRED_CSV = None  # Set to path, or None to use the latest from OUT_ROOT\n",
    "\n",
    "# Where is the detection metadata?\n",
    "HYBRID_ROOT = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", Path(OUT_ROOT) / \"hybrid\"))\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DETECTION METADATA\n",
    "# ============================================================================\n",
    "# Try to find metadata that has detection status per image\n",
    "metadata_candidates = [\n",
    "    HYBRID_ROOT / f\"detection_metadata_face_hands.csv\",  # Most complete\n",
    "    HYBRID_ROOT / f\"detection_metadata_{ABLATION_TYPE}.csv\",\n",
    "]\n",
    "\n",
    "metadata_df = None\n",
    "for mf in metadata_candidates:\n",
    "    if mf.exists():\n",
    "        metadata_df = pd.read_csv(mf)\n",
    "        print(f\"‚úÖ Loaded detection metadata from: {mf}\")\n",
    "        print(f\"   Columns: {list(metadata_df.columns)}\")\n",
    "        break\n",
    "\n",
    "if metadata_df is None:\n",
    "    print(\"‚ùå No detection metadata found. Cannot do subset-only evaluation.\")\n",
    "    print(\"   Run hybrid extraction first to generate detection_metadata_*.csv\")\n",
    "else:\n",
    "    # ============================================================================\n",
    "    # LOAD PREDICTIONS\n",
    "    # ============================================================================\n",
    "    if PRED_CSV is None:\n",
    "        # Try to find the latest prediction file\n",
    "        pred_dir = Path(OUT_ROOT) / \"preds\" / \"test\"\n",
    "        pred_files = sorted(pred_dir.glob(\"*.csv\"), key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "        if pred_files:\n",
    "            PRED_CSV = pred_files[0]\n",
    "            print(f\"üìÑ Using latest predictions: {PRED_CSV}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No prediction files found in {pred_dir}\")\n",
    "    \n",
    "    preds_df = pd.read_csv(PRED_CSV)\n",
    "    print(f\"   Total predictions: {len(preds_df)}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # FILTER TO DETECTED SUBSET\n",
    "    # ============================================================================\n",
    "    # Extract filename from prediction paths to match with metadata\n",
    "    preds_df[\"_filename\"] = preds_df[\"path\"].apply(lambda p: Path(p).name)\n",
    "    \n",
    "    # Also extract from metadata\n",
    "    if \"source_path\" in metadata_df.columns:\n",
    "        metadata_df[\"_filename\"] = metadata_df[\"source_path\"].apply(lambda p: Path(p).name)\n",
    "    elif \"path\" in metadata_df.columns:\n",
    "        metadata_df[\"_filename\"] = metadata_df[\"path\"].apply(lambda p: Path(p).name)\n",
    "    \n",
    "    # Determine which column indicates detection success\n",
    "    if ABLATION_TYPE == \"face\":\n",
    "        det_col = \"face_detected\" if \"face_detected\" in metadata_df.columns else None\n",
    "    else:  # hands\n",
    "        if \"hands_detected\" in metadata_df.columns:\n",
    "            det_col = \"hands_detected\"\n",
    "        elif \"left_hand_detected\" in metadata_df.columns:\n",
    "            metadata_df[\"hands_detected\"] = metadata_df[\"left_hand_detected\"] | metadata_df[\"right_hand_detected\"]\n",
    "            det_col = \"hands_detected\"\n",
    "        else:\n",
    "            det_col = None\n",
    "    \n",
    "    if det_col is None:\n",
    "        print(f\"‚ùå Cannot find {ABLATION_TYPE} detection column in metadata\")\n",
    "    else:\n",
    "        # Get list of filenames where detection succeeded\n",
    "        # Get list of filenames where detection succeeded\n",
    "        detected_df = metadata_df[metadata_df[det_col] == True].copy()\n",
    "        \n",
    "        # ADDITIONAL FILTER: For hands-only or face-only, exclude crops that are too large\n",
    "        # (they likely accidentally include other modalities due to over-padding)\n",
    "        MAX_AREA_FRAC = 0.50  # Exclude crops > 50% of image (tune as needed)\n",
    "        detected_df = detected_df[\n",
    "            (detected_df[\"roi_area_frac\"] <= MAX_AREA_FRAC) & \n",
    "            (detected_df[\"fallback_to_full\"] == False)\n",
    "        ]\n",
    "        print(f\"   Filtered: excluding crops with roi_area_frac > {MAX_AREA_FRAC}\")\n",
    "        print(f\"   Filtered: excluding fallback_to_full=True\")\n",
    "        n_before = len(metadata_df[metadata_df[det_col] == True])\n",
    "        n_after = len(detected_df)\n",
    "        print(f\"   After size filtering: {n_after}/{n_before} images ({100*n_after/n_before:.1f}%)\")\n",
    "        \n",
    "        detected_filenames = set(detected_df[\"_filename\"])\n",
    "        # Filter predictions\n",
    "        preds_subset = preds_df[preds_df[\"_filename\"].isin(detected_filenames)]\n",
    "        \n",
    "        print(f\"\\nüìä SUBSET-ONLY EVALUATION ({ABLATION_TYPE.upper()})\")\n",
    "        print(f\"=\"*50)\n",
    "        print(f\"   Full test set: {len(preds_df)} images\")\n",
    "        print(f\"   {ABLATION_TYPE.title()} detected: {len(preds_subset)} images ({100*len(preds_subset)/len(preds_df):.1f}%)\")\n",
    "        \n",
    "        # ============================================================================\n",
    "        # COMPUTE METRICS ON SUBSET\n",
    "        # ============================================================================\n",
    "        # Need ground truth labels - extract from path or merge with manifest\n",
    "        if \"label\" not in preds_subset.columns and \"class_id\" not in preds_subset.columns:\n",
    "            # Extract class from path (assuming c0/, c1/, etc. in path)\n",
    "            def extract_class(path):\n",
    "                parts = Path(path).parts\n",
    "                for p in reversed(parts):\n",
    "                    if p.startswith(\"c\") and len(p) == 2 and p[1].isdigit():\n",
    "                        return int(p[1])\n",
    "                return -1\n",
    "            preds_subset = preds_subset.copy()\n",
    "            preds_subset[\"label\"] = preds_subset[\"path\"].apply(extract_class)\n",
    "        elif \"class_id\" in preds_subset.columns:\n",
    "            preds_subset = preds_subset.copy()\n",
    "            preds_subset[\"label\"] = preds_subset[\"class_id\"].apply(\n",
    "                lambda x: int(x[1]) if isinstance(x, str) and x.startswith(\"c\") else int(x)\n",
    "            )\n",
    "        \n",
    "        # Convert predictions to int\n",
    "        if \"pred\" not in preds_subset.columns:\n",
    "            preds_subset[\"pred\"] = preds_subset[\"pred_class_id\"].apply(\n",
    "                lambda x: int(x[1]) if isinstance(x, str) and x.startswith(\"c\") else int(x)\n",
    "            )\n",
    "        \n",
    "        # Compute accuracy\n",
    "        valid_preds = preds_subset[preds_subset[\"label\"] >= 0]\n",
    "        accuracy = accuracy_score(valid_preds[\"label\"], valid_preds[\"pred\"])\n",
    "        \n",
    "        print(f\"\\n   SUBSET ACCURACY: {accuracy*100:.2f}%\")\n",
    "        print(f\"\\n   Per-class breakdown:\")\n",
    "        print(classification_report(valid_preds[\"label\"], valid_preds[\"pred\"], \n",
    "                                   target_names=[f\"c{i}\" for i in range(10)]))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(valid_preds[\"label\"], valid_preds[\"pred\"])\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "                   xticklabels=[f\"c{i}\" for i in range(10)],\n",
    "                   yticklabels=[f\"c{i}\" for i in range(10)])\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_ylabel(\"True\")\n",
    "        ax.set_title(f\"Confusion Matrix: {ABLATION_TYPE.title()}-Only Model (Subset Evaluation)\\n\"\n",
    "                    f\"Accuracy: {accuracy*100:.1f}% on {len(valid_preds)}/{len(preds_df)} images\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save\n",
    "        out_path = Path(OUT_ROOT) / \"metrics\" / f\"confusion_{ABLATION_TYPE}_only_subset.png\"\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(out_path, dpi=150)\n",
    "        plt.show()\n",
    "        print(f\"\\nüíæ Saved to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd887d43",
   "metadata": {},
   "source": [
    "## üìä 11.4c Per-Class Modality Comparison (Optional)\n",
    "\n",
    "Compare face-only, hands-only, and face+hands performance **per class** to answer:\n",
    "- Which classes are **hands-dominated**? (texting, drinking, radio)\n",
    "- Which classes are **face-dominated**? (talking to passenger, phone call)\n",
    "- Which classes need **both modalities**?\n",
    "\n",
    "**Prerequisites:** Run predictions for all three model variants first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Per-Class Modality Comparison: Face vs Hands vs Face+Hands\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Set prediction file paths for each model variant\n",
    "# ============================================================================\n",
    "# Set these to your actual prediction CSV files (or None if not available yet)\n",
    "FACE_ONLY_PREDS = None  # e.g., Path(OUT_ROOT) / \"preds/test/effb0_face_only_V1_test.csv\"\n",
    "HANDS_ONLY_PREDS = None  # e.g., Path(OUT_ROOT) / \"preds/test/effb0_hands_only_V1_test.csv\"\n",
    "FACE_HANDS_PREDS = None  # e.g., Path(OUT_ROOT) / \"preds/test/effb0_face_hands_V1_test.csv\"\n",
    "\n",
    "# Auto-detect from OUT_ROOT if not specified\n",
    "if FACE_HANDS_PREDS is None:\n",
    "    candidates = list(Path(OUT_ROOT).glob(\"preds/test/*face_hands*.csv\"))\n",
    "    if candidates:\n",
    "        FACE_HANDS_PREDS = sorted(candidates, key=lambda x: x.stat().st_mtime, reverse=True)[0]\n",
    "        print(f\"üìÅ Auto-detected face+hands predictions: {FACE_HANDS_PREDS.name}\")\n",
    "\n",
    "CLASS_NAMES = {\n",
    "    0: \"Safe driving\",\n",
    "    1: \"Texting (R)\",\n",
    "    2: \"Phone (R)\",\n",
    "    3: \"Texting (L)\",\n",
    "    4: \"Phone (L)\",\n",
    "    5: \"Radio\",\n",
    "    6: \"Drinking\",\n",
    "    7: \"Reaching back\",\n",
    "    8: \"Hair/makeup\",\n",
    "    9: \"Passenger\",\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTION\n",
    "# ============================================================================\n",
    "def load_and_compute_per_class_acc(pred_path, name):\n",
    "    \"\"\"Load predictions and compute per-class accuracy.\"\"\"\n",
    "    if pred_path is None or not Path(pred_path).exists():\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(pred_path)\n",
    "    \n",
    "    # Extract true label from path\n",
    "    def extract_class(path):\n",
    "        parts = Path(path).parts\n",
    "        for p in reversed(parts):\n",
    "            if p.startswith(\"c\") and len(p) == 2 and p[1].isdigit():\n",
    "                return int(p[1])\n",
    "        return -1\n",
    "    \n",
    "    df[\"true\"] = df[\"path\"].apply(extract_class)\n",
    "    df[\"pred\"] = df[\"pred_class_id\"].apply(\n",
    "        lambda x: int(x[1]) if isinstance(x, str) and x.startswith(\"c\") else int(x)\n",
    "    )\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    results = {}\n",
    "    for c in range(10):\n",
    "        class_df = df[df[\"true\"] == c]\n",
    "        if len(class_df) > 0:\n",
    "            acc = (class_df[\"true\"] == class_df[\"pred\"]).mean()\n",
    "            results[c] = {\"accuracy\": acc, \"count\": len(class_df)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD ALL PREDICTIONS\n",
    "# ============================================================================\n",
    "print(\"üìä Loading predictions for comparison...\\n\")\n",
    "\n",
    "modalities = {}\n",
    "if FACE_ONLY_PREDS:\n",
    "    modalities[\"Face Only\"] = load_and_compute_per_class_acc(FACE_ONLY_PREDS, \"face\")\n",
    "if HANDS_ONLY_PREDS:\n",
    "    modalities[\"Hands Only\"] = load_and_compute_per_class_acc(HANDS_ONLY_PREDS, \"hands\")\n",
    "if FACE_HANDS_PREDS:\n",
    "    modalities[\"Face+Hands\"] = load_and_compute_per_class_acc(FACE_HANDS_PREDS, \"face_hands\")\n",
    "\n",
    "if len(modalities) < 2:\n",
    "    print(\"‚ö†Ô∏è  Need at least 2 model variants to compare.\")\n",
    "    print(\"   Set FACE_ONLY_PREDS, HANDS_ONLY_PREDS, and/or FACE_HANDS_PREDS above.\")\n",
    "    print(\"\\n   Currently available predictions:\")\n",
    "    for f in Path(OUT_ROOT).glob(\"preds/test/*.csv\"):\n",
    "        print(f\"     - {f.name}\")\n",
    "else:\n",
    "    # ============================================================================\n",
    "    # CREATE COMPARISON TABLE\n",
    "    # ============================================================================\n",
    "    print(\"=\"*70)\n",
    "    print(\"PER-CLASS ACCURACY COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Build comparison dataframe\n",
    "    rows = []\n",
    "    for c in range(10):\n",
    "        row = {\"Class\": f\"c{c}\", \"Name\": CLASS_NAMES.get(c, f\"Class {c}\")}\n",
    "        for mod_name, mod_data in modalities.items():\n",
    "            if mod_data and c in mod_data:\n",
    "                row[mod_name] = f\"{mod_data[c]['accuracy']*100:.1f}%\"\n",
    "            else:\n",
    "                row[mod_name] = \"N/A\"\n",
    "        rows.append(row)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(rows)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # DETERMINE MODALITY DOMINANCE\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODALITY DOMINANCE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if \"Face Only\" in modalities and \"Hands Only\" in modalities:\n",
    "        for c in range(10):\n",
    "            face_acc = modalities[\"Face Only\"].get(c, {}).get(\"accuracy\", 0) if modalities[\"Face Only\"] else 0\n",
    "            hands_acc = modalities[\"Hands Only\"].get(c, {}).get(\"accuracy\", 0) if modalities[\"Hands Only\"] else 0\n",
    "            \n",
    "            diff = (hands_acc - face_acc) * 100\n",
    "            if abs(diff) < 5:\n",
    "                dominance = \"‚öñÔ∏è  BOTH (similar)\"\n",
    "            elif diff > 0:\n",
    "                dominance = f\"üñêÔ∏è HANDS (+{diff:.1f}pp)\"\n",
    "            else:\n",
    "                dominance = f\"üë§ FACE (+{-diff:.1f}pp)\"\n",
    "            \n",
    "            print(f\"  {CLASS_NAMES.get(c, f'c{c}'):20s}: {dominance}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # VISUALIZATION\n",
    "    # ============================================================================\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    x = np.arange(10)\n",
    "    width = 0.25\n",
    "    offset = 0\n",
    "    \n",
    "    colors = {\"Face Only\": \"#FF6B6B\", \"Hands Only\": \"#4ECDC4\", \"Face+Hands\": \"#45B7D1\"}\n",
    "    \n",
    "    for mod_name, mod_data in modalities.items():\n",
    "        if mod_data:\n",
    "            accs = [mod_data.get(c, {}).get(\"accuracy\", 0) * 100 for c in range(10)]\n",
    "            ax.bar(x + offset, accs, width, label=mod_name, color=colors.get(mod_name, \"gray\"))\n",
    "            offset += width\n",
    "    \n",
    "    ax.set_xlabel(\"Class\")\n",
    "    ax.set_ylabel(\"Accuracy (%)\")\n",
    "    ax.set_title(\"Per-Class Accuracy: Face-Only vs Hands-Only vs Face+Hands\")\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels([CLASS_NAMES.get(c, f\"c{c}\") for c in range(10)], rotation=45, ha=\"right\")\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.axhline(y=50, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    out_path = Path(OUT_ROOT) / \"metrics\" / \"modality_comparison_per_class.png\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"\\nüíæ Saved to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf0d477",
   "metadata": {},
   "source": [
    "## üìä 11.4d Side-by-Side Confusion Matrix Comparison (Face vs Hands)\n",
    "\n",
    "Compare confusion matrices for face-only vs hands-only models to see:\n",
    "- **Which class pairs get confused** with each modality\n",
    "- **Asymmetric confusions** (e.g., texting‚Üíphone but not phone‚Üítexting)\n",
    "- **Modality-specific weaknesses** (hands model confuses drinking/reaching, face model confuses phone poses)\n",
    "\n",
    "**Prerequisites:** Run predictions for both face-only and hands-only models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Side-by-Side Confusion Matrix Comparison: Face vs Hands\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Set prediction file paths\n",
    "# ============================================================================\n",
    "FACE_ONLY_PREDS = None  # e.g., Path(OUT_ROOT) / \"preds/test/effb0_face_only_V1_test.csv\"\n",
    "HANDS_ONLY_PREDS = None  # e.g., Path(OUT_ROOT) / \"preds/test/effb0_hands_only_V1_test.csv\"\n",
    "\n",
    "# Try to auto-detect\n",
    "for f in Path(OUT_ROOT).glob(\"preds/test/*.csv\"):\n",
    "    fname = f.name.lower()\n",
    "    if \"face_only\" in fname and FACE_ONLY_PREDS is None:\n",
    "        FACE_ONLY_PREDS = f\n",
    "        print(f\"üìÅ Auto-detected face-only: {f.name}\")\n",
    "    elif \"hands_only\" in fname and HANDS_ONLY_PREDS is None:\n",
    "        HANDS_ONLY_PREDS = f\n",
    "        print(f\"üìÅ Auto-detected hands-only: {f.name}\")\n",
    "\n",
    "CLASS_NAMES_SHORT = [\"Safe\", \"TxtR\", \"PhR\", \"TxtL\", \"PhL\", \"Radio\", \"Drink\", \"Reach\", \"Hair\", \"Pass\"]\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "def load_predictions_with_labels(pred_path):\n",
    "    \"\"\"Load predictions and extract true labels from paths.\"\"\"\n",
    "    df = pd.read_csv(pred_path)\n",
    "    \n",
    "    def extract_class(path):\n",
    "        parts = Path(path).parts\n",
    "        for p in reversed(parts):\n",
    "            if p.startswith(\"c\") and len(p) == 2 and p[1].isdigit():\n",
    "                return int(p[1])\n",
    "        return -1\n",
    "    \n",
    "    df[\"true\"] = df[\"path\"].apply(extract_class)\n",
    "    df[\"pred\"] = df[\"pred_class_id\"].apply(\n",
    "        lambda x: int(x[1]) if isinstance(x, str) and x.startswith(\"c\") else int(x)\n",
    "    )\n",
    "    return df[df[\"true\"] >= 0]\n",
    "\n",
    "def compute_confusion_diff(cm1, cm2):\n",
    "    \"\"\"Compute difference matrix: positive = cm1 is worse (more confusions).\"\"\"\n",
    "    # Normalize by row (per-class)\n",
    "    cm1_norm = cm1.astype(float) / cm1.sum(axis=1, keepdims=True)\n",
    "    cm2_norm = cm2.astype(float) / cm2.sum(axis=1, keepdims=True)\n",
    "    # Replace NaN with 0\n",
    "    cm1_norm = np.nan_to_num(cm1_norm)\n",
    "    cm2_norm = np.nan_to_num(cm2_norm)\n",
    "    return cm1_norm - cm2_norm\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD AND COMPUTE\n",
    "# ============================================================================\n",
    "if FACE_ONLY_PREDS is None or HANDS_ONLY_PREDS is None:\n",
    "    print(\"‚ö†Ô∏è  Need both face-only AND hands-only prediction files.\")\n",
    "    print(\"\\n   Set FACE_ONLY_PREDS and HANDS_ONLY_PREDS above, or train the models first.\")\n",
    "    print(\"\\n   Currently available predictions:\")\n",
    "    for f in Path(OUT_ROOT).glob(\"preds/test/*.csv\"):\n",
    "        print(f\"     - {f.name}\")\n",
    "else:\n",
    "    print(\"üìä Loading predictions...\\n\")\n",
    "    \n",
    "    face_df = load_predictions_with_labels(FACE_ONLY_PREDS)\n",
    "    hands_df = load_predictions_with_labels(HANDS_ONLY_PREDS)\n",
    "    \n",
    "    # Compute confusion matrices\n",
    "    cm_face = confusion_matrix(face_df[\"true\"], face_df[\"pred\"], labels=range(10))\n",
    "    cm_hands = confusion_matrix(hands_df[\"true\"], hands_df[\"pred\"], labels=range(10))\n",
    "    \n",
    "    # Accuracy\n",
    "    acc_face = (face_df[\"true\"] == face_df[\"pred\"]).mean()\n",
    "    acc_hands = (hands_df[\"true\"] == hands_df[\"pred\"]).mean()\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SIDE-BY-SIDE VISUALIZATION\n",
    "    # ============================================================================\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # Face confusion matrix\n",
    "    sns.heatmap(cm_face, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0],\n",
    "                xticklabels=CLASS_NAMES_SHORT, yticklabels=CLASS_NAMES_SHORT)\n",
    "    axes[0].set_title(f\"Face-Only Model\\nAccuracy: {acc_face*100:.1f}%\", fontsize=12)\n",
    "    axes[0].set_xlabel(\"Predicted\")\n",
    "    axes[0].set_ylabel(\"True\")\n",
    "    \n",
    "    # Hands confusion matrix\n",
    "    sns.heatmap(cm_hands, annot=True, fmt=\"d\", cmap=\"Greens\", ax=axes[1],\n",
    "                xticklabels=CLASS_NAMES_SHORT, yticklabels=CLASS_NAMES_SHORT)\n",
    "    axes[1].set_title(f\"Hands-Only Model\\nAccuracy: {acc_hands*100:.1f}%\", fontsize=12)\n",
    "    axes[1].set_xlabel(\"Predicted\")\n",
    "    axes[1].set_ylabel(\"True\")\n",
    "    \n",
    "    # Difference matrix (Face - Hands): Red = Face is worse, Blue = Hands is worse\n",
    "    diff = compute_confusion_diff(cm_face, cm_hands)\n",
    "    sns.heatmap(diff * 100, annot=True, fmt=\".1f\", cmap=\"RdBu_r\", center=0, ax=axes[2],\n",
    "                xticklabels=CLASS_NAMES_SHORT, yticklabels=CLASS_NAMES_SHORT,\n",
    "                vmin=-20, vmax=20)\n",
    "    axes[2].set_title(\"Difference (Face ‚àí Hands)\\nRed = Face worse, Blue = Hands worse\", fontsize=12)\n",
    "    axes[2].set_xlabel(\"Predicted\")\n",
    "    axes[2].set_ylabel(\"True\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    out_path = Path(OUT_ROOT) / \"metrics\" / \"confusion_face_vs_hands_comparison.png\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"\\nüíæ Saved to {out_path}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # TOP CONFUSION PAIRS ANALYSIS\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîç TOP CONFUSION PAIRS BY MODALITY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    CLASS_NAMES_FULL = {\n",
    "        0: \"Safe driving\", 1: \"Texting (R)\", 2: \"Phone (R)\", 3: \"Texting (L)\",\n",
    "        4: \"Phone (L)\", 5: \"Radio\", 6: \"Drinking\", 7: \"Reaching back\",\n",
    "        8: \"Hair/makeup\", 9: \"Passenger\"\n",
    "    }\n",
    "    \n",
    "    def get_top_confusions(cm, top_n=5):\n",
    "        \"\"\"Get top N off-diagonal confusions.\"\"\"\n",
    "        confusions = []\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                if i != j and cm[i, j] > 0:\n",
    "                    confusions.append((i, j, cm[i, j]))\n",
    "        return sorted(confusions, key=lambda x: -x[2])[:top_n]\n",
    "    \n",
    "    print(\"\\nüë§ FACE-ONLY Top Confusions:\")\n",
    "    for true_c, pred_c, count in get_top_confusions(cm_face):\n",
    "        print(f\"   {CLASS_NAMES_FULL[true_c]:20s} ‚Üí {CLASS_NAMES_FULL[pred_c]:20s}: {count} errors\")\n",
    "    \n",
    "    print(\"\\nüñêÔ∏è HANDS-ONLY Top Confusions:\")\n",
    "    for true_c, pred_c, count in get_top_confusions(cm_hands):\n",
    "        print(f\"   {CLASS_NAMES_FULL[true_c]:20s} ‚Üí {CLASS_NAMES_FULL[pred_c]:20s}: {count} errors\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # MODALITY-SPECIFIC INSIGHTS\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üí° MODALITY-SPECIFIC INSIGHTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Find pairs where one modality is much better\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if i != j:\n",
    "                face_rate = cm_face[i, j] / cm_face[i].sum() if cm_face[i].sum() > 0 else 0\n",
    "                hands_rate = cm_hands[i, j] / cm_hands[i].sum() if cm_hands[i].sum() > 0 else 0\n",
    "                \n",
    "                diff_pct = (face_rate - hands_rate) * 100\n",
    "                if abs(diff_pct) > 5:  # 5% difference threshold\n",
    "                    if diff_pct > 0:\n",
    "                        print(f\"   üñêÔ∏è Hands better at: {CLASS_NAMES_FULL[i][:15]:15s} vs {CLASS_NAMES_FULL[j][:15]:15s} (+{diff_pct:.1f}pp)\")\n",
    "                    else:\n",
    "                        print(f\"   üë§ Face better at:  {CLASS_NAMES_FULL[i][:15]:15s} vs {CLASS_NAMES_FULL[j][:15]:15s} (+{-diff_pct:.1f}pp)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c11b4c",
   "metadata": {},
   "source": [
    "## üî• 11.5 Grad-CAM Visualizations\n",
    "\n",
    "**Grad-CAM (Gradient-weighted Class Activation Mapping)** shows which regions of the image the model focuses on when making predictions.\n",
    "\n",
    "**Use cases:**\n",
    "- **Full image models:** Verify the model looks at face/hands, not background\n",
    "- **Hybrid crop models:** See which specific features (hand position, facial expression) matter most\n",
    "- **Thesis comparison:** Visual evidence of WHY ROI cropping helps\n",
    "\n",
    "This cell generates:\n",
    "1. Grad-CAM heatmaps for sample images\n",
    "2. Comparison of correct vs misclassified predictions\n",
    "3. Per-class attention patterns\n",
    "4. Saved visualizations for your thesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• Grad-CAM Visualization\n",
    "!pip -q install grad-cam\n",
    "\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision import transforms\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# Derive tag from checkpoint that was already selected in cell 11.2\n",
    "if \"LATEST_CKPT\" in dir() and LATEST_CKPT:\n",
    "    GRADCAM_TAG = Path(LATEST_CKPT).parent.parent.name  # e.g., effb0_InsightFace_MP_face_hands_V3\n",
    "else:\n",
    "    GRADCAM_TAG = globals().get(\"TRAIN_TAG\") or \"effb0_hybrid_face_hands\"\n",
    "\n",
    "MODEL_NAME = globals().get(\"MODEL_NAME\") or \"efficientnet_b0\"  # efficientnet_b0, resnet18, convnext_tiny\n",
    "# Use the same split as predictions (PRED_SPLIT), or default to \"test\"\n",
    "SPLIT_TO_ANALYZE = globals().get(\"PRED_SPLIT\") or \"test\"\n",
    "N_SAMPLES_PER_CLASS = 3  # How many samples per class to visualize\n",
    "N_MISCLASSIFIED = 6  # How many misclassified examples to show\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# ===== FIND CHECKPOINT =====\n",
    "# Use LATEST_CKPT if already selected (from cell 11.2), otherwise search\n",
    "if \"LATEST_CKPT\" in dir() and LATEST_CKPT and Path(LATEST_CKPT).exists():\n",
    "    ckpt_path = Path(LATEST_CKPT)\n",
    "    print(f\"üìÅ Using pre-selected checkpoint: {ckpt_path}\")\n",
    "else:\n",
    "    run_base = Path(CKPT_ROOT) / \"runs\" / GRADCAM_TAG\n",
    "    all_runs = sorted(run_base.glob(\"*/\"))\n",
    "    if not all_runs:\n",
    "        raise FileNotFoundError(f\"No run folders found under {run_base}\")\n",
    "    latest_run = all_runs[-1]\n",
    "    ckpt_path = latest_run / \"best.pt\"\n",
    "    if not ckpt_path.exists():\n",
    "        ckpt_path = latest_run / \"last.pt\"\n",
    "    print(f\"üìÅ Using checkpoint: {ckpt_path}\")\n",
    "\n",
    "# ===== LOAD MODEL =====\n",
    "from ddriver.models.registry import build_model, register_timm_backbone\n",
    "\n",
    "# Register timm backbone if needed\n",
    "try:\n",
    "    register_timm_backbone(MODEL_NAME)\n",
    "except (ImportError, ValueError):\n",
    "    pass  # Already registered or custom model\n",
    "\n",
    "# Load checkpoint to get num_classes\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "num_classes = ckpt[\"model_state_dict\"][\"classifier.weight\"].shape[0] if \"classifier.weight\" in ckpt[\"model_state_dict\"] else 10\n",
    "\n",
    "model = build_model(MODEL_NAME, num_classes=num_classes, pretrained=False)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"‚úÖ Loaded model: {MODEL_NAME} with {num_classes} classes\")\n",
    "\n",
    "# ===== DETERMINE TARGET LAYER FOR GRAD-CAM =====\n",
    "# Different architectures have different final conv layers\n",
    "# Handle both wrapped (model.backbone) and unwrapped (direct timm) models\n",
    "def get_target_layers(model, model_name):\n",
    "    \"\"\"Get appropriate target layers for Grad-CAM based on model architecture.\"\"\"\n",
    "    # Check if model has backbone wrapper\n",
    "    base = getattr(model, \"backbone\", model)\n",
    "    \n",
    "    if \"efficientnet\" in model_name.lower():\n",
    "        # timm EfficientNet: use conv_head or last block\n",
    "        if hasattr(base, \"conv_head\"):\n",
    "            return [base.conv_head]\n",
    "        elif hasattr(base, \"blocks\"):\n",
    "            return [base.blocks[-1]]\n",
    "        elif hasattr(base, \"features\"):\n",
    "            return [base.features[-1]]\n",
    "    elif \"resnet\" in model_name.lower():\n",
    "        if hasattr(base, \"layer4\"):\n",
    "            return [base.layer4[-1]]\n",
    "    elif \"convnext\" in model_name.lower():\n",
    "        if hasattr(base, \"stages\"):\n",
    "            return [base.stages[-1]]\n",
    "        elif hasattr(base, \"features\"):\n",
    "            return [base.features[-1]]\n",
    "    \n",
    "    # Fallback: try common patterns\n",
    "    for attr in [\"features\", \"blocks\", \"stages\", \"layer4\"]:\n",
    "        if hasattr(base, attr):\n",
    "            layer = getattr(base, attr)\n",
    "            if hasattr(layer, \"__getitem__\"):\n",
    "                return [layer[-1]]\n",
    "    \n",
    "    # Last resort: get second-to-last child\n",
    "    children = list(base.children())\n",
    "    if len(children) >= 2:\n",
    "        return [children[-2]]\n",
    "    raise ValueError(f\"Could not determine target layer for {model_name}\")\n",
    "\n",
    "target_layers = get_target_layers(model, MODEL_NAME)\n",
    "print(f\"üéØ Target layer for Grad-CAM: {target_layers[0].__class__.__name__}\")\n",
    "\n",
    "# ===== LOAD PREDICTIONS AND DATA =====\n",
    "# Find the predictions CSV - use PRED_TAG if set, otherwise construct from GRADCAM_TAG\n",
    "# Predictions are saved as {RUN_TAG}_{split}.csv, so we need to match that\n",
    "if \"PRED_TAG\" in dir() and PRED_TAG:\n",
    "    preds_tag = PRED_TAG\n",
    "else:\n",
    "    # Construct the expected predictions tag: {GRADCAM_TAG}_{split}\n",
    "    preds_tag = f\"{GRADCAM_TAG}_{SPLIT_TO_ANALYZE}\"\n",
    "\n",
    "preds_csv = Path(OUT_ROOT) / \"preds\" / SPLIT_TO_ANALYZE / f\"{preds_tag}.csv\"\n",
    "if not preds_csv.exists():\n",
    "    # Also try without the split suffix (in case predictions were saved differently)\n",
    "    alt_preds_csv = Path(OUT_ROOT) / \"preds\" / SPLIT_TO_ANALYZE / f\"{GRADCAM_TAG}.csv\"\n",
    "    if alt_preds_csv.exists():\n",
    "        preds_csv = alt_preds_csv\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Predictions not found at:\\n  - {preds_csv}\\n  - {alt_preds_csv}\\n\"\n",
    "            f\"Run the prediction cell first!\"\n",
    "        )\n",
    "\n",
    "preds_df = pd.read_csv(preds_csv)\n",
    "print(f\"üìä Loaded {len(preds_df)} predictions from {preds_csv}\")\n",
    "\n",
    "# Determine data root and manifest path based on data source\n",
    "if \"USE_HYBRID\" in globals() and USE_HYBRID:\n",
    "    data_root = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", Path(OUT_ROOT) / \"hybrid\"))\n",
    "    roi_variant = globals().get(\"ROI_VARIANT\", \"face_hands\")\n",
    "    manifest_path = data_root / f\"manifest_{roi_variant}.csv\"\n",
    "elif \"USE_YOLO\" in globals() and USE_YOLO:\n",
    "    data_root = Path(os.environ.get(\"YOLO_ROOT_LOCAL\", Path(OUT_ROOT) / \"yolo\"))\n",
    "    roi_variant = globals().get(\"ROI_VARIANT\", \"face_hands\")\n",
    "    manifest_path = data_root / f\"manifest_{roi_variant}.csv\"\n",
    "elif \"USE_MEDIAPIPE\" in globals() and USE_MEDIAPIPE:\n",
    "    data_root = Path(OUT_ROOT) / \"mediapipe\"\n",
    "    roi_variant = globals().get(\"ROI_VARIANT\", \"face_hands\")\n",
    "    manifest_path = data_root / f\"manifest_{roi_variant}.csv\"\n",
    "else:\n",
    "    data_root = Path(DATASET_ROOT)\n",
    "    manifest_path = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "print(f\"üìÅ Data root: {data_root}\")\n",
    "\n",
    "# Helper to find image with multiple fallback paths\n",
    "def find_image_path(path_str, data_root, dataset_root, roi_variant=None):\n",
    "    \"\"\"\n",
    "    Try multiple locations to find the image:\n",
    "    1. If path is absolute and exists, use it\n",
    "    2. Try data_root / path (for hybrid/yolo/mediapipe)\n",
    "    3. Try data_root / variant / path (if variant subfolder)\n",
    "    4. Try dataset_root / path (original images - warns if used)\n",
    "    5. Extract relative path and try common patterns\n",
    "    \"\"\"\n",
    "    path = Path(path_str)\n",
    "    \n",
    "    # 1. Absolute path that exists\n",
    "    if path.is_absolute() and path.exists():\n",
    "        return path, False  # (path, is_fallback)\n",
    "    \n",
    "    # 2. Try data_root / path directly\n",
    "    candidate = data_root / path_str\n",
    "    if candidate.exists():\n",
    "        return candidate, False\n",
    "    \n",
    "    # 3. If path doesn't start with variant, try adding it\n",
    "    if roi_variant and not path_str.startswith(roi_variant):\n",
    "        candidate = data_root / roi_variant / path_str\n",
    "        if candidate.exists():\n",
    "            return candidate, False\n",
    "    \n",
    "    # 4. Try with just the filename in the expected structure\n",
    "    # Extract class folder and filename from path\n",
    "    parts = Path(path_str).parts\n",
    "    if len(parts) >= 2:\n",
    "        class_folder = [p for p in parts if p.startswith('c') and len(p) == 2]\n",
    "        if class_folder:\n",
    "            filename = Path(path_str).name\n",
    "            # Try: data_root / variant / class / filename\n",
    "            if roi_variant:\n",
    "                candidate = data_root / roi_variant / class_folder[0] / filename\n",
    "                if candidate.exists():\n",
    "                    return candidate, False\n",
    "    \n",
    "    # 5. Fallback to original dataset (this shows WRONG images for cropped models!)\n",
    "    candidate = Path(dataset_root) / path_str\n",
    "    if candidate.exists():\n",
    "        return candidate, True  # Mark as fallback\n",
    "    \n",
    "    # Try removing any prefix from the path to get relative path\n",
    "    for marker in [\"auc.distracted.driver\", \"train/\", \"test/\"]:\n",
    "        if marker in path_str:\n",
    "            idx = path_str.find(marker)\n",
    "            rel_path = path_str[idx:]\n",
    "            candidate = Path(dataset_root) / rel_path\n",
    "            if candidate.exists():\n",
    "                return candidate, True\n",
    "    \n",
    "    return None, False\n",
    "\n",
    "# Track fallback warnings (use list to allow mutation in nested scope)\n",
    "_fallback_state = {\"warned\": False}\n",
    "\n",
    "# Load manifest to get ground truth labels\n",
    "manifest_df = pd.read_csv(manifest_path)\n",
    "print(f\"üìã Loaded manifest with {len(manifest_df)} entries from {manifest_path}\")\n",
    "\n",
    "# Debug: Show sample paths to help diagnose path issues\n",
    "print(f\"üìç Sample manifest paths: {manifest_df['path'].head(2).tolist()}\")\n",
    "\n",
    "# Merge predictions with manifest to get ground truth labels\n",
    "# Predictions have: path (may be absolute), pred_class_id (e.g., \"c0\")\n",
    "# Manifest has: path (relative crop path), class_id (e.g., \"c0\")\n",
    "# Match on CLASS + FILENAME to avoid duplicates (same filename exists in c0/, c1/, etc.)\n",
    "\n",
    "# Show data mode info (path format doesn't indicate what was actually loaded)\n",
    "if \"USE_HYBRID\" in globals() and USE_HYBRID:\n",
    "    print(\"‚úÖ HYBRID mode - model was trained/predicted on face+hands crops\")\n",
    "elif \"USE_YOLO\" in globals() and USE_YOLO:\n",
    "    print(\"‚úÖ YOLO mode - model was trained/predicted on YOLO-detected crops\")\n",
    "elif \"USE_MEDIAPIPE\" in globals() and USE_MEDIAPIPE:\n",
    "    print(\"‚úÖ MEDIAPIPE mode - model was trained/predicted on MediaPipe crops\")\n",
    "else:\n",
    "    print(\"üì∑ FULL IMAGE mode - model was trained/predicted on original full images\")\n",
    "\n",
    "# Extract class folder (c0-c9) from prediction path\n",
    "def extract_class_from_path(p):\n",
    "    parts = Path(p).parts\n",
    "    for part in parts:\n",
    "        if part.startswith('c') and len(part) == 2 and part[1].isdigit():\n",
    "            return part\n",
    "    return \"\"\n",
    "\n",
    "preds_df[\"_class\"] = preds_df[\"path\"].apply(extract_class_from_path)\n",
    "preds_df[\"_filename\"] = preds_df[\"path\"].apply(lambda p: Path(p).name)\n",
    "\n",
    "# Manifest has class_id column (c0-c9)\n",
    "manifest_df[\"_class\"] = manifest_df[\"class_id\"]\n",
    "manifest_df[\"_filename\"] = manifest_df[\"path\"].apply(lambda p: Path(p).name)\n",
    "\n",
    "# Merge on BOTH class and filename to get unique matches\n",
    "preds_df = preds_df.merge(\n",
    "    manifest_df[[\"_class\", \"_filename\", \"path\", \"class_id\"]].rename(columns={\"path\": \"crop_path\"}),\n",
    "    on=[\"_class\", \"_filename\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Check for unmatched or duplicate rows\n",
    "n_original = len(preds_df)\n",
    "n_unmatched = preds_df[\"class_id\"].isna().sum()\n",
    "if n_unmatched > 0:\n",
    "    print(f\"‚ö†Ô∏è  {n_unmatched}/{n_original} predictions couldn't be matched to manifest\")\n",
    "    preds_df = preds_df.dropna(subset=[\"class_id\"])\n",
    "print(f\"‚úÖ Final merged dataset: {len(preds_df)} rows (from {n_original} after merge)\")\n",
    "\n",
    "# Use crop_path for visualization (points to actual cropped images)\n",
    "preds_df[\"vis_path\"] = preds_df[\"crop_path\"]\n",
    "\n",
    "# Convert class_id strings (c0-c9) to integers (0-9)\n",
    "def class_to_int(class_id):\n",
    "    if pd.isna(class_id):\n",
    "        return -1\n",
    "    if isinstance(class_id, str) and class_id.startswith(\"c\"):\n",
    "        return int(class_id[1:])\n",
    "    return int(class_id)\n",
    "\n",
    "preds_df[\"label\"] = preds_df[\"class_id\"].apply(class_to_int)\n",
    "preds_df[\"pred\"] = preds_df[\"pred_class_id\"].apply(class_to_int)\n",
    "\n",
    "# Add placeholder confidence (not available from current prediction code)\n",
    "if \"confidence\" not in preds_df.columns:\n",
    "    preds_df[\"confidence\"] = 1.0  # Placeholder\n",
    "\n",
    "print(f\"‚úÖ Merged predictions with labels. Columns: {list(preds_df.columns)}\")\n",
    "print(f\"üìç Will visualize using crop_path: {preds_df['vis_path'].head(2).tolist()}\")\n",
    "\n",
    "# ===== CLASS NAMES =====\n",
    "CLASS_NAMES = {\n",
    "    0: \"Safe driving\",\n",
    "    1: \"Texting (right)\",\n",
    "    2: \"Talking (right)\",\n",
    "    3: \"Texting (left)\",\n",
    "    4: \"Talking (left)\",\n",
    "    5: \"Operating radio\",\n",
    "    6: \"Drinking\",\n",
    "    7: \"Reaching behind\",\n",
    "    8: \"Hair/makeup\",\n",
    "    9: \"Talking to passenger\",\n",
    "}\n",
    "\n",
    "# ===== TRANSFORMS =====\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def load_image(path):\n",
    "    \"\"\"Load image and return both tensor and RGB numpy array.\"\"\"\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img_resized = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    img_np = np.array(img_resized) / 255.0  # Normalized 0-1 for overlay\n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "    return img_tensor, img_np\n",
    "\n",
    "# ===== GENERATE GRAD-CAM =====\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "\n",
    "def generate_gradcam(img_path, target_class=None):\n",
    "    \"\"\"Generate Grad-CAM visualization for an image.\"\"\"\n",
    "    img_tensor, img_np = load_image(img_path)\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    \n",
    "    # Generate CAM (None = use predicted class)\n",
    "    grayscale_cam = cam(input_tensor=img_tensor, targets=None)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    # Overlay on image\n",
    "    visualization = show_cam_on_image(img_np.astype(np.float32), grayscale_cam, use_rgb=True)\n",
    "    return visualization, grayscale_cam\n",
    "\n",
    "# ===== 1. SAMPLE CORRECT PREDICTIONS PER CLASS =====\n",
    "print(\"\\nüé® Generating Grad-CAM for correct predictions per class...\")\n",
    "\n",
    "correct_df = preds_df[preds_df[\"label\"] == preds_df[\"pred\"]]\n",
    "fig, axes = plt.subplots(num_classes, N_SAMPLES_PER_CLASS * 2, figsize=(N_SAMPLES_PER_CLASS * 6, num_classes * 3))\n",
    "\n",
    "for class_id in range(num_classes):\n",
    "    class_samples = correct_df[correct_df[\"label\"] == class_id]\n",
    "    samples = class_samples.sample(min(N_SAMPLES_PER_CLASS, len(class_samples)), random_state=42)\n",
    "    \n",
    "    for i, (_, row) in enumerate(samples.iterrows()):\n",
    "        if i >= N_SAMPLES_PER_CLASS:\n",
    "            break\n",
    "        \n",
    "        # Use vis_path (crop path from manifest) for visualization\n",
    "        vis_path_str = row.get(\"vis_path\", row[\"path\"])\n",
    "        roi_var = globals().get(\"ROI_VARIANT\") if (\"USE_HYBRID\" in globals() or \"USE_YOLO\" in globals() or \"USE_MEDIAPIPE\" in globals()) else None\n",
    "        img_path, is_fallback = find_image_path(vis_path_str, data_root, DATASET_ROOT, roi_var)\n",
    "        \n",
    "        if is_fallback and not _fallback_state[\"warned\"]:\n",
    "            print(f\"‚ö†Ô∏è  WARNING: Using ORIGINAL full images for visualization (cropped images not found)\")\n",
    "            print(f\"   This means Grad-CAM heatmaps are overlaid on wrong images!\")\n",
    "            print(f\"   Expected path: {data_root / vis_path_str}\")\n",
    "            _fallback_state[\"warned\"] = True\n",
    "        \n",
    "        if img_path and img_path.exists():\n",
    "            # Original image\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "            axes[class_id, i * 2].imshow(img)\n",
    "            axes[class_id, i * 2].set_title(f\"{CLASS_NAMES.get(class_id, f'Class {class_id}')}\", fontsize=8)\n",
    "            axes[class_id, i * 2].axis(\"off\")\n",
    "            \n",
    "            # Grad-CAM\n",
    "            viz, _ = generate_gradcam(img_path)\n",
    "            axes[class_id, i * 2 + 1].imshow(viz)\n",
    "            axes[class_id, i * 2 + 1].set_title(f\"Grad-CAM (conf: {row['confidence']:.2f})\", fontsize=8)\n",
    "            axes[class_id, i * 2 + 1].axis(\"off\")\n",
    "        else:\n",
    "            axes[class_id, i * 2].text(0.5, 0.5, \"Not found\", ha=\"center\", va=\"center\")\n",
    "            axes[class_id, i * 2].axis(\"off\")\n",
    "            axes[class_id, i * 2 + 1].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Grad-CAM: Correct Predictions per Class ({MODEL_NAME})\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "gradcam_dir = Path(OUT_ROOT) / \"gradcam\" / GRADCAM_TAG\n",
    "gradcam_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(gradcam_dir / \"correct_per_class.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"üíæ Saved to {gradcam_dir / 'correct_per_class.png'}\")\n",
    "\n",
    "# ===== 2. MISCLASSIFIED EXAMPLES =====\n",
    "print(\"\\nüî¥ Generating Grad-CAM for misclassified examples...\")\n",
    "\n",
    "misclassified_df = preds_df[preds_df[\"label\"] != preds_df[\"pred\"]]\n",
    "if len(misclassified_df) > 0:\n",
    "    samples = misclassified_df.sample(min(N_MISCLASSIFIED, len(misclassified_df)), random_state=42)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(samples), 3, figsize=(12, len(samples) * 3))\n",
    "    if len(samples) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, (_, row) in enumerate(samples.iterrows()):\n",
    "        # Use vis_path (crop path from manifest) for visualization\n",
    "        vis_path_str = row.get(\"vis_path\", row[\"path\"])\n",
    "        roi_var = globals().get(\"ROI_VARIANT\") if (\"USE_HYBRID\" in globals() or \"USE_YOLO\" in globals() or \"USE_MEDIAPIPE\" in globals()) else None\n",
    "        img_path, is_fallback = find_image_path(vis_path_str, data_root, DATASET_ROOT, roi_var)\n",
    "        \n",
    "        if img_path and img_path.exists():\n",
    "            # Original\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "            axes[i, 0].imshow(img)\n",
    "            axes[i, 0].set_title(f\"True: {CLASS_NAMES.get(int(row['label']), row['label'])}\", fontsize=9)\n",
    "            axes[i, 0].axis(\"off\")\n",
    "            \n",
    "            # Grad-CAM\n",
    "            viz, heatmap = generate_gradcam(img_path)\n",
    "            axes[i, 1].imshow(viz)\n",
    "            axes[i, 1].set_title(f\"Pred: {CLASS_NAMES.get(int(row['pred']), row['pred'])} ({row['confidence']:.2f})\", fontsize=9)\n",
    "            axes[i, 1].axis(\"off\")\n",
    "            \n",
    "            # Heatmap only\n",
    "            axes[i, 2].imshow(heatmap, cmap=\"jet\")\n",
    "            axes[i, 2].set_title(\"Attention heatmap\", fontsize=9)\n",
    "            axes[i, 2].axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(f\"Grad-CAM: Misclassified Examples ({MODEL_NAME})\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(gradcam_dir / \"misclassified.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"üíæ Saved to {gradcam_dir / 'misclassified.png'}\")\n",
    "else:\n",
    "    print(\"‚úÖ No misclassified examples found!\")\n",
    "\n",
    "# ===== 3. SUMMARY STATISTICS =====\n",
    "print(\"\\nüìä Grad-CAM Analysis Summary:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Checkpoint: {ckpt_path.name}\")\n",
    "print(f\"   Split analyzed: {SPLIT_TO_ANALYZE}\")\n",
    "print(f\"   Total predictions: {len(preds_df)}\")\n",
    "print(f\"   Correct: {len(correct_df)} ({100*len(correct_df)/len(preds_df):.1f}%)\")\n",
    "print(f\"   Misclassified: {len(misclassified_df)} ({100*len(misclassified_df)/len(preds_df):.1f}%)\")\n",
    "print(f\"\\nüìÅ Visualizations saved to: {gradcam_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c94eb20",
   "metadata": {},
   "source": [
    "## üéØ 11.5b Advanced Grad-CAM Analysis (Stratified Sampling)\n",
    "\n",
    "This cell generates a **systematic Grad-CAM analysis** using stratified sampling:\n",
    "\n",
    "**Four categories of predictions:**\n",
    "1. ‚úÖ **Correct + High confidence** - What the model considers strong evidence\n",
    "2. ü§î **Correct + Low confidence** - Borderline cases, \"lucky guesses\"\n",
    "3. ‚ùå **Wrong + High confidence** - **Most important!** Reveals shortcuts/spurious patterns\n",
    "4. üòï **Wrong + Low confidence** - Reasonable confusions, ambiguous cases\n",
    "\n",
    "**Outputs:**\n",
    "- **Thesis gallery** (12-24 images): Representative examples for each \"story\"\n",
    "- **Extended gallery** (100-200 images): Full review set for appendix\n",
    "\n",
    "**Key questions this answers:**\n",
    "- Does the model focus on the right evidence when correct?\n",
    "- When wrong, is it looking at shortcuts (background/identity cues) or genuinely ambiguous actions?\n",
    "- Which classes rely on face vs hands cues?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d946f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Advanced Grad-CAM Analysis with Stratified Sampling\n",
    "# Requires: Run cell 11.5 first to set up model, predictions, and Grad-CAM\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "N_SAMPLES_PER_CATEGORY = 5       # Per class, per category (correct/wrong √ó high/low conf)\n",
    "CONFIDENCE_THRESHOLD = 0.8       # Above = high conf, below = low conf\n",
    "THESIS_SAMPLES_PER_STORY = 3     # For the main thesis gallery\n",
    "SAVE_EXTENDED_GALLERY = True     # Save all samples to appendix folder\n",
    "\n",
    "# Reuse variables from cell 11.5\n",
    "# Assumes: preds_df, model, cam, data_root, CLASS_NAMES, generate_gradcam, find_image_path\n",
    "\n",
    "# ============================================================================\n",
    "# STRATIFIED SAMPLING\n",
    "# ============================================================================\n",
    "print(\"üìä Stratifying predictions by correctness and confidence...\\n\")\n",
    "\n",
    "# Ensure we have confidence column\n",
    "if \"confidence\" not in preds_df.columns:\n",
    "    print(\"‚ö†Ô∏è  No 'confidence' column in predictions. Using placeholder (1.0).\")\n",
    "    print(\"   Re-run predictions with latest code to get real confidence scores.\")\n",
    "    preds_df[\"confidence\"] = 1.0\n",
    "\n",
    "# Create category columns\n",
    "preds_df[\"correct\"] = preds_df[\"label\"] == preds_df[\"pred\"]\n",
    "preds_df[\"high_conf\"] = preds_df[\"confidence\"] >= CONFIDENCE_THRESHOLD\n",
    "\n",
    "# Define the 4 categories\n",
    "categories = {\n",
    "    \"correct_high\": preds_df[preds_df[\"correct\"] & preds_df[\"high_conf\"]],\n",
    "    \"correct_low\": preds_df[preds_df[\"correct\"] & ~preds_df[\"high_conf\"]],\n",
    "    \"wrong_high\": preds_df[~preds_df[\"correct\"] & preds_df[\"high_conf\"]],\n",
    "    \"wrong_low\": preds_df[~preds_df[\"correct\"] & ~preds_df[\"high_conf\"]],\n",
    "}\n",
    "\n",
    "print(\"Category breakdown:\")\n",
    "for cat_name, cat_df in categories.items():\n",
    "    print(f\"  {cat_name}: {len(cat_df)} samples ({100*len(cat_df)/len(preds_df):.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLE SELECTION\n",
    "# ============================================================================\n",
    "print(\"\\nüé≤ Selecting stratified samples...\")\n",
    "\n",
    "selected_samples = {cat: [] for cat in categories}\n",
    "\n",
    "for cat_name, cat_df in categories.items():\n",
    "    if len(cat_df) == 0:\n",
    "        continue\n",
    "    \n",
    "    for class_id in range(10):\n",
    "        class_df = cat_df[cat_df[\"label\"] == class_id]\n",
    "        if len(class_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Sort by confidence (descending for high_conf categories, ascending for low)\n",
    "        if \"high\" in cat_name:\n",
    "            class_df = class_df.sort_values(\"confidence\", ascending=False)\n",
    "        else:\n",
    "            class_df = class_df.sort_values(\"confidence\", ascending=True)\n",
    "        \n",
    "        # Take top N samples\n",
    "        samples = class_df.head(N_SAMPLES_PER_CATEGORY)\n",
    "        selected_samples[cat_name].extend(samples.to_dict(\"records\"))\n",
    "\n",
    "print(\"\\nSelected samples per category:\")\n",
    "for cat_name, samples in selected_samples.items():\n",
    "    print(f\"  {cat_name}: {len(samples)} samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATE GRAD-CAM FOR ALL SELECTED SAMPLES\n",
    "# ============================================================================\n",
    "print(\"\\nüî• Generating Grad-CAM visualizations...\")\n",
    "\n",
    "gradcam_results = {cat: [] for cat in categories}\n",
    "roi_var = globals().get(\"ROI_VARIANT\") if any(globals().get(f\"USE_{m}\") for m in [\"HYBRID\", \"YOLO\", \"MEDIAPIPE\"]) else None\n",
    "\n",
    "for cat_name, samples in selected_samples.items():\n",
    "    print(f\"\\n  Processing {cat_name}...\")\n",
    "    for sample in samples:\n",
    "        vis_path = sample.get(\"vis_path\", sample.get(\"crop_path\", sample[\"path\"]))\n",
    "        img_path, _ = find_image_path(vis_path, data_root, DATASET_ROOT, roi_var)\n",
    "        \n",
    "        if img_path and img_path.exists():\n",
    "            try:\n",
    "                viz, heatmap = generate_gradcam(img_path)\n",
    "                gradcam_results[cat_name].append({\n",
    "                    **sample,\n",
    "                    \"img_path\": str(img_path),\n",
    "                    \"gradcam_viz\": viz,\n",
    "                    \"heatmap\": heatmap,\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ö†Ô∏è Failed for {vis_path}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Grad-CAM generation complete!\")\n",
    "for cat_name, results in gradcam_results.items():\n",
    "    print(f\"  {cat_name}: {len(results)} visualizations\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE EXTENDED GALLERY (for appendix)\n",
    "# ============================================================================\n",
    "if SAVE_EXTENDED_GALLERY:\n",
    "    gallery_dir = Path(OUT_ROOT) / \"gradcam\" / GRADCAM_TAG / \"stratified_gallery\"\n",
    "    gallery_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nüíæ Saving extended gallery to {gallery_dir}...\")\n",
    "    \n",
    "    for cat_name, results in gradcam_results.items():\n",
    "        cat_dir = gallery_dir / cat_name\n",
    "        cat_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        for i, r in enumerate(results):\n",
    "            true_label = CLASS_NAMES.get(int(r[\"label\"]), f\"c{r['label']}\")\n",
    "            pred_label = CLASS_NAMES.get(int(r[\"pred\"]), f\"c{r['pred']}\")\n",
    "            conf = r[\"confidence\"]\n",
    "            \n",
    "            # Save visualization\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "            \n",
    "            # Original image\n",
    "            img = Image.open(r[\"img_path\"]).convert(\"RGB\").resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(f\"True: {true_label}\", fontsize=10)\n",
    "            axes[0].axis(\"off\")\n",
    "            \n",
    "            # Grad-CAM overlay\n",
    "            axes[1].imshow(r[\"gradcam_viz\"])\n",
    "            axes[1].set_title(f\"Pred: {pred_label} ({conf:.2f})\", fontsize=10)\n",
    "            axes[1].axis(\"off\")\n",
    "            \n",
    "            # Heatmap\n",
    "            axes[2].imshow(r[\"heatmap\"], cmap=\"jet\")\n",
    "            axes[2].set_title(\"Attention\", fontsize=10)\n",
    "            axes[2].axis(\"off\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(cat_dir / f\"{i:03d}_true{r['label']}_pred{r['pred']}_conf{conf:.2f}.png\", dpi=100)\n",
    "            plt.close()\n",
    "    \n",
    "    print(f\"‚úÖ Extended gallery saved!\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE THESIS-READY SUMMARY FIGURE\n",
    "# ============================================================================\n",
    "print(\"\\nüìù Creating thesis-ready summary figure...\")\n",
    "\n",
    "# Focus on the most interesting category: WRONG + HIGH CONFIDENCE\n",
    "wrong_high = gradcam_results.get(\"wrong_high\", [])[:12]  # Top 12 most confident errors\n",
    "\n",
    "if len(wrong_high) > 0:\n",
    "    n_cols = 4\n",
    "    n_rows = min(3, (len(wrong_high) + n_cols - 1) // n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols * 2, figsize=(n_cols * 6, n_rows * 3))\n",
    "    axes = axes.reshape(-1) if n_rows > 1 else axes\n",
    "    \n",
    "    for i, r in enumerate(wrong_high[:n_rows * n_cols]):\n",
    "        true_label = CLASS_NAMES.get(int(r[\"label\"]), f\"c{r['label']}\")\n",
    "        pred_label = CLASS_NAMES.get(int(r[\"pred\"]), f\"c{r['pred']}\")\n",
    "        \n",
    "        # Original\n",
    "        img = Image.open(r[\"img_path\"]).convert(\"RGB\").resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        axes[i * 2].imshow(img)\n",
    "        axes[i * 2].set_title(f\"True: {true_label}\", fontsize=8)\n",
    "        axes[i * 2].axis(\"off\")\n",
    "        \n",
    "        # Grad-CAM\n",
    "        axes[i * 2 + 1].imshow(r[\"gradcam_viz\"])\n",
    "        axes[i * 2 + 1].set_title(f\"Pred: {pred_label} ({r['confidence']:.2f})\", fontsize=8)\n",
    "        axes[i * 2 + 1].axis(\"off\")\n",
    "    \n",
    "    # Hide unused axes\n",
    "    for j in range(len(wrong_high) * 2, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(\"High-Confidence Errors: Where Does the Model Look When Wrong?\", fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    thesis_path = Path(OUT_ROOT) / \"gradcam\" / GRADCAM_TAG / \"thesis_high_conf_errors.png\"\n",
    "    plt.savefig(thesis_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"üíæ Thesis figure saved to {thesis_path}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No high-confidence errors found (model may be very accurate or conservative)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä STRATIFIED GRAD-CAM ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Checkpoint: {Path(LATEST_CKPT).name if 'LATEST_CKPT' in dir() else 'unknown'}\")\n",
    "print(f\"Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "for cat_name, cat_df in categories.items():\n",
    "    emoji = {\"correct_high\": \"‚úÖüî•\", \"correct_low\": \"‚úÖü§î\", \"wrong_high\": \"‚ùåüî•\", \"wrong_low\": \"‚ùåü§î\"}[cat_name]\n",
    "    print(f\"  {emoji} {cat_name}: {len(cat_df)} ({100*len(cat_df)/len(preds_df):.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìÅ Outputs saved to: {Path(OUT_ROOT) / 'gradcam' / GRADCAM_TAG}\")\n",
    "print(f\"   - stratified_gallery/  (extended gallery for appendix)\")\n",
    "print(f\"   - thesis_high_conf_errors.png  (main thesis figure)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b11bb",
   "metadata": {},
   "source": [
    "## üîç 11.5c Grad-CAM for Specific Error Types\n",
    "\n",
    "Investigate **specific confusion pairs** to understand why the model fails.\n",
    "\n",
    "Example pairs to analyze:\n",
    "- **Texting Left ‚Üí Talking Left**: Same side, different action (phone vs hand position)\n",
    "- **Drinking ‚Üí Reaching Back**: Similar arm motion\n",
    "- **Texting Right ‚Üí Phone Right**: Both involve phone, different grip\n",
    "- **Safe ‚Üí Texting**: Why does the model hallucinate phone use?\n",
    "\n",
    "This cell generates Grad-CAM visualizations for all examples of a specific (true‚Üípred) confusion pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eecd41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Grad-CAM for Specific Error Types\n",
    "# Requires: Run cell 11.5 first to set up model, predictions, and Grad-CAM\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Which confusion pair to investigate?\n",
    "# ============================================================================\n",
    "# Set the TRUE class and PREDICTED class to investigate\n",
    "TRUE_CLASS = 3   # Texting Left\n",
    "PRED_CLASS = 4   # Phone Left\n",
    "\n",
    "# You can also specify multiple pairs to analyze at once:\n",
    "CONFUSION_PAIRS = [\n",
    "    (3, 4, \"Texting Left ‚Üí Phone Left\"),      # Similar: both left side, phone involved\n",
    "    (1, 2, \"Texting Right ‚Üí Phone Right\"),    # Similar: both right side, phone\n",
    "    (6, 7, \"Drinking ‚Üí Reaching Back\"),       # Similar: arm raised\n",
    "    (0, 1, \"Safe ‚Üí Texting Right\"),           # Why hallucinate phone?\n",
    "    (9, 0, \"Passenger ‚Üí Safe\"),               # Head position confusion\n",
    "]\n",
    "\n",
    "MAX_SAMPLES = 8  # Maximum samples per confusion pair\n",
    "\n",
    "CLASS_NAMES = {\n",
    "    0: \"Safe driving\", 1: \"Texting (R)\", 2: \"Phone (R)\", 3: \"Texting (L)\",\n",
    "    4: \"Phone (L)\", 5: \"Radio\", 6: \"Drinking\", 7: \"Reaching back\",\n",
    "    8: \"Hair/makeup\", 9: \"Passenger\"\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# CHECK PREREQUISITES\n",
    "# ============================================================================\n",
    "required_vars = [\"preds_df\", \"model\", \"cam\", \"data_root\", \"generate_gradcam\", \"find_image_path\"]\n",
    "missing = [v for v in required_vars if v not in dir()]\n",
    "if missing:\n",
    "    print(\"‚ùå Missing required variables from cell 11.5:\", missing)\n",
    "    print(\"   Run the Grad-CAM Visualization cell (11.5) first!\")\n",
    "else:\n",
    "    print(\"‚úÖ All prerequisites loaded\")\n",
    "    \n",
    "    # Get ROI variant if using crops\n",
    "    roi_var = globals().get(\"ROI_VARIANT\") if any(globals().get(f\"USE_{m}\") for m in [\"HYBRID\", \"YOLO\", \"MEDIAPIPE\"]) else None\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ANALYZE EACH CONFUSION PAIR\n",
    "    # ============================================================================\n",
    "    for true_c, pred_c, description in CONFUSION_PAIRS:\n",
    "        \n",
    "        # Find samples with this confusion\n",
    "        confused = preds_df[(preds_df[\"label\"] == true_c) & (preds_df[\"pred\"] == pred_c)]\n",
    "        \n",
    "        if len(confused) == 0:\n",
    "            print(f\"\\n‚¨ú {description}: No examples found (model never makes this error)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üîç {description}\")\n",
    "        print(f\"   Found {len(confused)} examples of this confusion\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Sample up to MAX_SAMPLES\n",
    "        samples = confused.head(MAX_SAMPLES)\n",
    "        n_samples = len(samples)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(n_samples, 3, figsize=(12, n_samples * 3))\n",
    "        if n_samples == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, (_, row) in enumerate(samples.iterrows()):\n",
    "            vis_path = row.get(\"vis_path\", row.get(\"crop_path\", row[\"path\"]))\n",
    "            img_path, _ = find_image_path(vis_path, data_root, DATASET_ROOT, roi_var)\n",
    "            \n",
    "            if img_path and img_path.exists():\n",
    "                try:\n",
    "                    # Load original image\n",
    "                    img = Image.open(img_path).convert(\"RGB\").resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "                    axes[i, 0].imshow(img)\n",
    "                    axes[i, 0].set_title(f\"True: {CLASS_NAMES[true_c]}\", fontsize=9)\n",
    "                    axes[i, 0].axis(\"off\")\n",
    "                    \n",
    "                    # Generate Grad-CAM\n",
    "                    viz, heatmap = generate_gradcam(img_path)\n",
    "                    \n",
    "                    conf = row.get(\"confidence\", 0)\n",
    "                    axes[i, 1].imshow(viz)\n",
    "                    axes[i, 1].set_title(f\"Pred: {CLASS_NAMES[pred_c]} ({conf:.2f})\", fontsize=9)\n",
    "                    axes[i, 1].axis(\"off\")\n",
    "                    \n",
    "                    # Heatmap only\n",
    "                    axes[i, 2].imshow(heatmap, cmap=\"jet\")\n",
    "                    axes[i, 2].set_title(\"Attention Focus\", fontsize=9)\n",
    "                    axes[i, 2].axis(\"off\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è Error processing {vis_path}: {e}\")\n",
    "                    for j in range(3):\n",
    "                        axes[i, j].text(0.5, 0.5, \"Error\", ha=\"center\", va=\"center\")\n",
    "                        axes[i, j].axis(\"off\")\n",
    "            else:\n",
    "                for j in range(3):\n",
    "                    axes[i, j].text(0.5, 0.5, \"Not found\", ha=\"center\", va=\"center\")\n",
    "                    axes[i, j].axis(\"off\")\n",
    "        \n",
    "        plt.suptitle(f\"Confusion Analysis: {description}\\n({n_samples} examples)\", fontsize=12, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save\n",
    "        safe_name = description.replace(\" \", \"_\").replace(\"‚Üí\", \"to\").replace(\"/\", \"_\")\n",
    "        out_path = Path(OUT_ROOT) / \"gradcam\" / GRADCAM_TAG / \"confusion_analysis\" / f\"{safe_name}.png\"\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        print(f\"   üíæ Saved to {out_path}\")\n",
    "        \n",
    "        # ============================================================================\n",
    "        # ANALYZE WHAT THE MODEL IS LOOKING AT\n",
    "        # ============================================================================\n",
    "        print(f\"\\n   üìù Analysis hints for '{description}':\")\n",
    "        if true_c in [1, 3] and pred_c in [2, 4]:  # Texting ‚Üí Phone\n",
    "            print(\"      - Look for: Is the attention on the phone, or on the ear/cheek area?\")\n",
    "            print(\"      - Texting = phone in hand, looking down. Phone = near ear/cheek.\")\n",
    "        elif true_c in [6] and pred_c in [7]:  # Drinking ‚Üí Reaching\n",
    "            print(\"      - Look for: Is the attention on the cup/bottle, or just the arm motion?\")\n",
    "            print(\"      - Both involve raised arm. Cup shape is the key differentiator.\")\n",
    "        elif true_c == 0 and pred_c in [1, 3]:  # Safe ‚Üí Texting\n",
    "            print(\"      - Look for: What spurious cue triggered the 'texting' prediction?\")\n",
    "            print(\"      - Could be dashboard, shadows, or similar-looking hand position.\")\n",
    "        elif true_c == 9 and pred_c in [0]:  # Passenger ‚Üí Safe\n",
    "            print(\"      - Look for: Is the model ignoring the head orientation/gaze?\")\n",
    "            print(\"      - Passenger involves turned head; Safe is forward-facing.\")\n",
    "        else:\n",
    "            print(\"      - Examine where attention is focused vs. where it should be.\")\n",
    "            print(\"      - Look for background cues vs. action-relevant regions.\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # SUMMARY\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä CONFUSION PAIR SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nAnalyzed {len(CONFUSION_PAIRS)} confusion pairs\")\n",
    "    print(f\"Visualizations saved to: {Path(OUT_ROOT) / 'gradcam' / GRADCAM_TAG / 'confusion_analysis'}\")\n",
    "    print(\"\\nTo add more pairs, edit CONFUSION_PAIRS at the top of this cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64534ea4",
   "metadata": {},
   "source": [
    "## üî¨ 11.5a Grad-CAM Comparison: Full Image vs Hybrid Crops (Optional)\n",
    "\n",
    "If you've trained both a **full-image model** and a **hybrid crop model**, run this cell to generate side-by-side comparisons showing how cropping changes the model's attention.\n",
    "\n",
    "**Thesis insight:** This demonstrates WHY ROI cropping helps ‚Äî the full-image model may attend to irrelevant regions while the crop model focuses on meaningful features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32584ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ Grad-CAM Comparison: Full Image vs Hybrid Crops\n",
    "# Requires: trained models for BOTH full images AND hybrid crops\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision import transforms\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# Configure your models for comparison\n",
    "# You need predictions from BOTH models. Run cell 11.3 separately for each.\n",
    "\n",
    "# Model architecture (must be the same for both)\n",
    "MODEL_NAME = \"efficientnet_b0\"            # Must be same architecture for both\n",
    "\n",
    "# Split to analyze\n",
    "SPLIT = \"test\"  # or \"val\"\n",
    "\n",
    "# === FULL IMAGE MODEL ===\n",
    "# The tag used for your full-image model training run\n",
    "FULL_IMAGE_TAG = \"effb0_full_images\"      # <-- Change to your full-image model run tag\n",
    "# Prediction file name (usually: <RUN_TAG>_<SPLIT>.csv)\n",
    "FULL_PREDS_FILENAME = f\"{FULL_IMAGE_TAG}_{SPLIT}.csv\"  # e.g., effb0_full_images_test.csv\n",
    "\n",
    "# === HYBRID CROP MODEL ===\n",
    "# The tag used for your hybrid model training run\n",
    "HYBRID_TAG = \"effb0_InsightFace_MP_face_hands_V3\"  # <-- Change to your hybrid model run tag\n",
    "HYBRID_VARIANT = \"face_hands\"             # The ROI variant used\n",
    "# Prediction file name\n",
    "HYBRID_PREDS_FILENAME = f\"{HYBRID_TAG}_{SPLIT}.csv\"  # e.g., effb0_InsightFace_MP_face_hands_V3_test.csv\n",
    "\n",
    "N_COMPARISON_SAMPLES = 6\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Print expected file locations\n",
    "print(\"üìÅ Expected prediction files:\")\n",
    "print(f\"   Full:   {Path(OUT_ROOT) / 'preds' / SPLIT / FULL_PREDS_FILENAME}\")\n",
    "print(f\"   Hybrid: {Path(OUT_ROOT) / 'preds' / SPLIT / HYBRID_PREDS_FILENAME}\")\n",
    "print(\"   (If these don't exist, run cell 11.3 for each model first)\")\n",
    "\n",
    "# ===== HELPER FUNCTIONS =====\n",
    "def load_model_and_cam(tag, model_name):\n",
    "    \"\"\"Load a model and create GradCAM for it.\"\"\"\n",
    "    from ddriver.models.registry import build_model, register_timm_backbone\n",
    "    \n",
    "    # Register timm backbone if needed\n",
    "    try:\n",
    "        register_timm_backbone(model_name)\n",
    "    except (ImportError, ValueError):\n",
    "        pass  # Already registered or custom model\n",
    "    \n",
    "    run_base = Path(CKPT_ROOT) / \"runs\" / tag\n",
    "    all_runs = sorted(run_base.glob(\"*/\"))\n",
    "    if not all_runs:\n",
    "        return None, None, None\n",
    "    latest_run = all_runs[-1]\n",
    "    ckpt_path = latest_run / \"best.pt\"\n",
    "    if not ckpt_path.exists():\n",
    "        ckpt_path = latest_run / \"last.pt\"\n",
    "    \n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    num_classes = ckpt[\"model_state_dict\"].get(\"classifier.weight\", torch.zeros(10, 1)).shape[0]\n",
    "    \n",
    "    model = build_model(model_name, num_classes=num_classes, pretrained=False)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    # Get target layers - handle both wrapped and unwrapped models\n",
    "    base = getattr(model, \"backbone\", model)\n",
    "    if \"efficientnet\" in model_name.lower():\n",
    "        if hasattr(base, \"conv_head\"):\n",
    "            target_layers = [base.conv_head]\n",
    "        elif hasattr(base, \"blocks\"):\n",
    "            target_layers = [base.blocks[-1]]\n",
    "        else:\n",
    "            target_layers = [base.features[-1]]\n",
    "    elif \"resnet\" in model_name.lower():\n",
    "        target_layers = [base.layer4[-1]] if hasattr(base, \"layer4\") else [list(base.children())[-2]]\n",
    "    elif \"convnext\" in model_name.lower():\n",
    "        if hasattr(base, \"stages\"):\n",
    "            target_layers = [base.stages[-1]]\n",
    "        else:\n",
    "            target_layers = [base.features[-1]]\n",
    "    else:\n",
    "        # Fallback\n",
    "        for attr in [\"features\", \"blocks\", \"stages\"]:\n",
    "            if hasattr(base, attr):\n",
    "                target_layers = [getattr(base, attr)[-1]]\n",
    "                break\n",
    "        else:\n",
    "            target_layers = [list(base.children())[-2]]\n",
    "    \n",
    "    cam = GradCAM(model=model, target_layers=target_layers)\n",
    "    return model, cam, ckpt_path\n",
    "\n",
    "# Load both models\n",
    "print(\"Loading models...\")\n",
    "full_model, full_cam, full_ckpt = load_model_and_cam(FULL_IMAGE_TAG, MODEL_NAME)\n",
    "hybrid_model, hybrid_cam, hybrid_ckpt = load_model_and_cam(HYBRID_TAG, MODEL_NAME)\n",
    "\n",
    "if full_model is None:\n",
    "    print(f\"‚ö†Ô∏è Full image model not found: {FULL_IMAGE_TAG}\")\n",
    "    print(\"   Train a full-image model first, or update FULL_IMAGE_TAG\")\n",
    "if hybrid_model is None:\n",
    "    print(f\"‚ö†Ô∏è Hybrid model not found: {HYBRID_TAG}\")\n",
    "    print(\"   Train a hybrid model first, or update HYBRID_TAG\")\n",
    "\n",
    "if full_model is None or hybrid_model is None:\n",
    "    raise RuntimeError(\"Both models required for comparison. Check tags above.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "full_model = full_model.to(device)\n",
    "hybrid_model = hybrid_model.to(device)\n",
    "print(f\"‚úÖ Loaded both models\")\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load predictions to find common samples\n",
    "full_preds_path = Path(OUT_ROOT) / \"preds\" / SPLIT / FULL_PREDS_FILENAME\n",
    "hybrid_preds_path = Path(OUT_ROOT) / \"preds\" / SPLIT / HYBRID_PREDS_FILENAME\n",
    "\n",
    "if not full_preds_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Full-image predictions not found: {full_preds_path}\\n\"\n",
    "        f\"   To generate: Run cell 11.2 with FULL_IMAGE_TAG checkpoint, then run cell 11.3\"\n",
    "    )\n",
    "if not hybrid_preds_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Hybrid predictions not found: {hybrid_preds_path}\\n\"\n",
    "        f\"   To generate: Run cell 11.2 with HYBRID_TAG checkpoint, then run cell 11.3\"\n",
    "    )\n",
    "\n",
    "full_preds = pd.read_csv(full_preds_path)\n",
    "hybrid_preds = pd.read_csv(hybrid_preds_path)\n",
    "print(f\"üìä Loaded {len(full_preds)} full-image predictions, {len(hybrid_preds)} hybrid predictions\")\n",
    "\n",
    "# Convert pred_class_id (c0-c9) to integers for display\n",
    "def class_to_int(class_id):\n",
    "    if isinstance(class_id, str) and class_id.startswith(\"c\"):\n",
    "        return int(class_id[1:])\n",
    "    return int(class_id) if pd.notna(class_id) else -1\n",
    "\n",
    "full_preds[\"pred\"] = full_preds[\"pred_class_id\"].apply(class_to_int)\n",
    "hybrid_preds[\"pred\"] = hybrid_preds[\"pred_class_id\"].apply(class_to_int)\n",
    "\n",
    "# Full images path and hybrid crops path\n",
    "full_data_root = Path(DATASET_ROOT)\n",
    "hybrid_data_root = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", Path(OUT_ROOT) / \"hybrid\"))\n",
    "\n",
    "# Load hybrid manifest once (not in loop)\n",
    "hybrid_manifest_path = hybrid_data_root / f\"manifest_{HYBRID_VARIANT}.csv\"\n",
    "if not hybrid_manifest_path.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Hybrid manifest not found: {hybrid_manifest_path}\")\n",
    "hybrid_manifest = pd.read_csv(hybrid_manifest_path)\n",
    "print(f\"üìã Loaded hybrid manifest with {len(hybrid_manifest)} entries\")\n",
    "\n",
    "# Sample images\n",
    "samples = full_preds.sample(min(N_COMPARISON_SAMPLES, len(full_preds)), random_state=42)\n",
    "\n",
    "# Generate comparison\n",
    "fig, axes = plt.subplots(len(samples), 4, figsize=(16, len(samples) * 3.5))\n",
    "\n",
    "CLASS_NAMES = {\n",
    "    0: \"Safe\", 1: \"Text-R\", 2: \"Talk-R\", 3: \"Text-L\", 4: \"Talk-L\",\n",
    "    5: \"Radio\", 6: \"Drink\", 7: \"Reach\", 8: \"Hair\", 9: \"Passenger\"\n",
    "}\n",
    "\n",
    "for i, (_, row) in enumerate(samples.iterrows()):\n",
    "    # Find corresponding hybrid path\n",
    "    full_path = full_data_root / row[\"path\"]\n",
    "    \n",
    "    # Match in hybrid manifest by finding same original image\n",
    "    # The hybrid manifest has \"original_path\" column that links to original images\n",
    "    orig_filename = Path(row[\"path\"]).name\n",
    "    match = hybrid_manifest[hybrid_manifest[\"original_path\"].str.contains(orig_filename, na=False)]\n",
    "    \n",
    "    if len(match) == 0:\n",
    "        continue\n",
    "    hybrid_path = hybrid_data_root / match.iloc[0][\"path\"]\n",
    "    \n",
    "    # Load and process full image\n",
    "    if full_path.exists():\n",
    "        full_img = Image.open(full_path).convert(\"RGB\")\n",
    "        full_img_resized = full_img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        full_np = np.array(full_img_resized) / 255.0\n",
    "        full_tensor = transform(full_img).unsqueeze(0).to(device)\n",
    "        \n",
    "        full_cam_result = full_cam(input_tensor=full_tensor, targets=None)[0]\n",
    "        full_viz = show_cam_on_image(full_np.astype(np.float32), full_cam_result, use_rgb=True)\n",
    "        \n",
    "        axes[i, 0].imshow(full_img_resized)\n",
    "        axes[i, 0].set_title(f\"Full Image\\nTrue: {CLASS_NAMES.get(int(row['label']), row['label'])}\", fontsize=9)\n",
    "        axes[i, 0].axis(\"off\")\n",
    "        \n",
    "        axes[i, 1].imshow(full_viz)\n",
    "        axes[i, 1].set_title(f\"Full Grad-CAM\\nPred: {CLASS_NAMES.get(int(row['pred']), row['pred'])}\", fontsize=9)\n",
    "        axes[i, 1].axis(\"off\")\n",
    "    \n",
    "    # Load and process hybrid crop\n",
    "    if hybrid_path.exists():\n",
    "        hybrid_img = Image.open(hybrid_path).convert(\"RGB\")\n",
    "        hybrid_img_resized = hybrid_img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        hybrid_np = np.array(hybrid_img_resized) / 255.0\n",
    "        hybrid_tensor = transform(hybrid_img).unsqueeze(0).to(device)\n",
    "        \n",
    "        hybrid_cam_result = hybrid_cam(input_tensor=hybrid_tensor, targets=None)[0]\n",
    "        hybrid_viz = show_cam_on_image(hybrid_np.astype(np.float32), hybrid_cam_result, use_rgb=True)\n",
    "        \n",
    "        axes[i, 2].imshow(hybrid_img_resized)\n",
    "        axes[i, 2].set_title(f\"Hybrid Crop\", fontsize=9)\n",
    "        axes[i, 2].axis(\"off\")\n",
    "        \n",
    "        axes[i, 3].imshow(hybrid_viz)\n",
    "        hybrid_row = hybrid_preds[hybrid_preds[\"path\"].str.contains(Path(hybrid_path).name, na=False)]\n",
    "        pred_label = hybrid_row.iloc[0][\"pred\"] if len(hybrid_row) > 0 else \"?\"\n",
    "        axes[i, 3].set_title(f\"Hybrid Grad-CAM\\nPred: {CLASS_NAMES.get(int(pred_label), pred_label)}\", fontsize=9)\n",
    "        axes[i, 3].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Grad-CAM Comparison: Full Image vs Hybrid Crop ({MODEL_NAME})\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "comparison_path = Path(OUT_ROOT) / \"gradcam\" / \"comparison_full_vs_hybrid.png\"\n",
    "comparison_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(comparison_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"\\nüíæ Saved comparison to: {comparison_path}\")\n",
    "print(\"\\nüìù Thesis talking point:\")\n",
    "print(\"   'The full-image model attends to [background/seat/etc] while the\")\n",
    "print(\"    hybrid-crop model focuses on [hand position/facial features/etc],\")\n",
    "print(\"    demonstrating the value of ROI-based preprocessing.'\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
