{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7296b001",
   "metadata": {},
   "source": [
    "\n",
    "# üöÄ Colab Setup ‚Äî **CNNs-distracted-driving** (hardcoded + config-aware)\n",
    "\n",
    "This version is **simplified and hardcoded** for your repo and URL, and it **respects your `src/ddriver/config.py`**.\n",
    "- Repo name fixed to **`CNNs-distracted-driving`**\n",
    "- Repo URL fixed to **`https://github.com/ClaudiaCPach/CNNs-distracted-driving`**\n",
    "- Uses your `config.py` convention: when running in Colab, we **set env vars** (`DRIVE_PATH`, `DATASET_ROOT`, `OUT_ROOT`, `CKPT_ROOT`, `FAST_DATA`) so your code reads correct paths via `ddriver.config`.\n",
    "- Optional `FAST_DATA` at `/content/data` for faster I/O (if you later copy data there).\n",
    "\n",
    "> Run cells **top ‚Üí bottom** the first time. Re-run **Update repo** to pull new commits after you push.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0fd729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîß 0) (Optional) quick GPU check\n",
    "!nvidia-smi || echo \"No GPU detected ‚Äî CPU runtime is okay for setup steps.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîß 1) Fixed config for your repo + Drive layout\n",
    "import os\n",
    "\n",
    "REPO_URL       = \"https://github.com/ClaudiaCPach/CNNs-distracted-driving\"\n",
    "REPO_DIRNAME   = \"CNNs-distracted-driving\"   # hardcoded\n",
    "BRANCH         = \"main\"\n",
    "PROJECT_ROOT   = f\"/content/{REPO_DIRNAME}\"  # where the repo will live in Colab\n",
    "\n",
    "# Your persistent Google Drive base folder (matches your project docs):\n",
    "DRIVE_PATH       = \"/content/drive/MyDrive/TFM\"\n",
    "DRIVE_DATA_ROOT  = f\"{DRIVE_PATH}/data\"      # contains auc.distracted.driver.dataset_v2\n",
    "\n",
    "# Optional: a fast, ephemeral workspace inside the VM\n",
    "FAST_DATA        = \"/content/data\"           # rsync target for faster I/O (lives on the VM SSD)\n",
    "\n",
    "# Start with Drive as the canonical dataset root; later cells can switch to FAST_DATA\n",
    "DATASET_ROOT     = DRIVE_DATA_ROOT\n",
    "OUT_ROOT         = f\"{DRIVE_PATH}/outputs\"\n",
    "CKPT_ROOT        = f\"{DRIVE_PATH}/checkpoints\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîå 2) Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "print(\"‚úÖ Drive mounted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df094a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìÅ 3) Clone or update the repo (no name inference ‚Äî all hardcoded)\n",
    "import os, subprocess\n",
    "\n",
    "def sh(cmd):\n",
    "    print(f\"\\n$ {cmd}\")\n",
    "    rc = subprocess.call(cmd, shell=True, executable=\"/bin/bash\")\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {rc}: {cmd}\")\n",
    "\n",
    "if os.path.isdir(PROJECT_ROOT):\n",
    "    print(f\"üìÅ Repo already present at {PROJECT_ROOT}. Pulling latest on branch {BRANCH}...\")\n",
    "    sh(f\"cd {PROJECT_ROOT} && git fetch origin {BRANCH} && git checkout {BRANCH} && git pull --rebase origin {BRANCH}\")\n",
    "else:\n",
    "    print(f\"‚¨áÔ∏è Cloning {REPO_URL} ‚Üí {PROJECT_ROOT}\")\n",
    "    sh(f\"git clone --branch {BRANCH} {REPO_URL} {PROJECT_ROOT}\")\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üì¶ 4) Install the repo (editable) + requirements (uses pyproject.toml if present)\n",
    "import os, subprocess\n",
    "\n",
    "def sh(cmd):\n",
    "    print(f\"\\n$ {cmd}\")\n",
    "    rc = subprocess.call(cmd, shell=True, executable=\"/bin/bash\")\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {rc}: {cmd}\")\n",
    "\n",
    "print(\"üîÑ Upgrading pip/setuptools/wheel...\")\n",
    "sh(\"python -m pip install --upgrade pip setuptools wheel\")\n",
    "\n",
    "has_pyproject = os.path.exists(os.path.join(PROJECT_ROOT, \"pyproject.toml\"))\n",
    "if has_pyproject:\n",
    "    print(\"üì¶ Editable install from pyproject.toml ...\")\n",
    "    sh(f\"cd {PROJECT_ROOT} && pip install -e .\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No pyproject.toml found. Skipping editable install.\")\n",
    "\n",
    "req_path = os.path.join(PROJECT_ROOT, \"requirements.txt\")\n",
    "if os.path.exists(req_path):\n",
    "    print(\"üìù Installing requirements.txt...\")\n",
    "    sh(f\"pip install -r {req_path}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No requirements.txt found ‚Äî continuing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accfc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üå≥ 5) Configure environment for your ddriver.config (Colab branch)\n",
    "# Your config.py reads env vars and falls back to sensible defaults when in Colab.\n",
    "import os\n",
    "\n",
    "os.environ[\"DRIVE_PATH\"]   = DRIVE_PATH\n",
    "os.environ[\"DATASET_ROOT\"] = DATASET_ROOT\n",
    "os.environ[\"OUT_ROOT\"]     = OUT_ROOT\n",
    "os.environ[\"CKPT_ROOT\"]    = CKPT_ROOT\n",
    "os.environ[\"FAST_DATA\"]    = FAST_DATA\n",
    "\n",
    "# Also write a .env (harmless in Colab; helpful if code calls load_dotenv())\n",
    "env_text = f\"\"\"DRIVE_PATH={DRIVE_PATH}\n",
    "DATASET_ROOT={DATASET_ROOT}\n",
    "OUT_ROOT={OUT_ROOT}\n",
    "CKPT_ROOT={CKPT_ROOT}\n",
    "FAST_DATA={FAST_DATA}\n",
    "\"\"\"\n",
    "with open(os.path.join(PROJECT_ROOT, \".env\"), \"w\") as f:\n",
    "    f.write(env_text)\n",
    "\n",
    "print(\"‚úÖ Environment variables set for ddriver.config\")\n",
    "print(\"\\nSummary:\")\n",
    "for k in [\"DRIVE_PATH\",\"DATASET_ROOT\",\"OUT_ROOT\",\"CKPT_ROOT\",\"FAST_DATA\"]:\n",
    "    print(f\"{k} = {os.environ[k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ 8) Import smoke test (uses your package + config.py)\n",
    "import sys, os\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "sys.path.append(os.path.join(PROJECT_ROOT, \"src\"))  # <‚Äî lets Python find src/ddriver\n",
    "\n",
    "try:\n",
    "    import ddriver\n",
    "    print(\"ddriver imported OK from:\", ddriver.__file__)\n",
    "    # Confirm config picks up Colab env:\n",
    "    try:\n",
    "        from ddriver import config\n",
    "        print(\"Loaded ddriver.config successfully.\")\n",
    "        # Echo the resolved paths from config (they are pathlib.Path objects)\n",
    "        print(\"config.DATASET_ROOT =\", config.DATASET_ROOT)\n",
    "        print(\"config.OUT_ROOT     =\", config.OUT_ROOT)\n",
    "        print(\"config.CKPT_ROOT    =\", config.CKPT_ROOT)\n",
    "        print(\"config.FAST_DATA    =\", config.FAST_DATA)\n",
    "    except Exception as e:\n",
    "        print(\"Note: ddriver.config not imported:\", e)\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Import failed ‚Äî check package name/setup.\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374139f5",
   "metadata": {},
   "source": [
    "# üìã 9) Generate Manifest and Split CSVs\n",
    "\n",
    "This step creates the CSV files that tell your code where all the images are and which ones go to train/val/test.\n",
    "\n",
    "**What this does:**\n",
    "- Scans all your images in the dataset folder\n",
    "- Creates a big list (manifest.csv) with info about every image\n",
    "- Creates three smaller lists (train.csv, val.csv, test.csv) that say which images belong where\n",
    "- Saves everything to your Google Drive so it's permanent\n",
    "\n",
    "**Why we need this:**\n",
    "- Your training code needs to know which images to use\n",
    "- The manifest remembers which driver each image belongs to (for VAL split)\n",
    "- The split CSVs organize images into train/val/test groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8883386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the manifest generator\n",
    "# This is like asking a librarian to catalog all your books and create reading lists\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Make sure we can import ddriver\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Run the manifest script\n",
    "# --write-split-lists means \"also create train.csv, val.csv, test.csv files\"\n",
    "manifest_cmd = f\"cd {PROJECT_ROOT} && python -m ddriver.data.manifest --write-split-lists\"\n",
    "\n",
    "print(\"üî® Generating manifest and split CSVs...\")\n",
    "print(f\"Running: {manifest_cmd}\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    manifest_cmd,\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Show what happened\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Warnings/Errors:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n‚úÖ Manifest and split CSVs generated successfully!\")\n",
    "    print(f\"   Manifest: {os.environ['OUT_ROOT']}/manifests/manifest.csv\")\n",
    "    print(f\"   Train split: {os.environ['OUT_ROOT']}/splits/train.csv\")\n",
    "    print(f\"   Val split: {os.environ['OUT_ROOT']}/splits/val.csv\")\n",
    "    print(f\"   Test split: {os.environ['OUT_ROOT']}/splits/test.csv\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Error generating manifest (exit code {result.returncode})\")\n",
    "    raise RuntimeError(\"Manifest generation failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: Did the CSVs get created?\n",
    "# This is like checking that the librarian actually wrote down all the book lists\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "manifest_path = Path(os.environ['OUT_ROOT']) / \"manifests\" / \"manifest.csv\"\n",
    "train_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"train.csv\"\n",
    "val_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"val.csv\"\n",
    "test_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"test.csv\"\n",
    "\n",
    "print(\"üìä Checking CSV files...\\n\")\n",
    "\n",
    "for name, path in [(\"Manifest\", manifest_path), (\"Train\", train_path), (\"Val\", val_path), (\"Test\", test_path)]:\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"‚úÖ {name}: {len(df)} rows, columns: {list(df.columns)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name}: File not found at {path}\")\n",
    "\n",
    "# Show a sample from the manifest\n",
    "if manifest_path.exists():\n",
    "    print(\"\\nüìÑ Sample from manifest (first 3 rows):\")\n",
    "    sample = pd.read_csv(manifest_path).head(3)\n",
    "    print(sample[['path', 'class_id', 'driver_id', 'camera', 'split']].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc2b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny balanced subset for quick testing\n",
    "# Run this cell ONCE to create train_small.csv, then use it for fast experiments\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from ddriver import config\n",
    "\n",
    "train_csv = Path(config.OUT_ROOT) / \"splits\" / \"train.csv\"\n",
    "train_small_csv = Path(config.OUT_ROOT) / \"splits\" / \"train_small.csv\"\n",
    "\n",
    "print(f\"Reading {train_csv}...\")\n",
    "df = pd.read_csv(train_csv)\n",
    "\n",
    "# Get 20 images per class (balanced)\n",
    "small = df.groupby(\"class_id\").head(20)\n",
    "\n",
    "print(f\"Original train.csv: {len(df)} images\")\n",
    "print(f\"Small subset: {len(small)} images ({len(small) // 10} per class)\")\n",
    "print(f\"\\nClass distribution in small subset:\")\n",
    "print(small[\"class_id\"].value_counts().sort_index())\n",
    "\n",
    "small.to_csv(train_small_csv, index=False)\n",
    "print(f\"\\n‚úÖ Saved to {train_small_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c222e9",
   "metadata": {},
   "source": [
    "### ‚ö°Ô∏è Tiny-train option\n",
    "\n",
    "Set `USE_TINY_SPLIT = True` in the training cell below to replace the heavy\n",
    "`train.csv` with the quick `train_small.csv` (20 images per class). Validation\n",
    "and test splits stay full so you still see realistic metrics.\n",
    "\n",
    "Run the \"Create a tiny balanced subset\" cell once per Drive setup before\n",
    "enabling this flag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041552b4",
   "metadata": {},
   "source": [
    "# üß™ 10) Test dataset.py and datamod.py\n",
    "\n",
    "Now let's make sure the code that loads images actually works!\n",
    "\n",
    "**What we're testing:**\n",
    "1. **dataset.py** - Can it load a single image and give us the right info?\n",
    "2. **datamod.py** - Can it create data loaders that give us batches of images?\n",
    "\n",
    "**Why test this:**\n",
    "- If these don't work, training will fail\n",
    "- Better to catch problems now than later\n",
    "- We want to see that images load correctly and labels are right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501a4c7",
   "metadata": {},
   "source": [
    "## üîç MediaPipe Crop Quality Audit\n",
    "\n",
    "**How to run a FAST audit (recommended):**\n",
    "1. Run the \"Copy crops to /content\" cell 32 below (copies from Drive to fast local SSD)\n",
    "2. Run the audit cell (it auto-detects the local copy and uses it)\n",
    "\n",
    "**Two modes:**\n",
    "- **Full mode**: Uses `detection_metadata_{variant}.csv` (has face/hand detection info)\n",
    "- **Lite mode**: Uses `manifest_{variant}.csv` (infers fallback from crop dimensions)\n",
    "\n",
    "**What you get:**\n",
    "- Numeric stats: fallback rates, ROI area/aspect distributions\n",
    "- Breakdowns by class/camera/split\n",
    "- Visual grids: \"worst suspects\" (tiny crops, fallbacks, extreme aspects)\n",
    "- Per-class sample grids\n",
    "\n",
    "**Path conventions:** All CSVs store relative paths. At runtime, paths are resolved using `config.OUT_ROOT` or `config.FAST_DATA` depending on where the crops are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Run MediaPipe Crop Quality Audit\n",
    "# Auto-detects whether crops are in /content (fast) or Drive, and which mode to use.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ddriver import config\n",
    "from ddriver.data.mediapipe_audit import generate_audit_report, get_crop_root\n",
    "\n",
    "VARIANT = \"face_hands\"  # must match the variant you extracted\n",
    "\n",
    "# Auto-detect crop root (prefers FAST_DATA if available)\n",
    "crop_root = get_crop_root(prefer_fast=True)\n",
    "print(f\"üìÅ Using crop root: {crop_root}\")\n",
    "\n",
    "# Look for metadata/manifest CSVs in the same location\n",
    "metadata_csv = crop_root.parent / f\"detection_metadata_{VARIANT}.csv\"\n",
    "manifest_csv = crop_root.parent / f\"manifest_{VARIANT}.csv\"\n",
    "\n",
    "# Fall back to OUT_ROOT if not found in FAST_DATA\n",
    "if not metadata_csv.exists() and not manifest_csv.exists():\n",
    "    metadata_csv = config.OUT_ROOT / \"mediapipe\" / f\"detection_metadata_{VARIANT}.csv\"\n",
    "    manifest_csv = config.OUT_ROOT / \"mediapipe\" / f\"manifest_{VARIANT}.csv\"\n",
    "\n",
    "# Output directory (always on Drive for persistence)\n",
    "audit_output = config.OUT_ROOT / \"mediapipe\" / \"audit\" / VARIANT\n",
    "\n",
    "# Run the audit\n",
    "if metadata_csv.exists():\n",
    "    print(f\"‚úÖ Found detection metadata: {metadata_csv}\")\n",
    "    audit_result = generate_audit_report(\n",
    "        metadata_csv=metadata_csv,\n",
    "        crop_root=crop_root,\n",
    "        output_dir=audit_output,\n",
    "        variant=VARIANT,\n",
    "        n_samples=25,\n",
    "        save_figures=True,\n",
    "        show_figures=True,\n",
    "    )\n",
    "elif manifest_csv.exists():\n",
    "    print(f\"‚ö†Ô∏è Using manifest (lite mode): {manifest_csv}\")\n",
    "    audit_result = generate_audit_report(\n",
    "        manifest_csv=manifest_csv,\n",
    "        crop_root=crop_root,\n",
    "        output_dir=audit_output,\n",
    "        variant=VARIANT,\n",
    "        n_samples=25,\n",
    "        save_figures=True,\n",
    "        show_figures=True,\n",
    "    )\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Neither metadata nor manifest found. Run extraction first.\\n\"\n",
    "        f\"  Checked: {metadata_csv}\\n\"\n",
    "        f\"  Checked: {manifest_csv}\"\n",
    "    )\n",
    "\n",
    "# Store results for later cells\n",
    "stats = audit_result[\"stats\"]\n",
    "breakdowns = audit_result[\"breakdowns\"]\n",
    "lite_mode = audit_result[\"lite_mode\"]\n",
    "crop_root = audit_result[\"crop_root\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Audit complete! Outputs saved to: {audit_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec6bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Display Audit Summary Stats (uses results from previous cell)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä DETECTION SUMMARY STATS\")\n",
    "if lite_mode:\n",
    "    print(\"   [LITE MODE - face/hand detection info not available]\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total images processed: {stats['total_images']}\")\n",
    "\n",
    "# Face/hand detection (full mode only)\n",
    "if not lite_mode and \"face_detected_pct\" in stats:\n",
    "    print(f\"\\nüéØ Detection rates:\")\n",
    "    print(f\"   Face detected: {stats['face_detected_pct']:.1f}%\")\n",
    "    print(f\"   Hands: 0={stats['hands_0_pct']:.1f}%, 1={stats['hands_1_pct']:.1f}%, 2={stats['hands_2_pct']:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Fallback to full frame: {stats['fallback_count']} ({stats['fallback_pct']:.1f}%)\")\n",
    "print(f\"   Fallback reasons: {stats['fallback_reasons']}\")\n",
    "\n",
    "print(f\"\\nüìê ROI statistics:\")\n",
    "print(f\"   Area fraction: mean={stats['roi_area_frac_mean']:.3f}, std={stats['roi_area_frac_std']:.3f}\")\n",
    "print(f\"   Area percentiles: 5%={stats['roi_area_frac_p5']:.3f}, 25%={stats['roi_area_frac_p25']:.3f}, 50%={stats['roi_area_frac_median']:.3f}\")\n",
    "print(f\"   Aspect ratio: mean={stats['roi_aspect_mean']:.3f}, min={stats['roi_aspect_min']:.3f}, max={stats['roi_aspect_max']:.3f}\")\n",
    "\n",
    "# Detection types (full mode only)\n",
    "if not lite_mode and \"detection_used_distribution\" in stats:\n",
    "    print(f\"\\nüè∑Ô∏è  Detection types used:\")\n",
    "    for dtype, count in stats['detection_used_distribution'].items():\n",
    "        pct = 100 * count / stats['total_images']\n",
    "        print(f\"   {dtype}: {count} ({pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e75359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Display Breakdown by Class and Camera (uses results from audit cell)\n",
    "import pandas as pd\n",
    "\n",
    "# Class breakdown\n",
    "if \"class_id\" in breakdowns:\n",
    "    print(\"üìã BREAKDOWN BY CLASS\")\n",
    "    print(\"-\" * 100)\n",
    "    print(breakdowns[\"class_id\"].to_string(index=False))\n",
    "    print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Class breakdown not available\")\n",
    "\n",
    "# Camera breakdown\n",
    "if \"camera\" in breakdowns:\n",
    "    print(\"üìã BREAKDOWN BY CAMERA\")\n",
    "    print(\"-\" * 100)\n",
    "    print(breakdowns[\"camera\"].to_string(index=False))\n",
    "    print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Camera breakdown not available\")\n",
    "\n",
    "# Split breakdown (train/val/test)\n",
    "if \"split\" in breakdowns:\n",
    "    print(\"üìã BREAKDOWN BY SPLIT\")\n",
    "    print(\"-\" * 100)\n",
    "    print(breakdowns[\"split\"].to_string(index=False))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Split breakdown not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55662c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñºÔ∏è Re-display saved grids from disk (if you need to see them again)\n",
    "# Note: Grids were already shown inline when you ran the audit cell above!\n",
    "\n",
    "from IPython.display import display, Image as IPImage\n",
    "from ddriver import config\n",
    "\n",
    "VARIANT = \"face_hands\"\n",
    "audit_output = config.OUT_ROOT / \"mediapipe\" / \"audit\" / VARIANT\n",
    "\n",
    "print(\"üìÅ Saved grids location:\", audit_output)\n",
    "print(\"   (These were already displayed inline during the audit)\\n\")\n",
    "\n",
    "# List what's available\n",
    "grids = [\n",
    "    (\"grid_area_small.png\", \"üî¨ Smallest ROI crops\"),\n",
    "    (\"grid_fallback.png\", \"‚ö†Ô∏è Fallback to full frame\"),\n",
    "    (\"grid_aspect_extreme.png\", \"üìê Extreme aspect ratios\"),\n",
    "]\n",
    "# Add full-mode only grids\n",
    "if not lite_mode:\n",
    "    grids.extend([\n",
    "        (\"grid_no_hands.png\", \"üë§ Face detected but no hands\"),\n",
    "        (\"grid_one_hand.png\", \"‚úã Only one hand detected\"),\n",
    "    ])\n",
    "\n",
    "for filename, title in grids:\n",
    "    grid_path = audit_output / filename\n",
    "    if grid_path.exists():\n",
    "        print(f\"‚úÖ {title}: {grid_path.name}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {title}: not found\")\n",
    "\n",
    "# Uncomment below to re-display a specific grid:\n",
    "# display(IPImage(filename=str(audit_output / \"grid_area_small.png\"), width=900))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec581c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üè∑Ô∏è Per-Class Sample Grids info\n",
    "# Note: These were already displayed inline during the audit!\n",
    "\n",
    "from IPython.display import display, Image as IPImage\n",
    "from ddriver import config\n",
    "\n",
    "VARIANT = \"face_hands\"\n",
    "audit_output = config.OUT_ROOT / \"mediapipe\" / \"audit\" / VARIANT\n",
    "\n",
    "# Class labels for reference\n",
    "CLASS_LABELS = {\n",
    "    0: \"c0 - Safe driving\",\n",
    "    1: \"c1 - Texting (right)\",\n",
    "    2: \"c2 - Phone (right)\",\n",
    "    3: \"c3 - Texting (left)\",\n",
    "    4: \"c4 - Phone (left)\",\n",
    "    5: \"c5 - Radio\",\n",
    "    6: \"c6 - Drinking\",\n",
    "    7: \"c7 - Reaching behind\",\n",
    "    8: \"c8 - Hair/makeup\",\n",
    "    9: \"c9 - Talking to passenger\",\n",
    "}\n",
    "\n",
    "print(\"üìÅ Per-class grids saved at:\", audit_output)\n",
    "print(\"   (Already displayed inline during audit)\\n\")\n",
    "\n",
    "# Check what's available\n",
    "for class_id in range(10):\n",
    "    grid_path = audit_output / f\"grid_class_{class_id}.png\"\n",
    "    label = CLASS_LABELS.get(class_id, f\"Class {class_id}\")\n",
    "    if grid_path.exists():\n",
    "        print(f\"‚úÖ {label}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {label}: not found\")\n",
    "\n",
    "# Uncomment to re-display specific class grids:\n",
    "# for class_id in [0, 1, 7]:  # Adjust class IDs as needed\n",
    "#     grid_path = audit_output / f\"grid_class_{class_id}.png\"\n",
    "#     if grid_path.exists():\n",
    "#         print(f\"\\nüè∑Ô∏è {CLASS_LABELS.get(class_id, f'Class {class_id}')}\")\n",
    "#         display(IPImage(filename=str(grid_path), width=900))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6044f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Can dataset.py load a single image?\n",
    "# This is like testing if a worker can fetch one book from the library\n",
    "\n",
    "from ddriver.data.dataset import AucDriverDataset\n",
    "from torchvision import transforms as T\n",
    "from pathlib import Path\n",
    "\n",
    "# Get paths from config\n",
    "manifest_csv = Path(os.environ['OUT_ROOT']) / \"manifests\" / \"manifest.csv\"\n",
    "val_split_csv = Path(os.environ['OUT_ROOT']) / \"splits\" / \"val.csv\"\n",
    "\n",
    "print(\"üß™ Test 1: Testing AucDriverDataset (dataset.py)\")\n",
    "print(f\"   Manifest: {manifest_csv}\")\n",
    "print(f\"   Using Val split: {val_split_csv}\\n\")\n",
    "\n",
    "try:\n",
    "    # Create a simple dataset (no fancy transforms, just load the image)\n",
    "    simple_transforms = T.ToTensor()  # Just convert to tensor, no augmentation\n",
    "    \n",
    "    val_dataset = AucDriverDataset(\n",
    "        manifest_csv=manifest_csv,\n",
    "        split_csv=val_split_csv,\n",
    "        transforms=simple_transforms\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset created! It has {len(val_dataset)} images in VAL split\")\n",
    "    \n",
    "    # Try to load the first image\n",
    "    print(\"\\nüìñ Loading first image from VAL split...\")\n",
    "    sample = val_dataset[0]\n",
    "    \n",
    "    print(f\"‚úÖ Image loaded successfully!\")\n",
    "    print(f\"   Image shape: {sample['image'].shape} (should be [3, height, width])\")\n",
    "    print(f\"   Label: {sample['label']} (should be 0-9)\")\n",
    "    print(f\"   Driver ID: {sample['driver_id']} (VAL should have driver IDs)\")\n",
    "    print(f\"   Camera: {sample['camera']} (should be 'cam1' or 'cam2')\")\n",
    "    print(f\"   Path: {sample['path'][:80]}...\")  # Show first 80 chars\n",
    "    \n",
    "    # Check that label is valid (0-9)\n",
    "    if 0 <= sample['label'] <= 9:\n",
    "        print(f\"   ‚úÖ Label is valid (0-9)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Label {sample['label']} is NOT in range 0-9!\")\n",
    "    \n",
    "    # Check that VAL has driver IDs\n",
    "    if sample['driver_id'] is not None:\n",
    "        print(f\"   ‚úÖ VAL split has driver ID (as expected)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  VAL split missing driver ID (might be okay if this image wasn't in your DRIVER_RANGES)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Test 1 PASSED: dataset.py works!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 1 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286fdbe",
   "metadata": {},
   "source": [
    "# üßµ 11) Full pipeline (train ‚Üí predict ‚Üí metrics)\n",
    "\n",
    "Now that data loading is working, these next cells show how to:\n",
    "1. Register the model you want (e.g., `resnet18` from timm)\n",
    "2. Run training from the command line helper\n",
    "3. Generate predictions from a checkpoint\n",
    "4. Evaluate metrics and save all logs to Drive\n",
    "\n",
    "> You can change the `RUN_TAG`, model name, epochs, etc. in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register models you want to use (run once per runtime)\n",
    "# This example uses timm's convnext_tiny.\n",
    "\n",
    "!pip -q install timm\n",
    "\n",
    "from ddriver.models import registry\n",
    "\n",
    "registry.register_timm_backbone(\"efficientnet_b0\")\n",
    "print(\"Available models:\", registry.available_models()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"mediapipe==0.10.14\" \"protobuf<5\" \"opencv-python-headless<4.11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß≠ Generate MediaPipe ROI crops (face, hands, face+hands)\n",
    "# Run once per runtime/variant. Produces new manifest/splits under OUT_ROOT/mediapipe.\n",
    "!pip -q install mediapipe opencv-python-headless\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "VARIANT = \"face_hands\"  # choose: face | hands | face_hands\n",
    "OUTPUT_ROOT = Path(OUT_ROOT) / \"mediapipe\"\n",
    "manifest_csv = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "splits_root = Path(OUT_ROOT) / \"splits\"\n",
    "\n",
    "extract_cmd = f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.data.mediapipe_extract \\\n",
    "  --manifest {manifest_csv} \\\n",
    "  --splits-root {splits_root} \\\n",
    "  --dataset-root {DATASET_ROOT} \\\n",
    "  --output-root {OUTPUT_ROOT} \\\n",
    "  --variant {VARIANT} \\\n",
    "  --max-side 720 \\\n",
    "  --model-complexity 2 \\\n",
    "  --min-detection-area-frac 0.05 \\\n",
    "  --min-area-frac 0.10 \\\n",
    "  --min-aspect 0.20 \\\n",
    "  --pad-frac 0.20 \\\n",
    "  --face-extra-down-frac 0.35 \\\n",
    "  --overwrite\n",
    "\"\"\"\n",
    "\n",
    "print(\"Running MediaPipe extraction for variant:\", VARIANT)\n",
    "print(extract_cmd)\n",
    "proc = subprocess.Popen(\n",
    "    extract_cmd,\n",
    "    shell=True,\n",
    "    text=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "if proc.stdout is None:\n",
    "    raise RuntimeError(\"Extraction process has no stdout pipe.\")\n",
    "for line in proc.stdout:\n",
    "    print(line, end=\"\")\n",
    "proc.wait()\n",
    "if proc.returncode != 0:\n",
    "    raise RuntimeError(\"MediaPipe extraction failed. Check logs above.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76811eb",
   "metadata": {},
   "source": [
    "## üéØ YOLO-World ROI Extraction (Alternative to MediaPipe)\n",
    "\n",
    "YOLO-World uses open-vocabulary detection to find faces and hands without custom training.\n",
    "This is an alternative to the MediaPipe pipeline above.\n",
    "\n",
    "**Advantages over MediaPipe:**\n",
    "- Better detection accuracy for occluded/partial views\n",
    "- Confidence scores for filtering\n",
    "- Faster inference on GPU\n",
    "\n",
    "**Choose ONE pipeline:** Either run MediaPipe extraction OR YOLO extraction, not both.\n",
    "The training cell below lets you pick which pipeline's crops to use (`USE_MEDIAPIPE` vs `USE_YOLO`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Generate YOLO-World ROI crops (face, hands, face+hands)\n",
    "# Alternative to MediaPipe - uses open-vocabulary detection.\n",
    "# Run once per runtime/variant. Produces new manifest/splits under OUT_ROOT/yolo.\n",
    "\n",
    "!pip -q install ultralytics\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "VARIANT = \"face_hands\"  # choose: face | hands | face_hands\n",
    "YOLO_OUTPUT_ROOT = Path(OUT_ROOT) / \"yolo\"\n",
    "manifest_csv = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "splits_root = Path(OUT_ROOT) / \"splits\"\n",
    "\n",
    "# ===== TEST MODE OPTIONS (toggle these!) =====\n",
    "# Option 1: Use train_small.csv for quick testing (~200 images)\n",
    "TEST_MODE = True  # Set False for full extraction\n",
    "SAMPLE_CSV = Path(OUT_ROOT) / \"splits\" / \"train_small.csv\"  # Small balanced subset\n",
    "\n",
    "# Option 2: Limit to first N images (even faster for debugging)\n",
    "LIMIT = None  # Set to e.g. 50 for super quick test, None for no limit\n",
    "\n",
    "# Build command\n",
    "sample_flag = f\"--sample-csv {SAMPLE_CSV}\" if TEST_MODE and SAMPLE_CSV.exists() else \"\"\n",
    "limit_flag = f\"--limit {LIMIT}\" if LIMIT else \"\"\n",
    "\n",
    "extract_cmd = f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.data.yolo_extract \\\n",
    "  --manifest {manifest_csv} \\\n",
    "  --splits-root {splits_root} \\\n",
    "  --dataset-root {DATASET_ROOT} \\\n",
    "  --output-root {YOLO_OUTPUT_ROOT} \\\n",
    "  --variant {VARIANT} \\\n",
    "  --model-size m \\\n",
    "  --confidence 0.15 \\\n",
    "  --min-detection-area-frac 0.03 \\\n",
    "  --min-area-frac 0.08 \\\n",
    "  --min-aspect 0.20 \\\n",
    "  --pad-frac 0.20 \\\n",
    "  {sample_flag} \\\n",
    "  {limit_flag} \\\n",
    "  --overwrite\n",
    "\"\"\"\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\"‚ö° TEST MODE: Using small sample for quick testing\")\n",
    "    print(f\"   Sample CSV: {SAMPLE_CSV}\")\n",
    "    if LIMIT:\n",
    "        print(f\"   Limit: {LIMIT} images\")\n",
    "else:\n",
    "    print(\"ü™µ FULL MODE: Processing all images\")\n",
    "\n",
    "print(f\"\\nRunning YOLO-World extraction for variant: {VARIANT}\")\n",
    "print(extract_cmd)\n",
    "proc = subprocess.Popen(\n",
    "    extract_cmd,\n",
    "    shell=True,\n",
    "    text=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "if proc.stdout is None:\n",
    "    raise RuntimeError(\"Extraction process has no stdout pipe.\")\n",
    "for line in proc.stdout:\n",
    "    print(line, end=\"\")\n",
    "proc.wait()\n",
    "if proc.returncode != 0:\n",
    "    raise RuntimeError(\"YOLO extraction failed. Check logs above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5c186",
   "metadata": {},
   "source": [
    "## üöö Copy YOLO crops to /content (optional, faster I/O)\n",
    "\n",
    "Use this if training/audit from Drive is slow. It copies the generated YOLO crops/CSVs into `/content/data/yolo/<variant>` and updates paths for the current runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöö Copy YOLO crops to /content for faster I/O\n",
    "import os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your variant to the one you extracted already\n",
    "YOLO_VARIANT = \"face_hands\"  # face | hands | face_hands\n",
    "SRC_ROOT = Path(OUT_ROOT) / \"yolo\"\n",
    "SRC_VARIANT_DIR = SRC_ROOT / YOLO_VARIANT\n",
    "DST_ROOT = Path(\"/content/data/yolo\") / YOLO_VARIANT\n",
    "\n",
    "if not SRC_VARIANT_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Source YOLO folder not found: {SRC_VARIANT_DIR}\\nRun the YOLO extraction cell first.\")\n",
    "\n",
    "print(f\"Copying YOLO crops from {SRC_VARIANT_DIR} -> {DST_ROOT}\")\n",
    "DST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy images directory tree\n",
    "file_count = 0\n",
    "for src_dir, _, files in os.walk(SRC_VARIANT_DIR):\n",
    "    rel_dir = Path(src_dir).relative_to(SRC_VARIANT_DIR)\n",
    "    dst_dir = DST_ROOT / rel_dir\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for fname in files:\n",
    "        src_path = Path(src_dir) / fname\n",
    "        dst_path = dst_dir / fname\n",
    "        if not dst_path.exists():\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            file_count += 1\n",
    "print(f\"   Copied {file_count} image files\")\n",
    "\n",
    "# Copy manifest/split CSVs that live one level above the variant folder\n",
    "csv_names = [\n",
    "    f\"manifest_{YOLO_VARIANT}.csv\",\n",
    "    f\"train_{YOLO_VARIANT}.csv\",\n",
    "    f\"val_{YOLO_VARIANT}.csv\",\n",
    "    f\"test_{YOLO_VARIANT}.csv\",\n",
    "    f\"detection_metadata_{YOLO_VARIANT}.csv\",  # for auditing\n",
    "]\n",
    "for fname in csv_names:\n",
    "    src_csv = SRC_ROOT / fname\n",
    "    if not src_csv.exists():\n",
    "        print(f\"   ‚ö†Ô∏è Skipping {fname} (not found)\")\n",
    "        continue\n",
    "    dst_csv = DST_ROOT.parent / fname\n",
    "    shutil.copy2(src_csv, dst_csv)\n",
    "    print(f\"   ‚úÖ Copied {fname}\")\n",
    "\n",
    "# Point env vars for this runtime to the local YOLO copy\n",
    "os.environ[\"YOLO_ROOT_LOCAL\"] = str(DST_ROOT.parent)\n",
    "print(f\"\\n‚úÖ Copy complete! Set USE_YOLO=True in training cell.\")\n",
    "print(f\"   YOLO_ROOT_LOCAL = {os.environ['YOLO_ROOT_LOCAL']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223023bd",
   "metadata": {},
   "source": [
    "## üîç YOLO Crop Quality Audit\n",
    "\n",
    "Quick stats and visual inspection of YOLO-World crops. Uses `detection_metadata_{variant}.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç YOLO Crop Quality Audit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ddriver import config\n",
    "\n",
    "VARIANT = \"face_hands\"  # must match the variant you extracted\n",
    "\n",
    "# Auto-detect: prefer local copy if available\n",
    "yolo_root_local = Path(os.environ.get(\"YOLO_ROOT_LOCAL\", \"\"))\n",
    "if yolo_root_local.exists():\n",
    "    yolo_root = yolo_root_local\n",
    "else:\n",
    "    yolo_root = Path(OUT_ROOT) / \"yolo\"\n",
    "\n",
    "metadata_csv = yolo_root / f\"detection_metadata_{VARIANT}.csv\"\n",
    "if not metadata_csv.exists():\n",
    "    raise FileNotFoundError(f\"Detection metadata not found: {metadata_csv}\\nRun YOLO extraction first.\")\n",
    "\n",
    "print(f\"üìÅ Loading metadata from: {metadata_csv}\")\n",
    "df = pd.read_csv(metadata_csv)\n",
    "\n",
    "# Summary stats\n",
    "n_total = len(df)\n",
    "n_fallback = df[\"fallback_to_full\"].sum()\n",
    "n_face = (df[\"face_count\"] > 0).sum()\n",
    "n_hands = (df[\"hand_count\"] > 0).sum()\n",
    "n_face_and_hands = ((df[\"face_count\"] > 0) & (df[\"hand_count\"] > 0)).sum()\n",
    "avg_face_conf = df.loc[df[\"face_confidence\"] > 0, \"face_confidence\"].mean()\n",
    "avg_hand_conf = df.loc[df[\"hand_confidence\"] > 0, \"hand_confidence\"].mean()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä YOLO DETECTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total images: {n_total}\")\n",
    "print(f\"\\nüéØ Detection rates:\")\n",
    "print(f\"   Face detected: {n_face} ({100*n_face/n_total:.1f}%)\")\n",
    "print(f\"   Hands detected: {n_hands} ({100*n_hands/n_total:.1f}%)\")\n",
    "print(f\"   Both face+hands: {n_face_and_hands} ({100*n_face_and_hands/n_total:.1f}%)\")\n",
    "print(f\"\\nüìà Confidence scores:\")\n",
    "print(f\"   Avg face confidence: {avg_face_conf:.3f}\" if not np.isnan(avg_face_conf) else \"   Avg face confidence: N/A\")\n",
    "print(f\"   Avg hand confidence: {avg_hand_conf:.3f}\" if not np.isnan(avg_hand_conf) else \"   Avg hand confidence: N/A\")\n",
    "print(f\"\\n‚ö†Ô∏è  Fallback to full frame: {n_fallback} ({100*n_fallback/n_total:.1f}%)\")\n",
    "\n",
    "# Fallback reason breakdown\n",
    "fallback_df = df[df[\"fallback_to_full\"]]\n",
    "if len(fallback_df) > 0:\n",
    "    print(f\"   Fallback reasons:\")\n",
    "    for reason, count in fallback_df[\"fallback_reason\"].value_counts().items():\n",
    "        print(f\"      - {reason}: {count} ({100*count/n_total:.1f}%)\")\n",
    "\n",
    "# ROI stats (for non-fallback images)\n",
    "non_fallback = df[~df[\"fallback_to_full\"]]\n",
    "if len(non_fallback) > 0:\n",
    "    print(f\"\\nüìê ROI statistics (non-fallback only, n={len(non_fallback)}):\")\n",
    "    print(f\"   Raw detection area: mean={non_fallback['raw_detection_area_frac'].mean():.3f}, \"\n",
    "          f\"std={non_fallback['raw_detection_area_frac'].std():.3f}\")\n",
    "    print(f\"   Final ROI area: mean={non_fallback['roi_area_frac'].mean():.3f}, \"\n",
    "          f\"std={non_fallback['roi_area_frac'].std():.3f}\")\n",
    "    print(f\"   Aspect ratio: mean={non_fallback['roi_aspect'].mean():.3f}, \"\n",
    "          f\"min={non_fallback['roi_aspect'].min():.3f}, max={non_fallback['roi_aspect'].max():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3602de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã YOLO Breakdown by Camera and Class\n",
    "print(\"\\nüìã BREAKDOWN BY CAMERA\")\n",
    "print(\"-\" * 80)\n",
    "camera_stats = df.groupby(\"camera\").agg({\n",
    "    \"fallback_to_full\": [\"sum\", \"mean\"],\n",
    "    \"roi_area_frac\": \"mean\",\n",
    "    \"face_count\": lambda x: (x > 0).mean(),\n",
    "    \"hand_count\": lambda x: (x > 0).mean(),\n",
    "}).round(3)\n",
    "camera_stats.columns = [\"fallback_count\", \"fallback_pct\", \"mean_roi_area\", \"face_rate\", \"hand_rate\"]\n",
    "camera_stats[\"fallback_pct\"] = (camera_stats[\"fallback_pct\"] * 100).round(1)\n",
    "camera_stats[\"face_rate\"] = (camera_stats[\"face_rate\"] * 100).round(1)\n",
    "camera_stats[\"hand_rate\"] = (camera_stats[\"hand_rate\"] * 100).round(1)\n",
    "print(camera_stats.to_string())\n",
    "\n",
    "print(\"\\nüìã BREAKDOWN BY CLASS\")\n",
    "print(\"-\" * 80)\n",
    "class_stats = df.groupby(\"class_id\").agg({\n",
    "    \"fallback_to_full\": [\"sum\", \"mean\"],\n",
    "    \"roi_area_frac\": \"mean\",\n",
    "    \"face_count\": lambda x: (x > 0).mean(),\n",
    "    \"hand_count\": lambda x: (x > 0).mean(),\n",
    "}).round(3)\n",
    "class_stats.columns = [\"fallback_count\", \"fallback_pct\", \"mean_roi_area\", \"face_rate\", \"hand_rate\"]\n",
    "class_stats[\"fallback_pct\"] = (class_stats[\"fallback_pct\"] * 100).round(1)\n",
    "class_stats[\"face_rate\"] = (class_stats[\"face_rate\"] * 100).round(1)\n",
    "class_stats[\"hand_rate\"] = (class_stats[\"hand_rate\"] * 100).round(1)\n",
    "print(class_stats.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ef94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñºÔ∏è Visual Sample Grid - YOLO Crops\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def show_sample_grid(df_subset, title, crop_root, n_samples=12, n_cols=4):\n",
    "    \"\"\"Display a grid of sample crops from a DataFrame subset.\"\"\"\n",
    "    samples = df_subset.sample(n=min(n_samples, len(df_subset)), random_state=42)\n",
    "    n_rows = (len(samples) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]\n",
    "    \n",
    "    for ax, (_, row) in zip(axes, samples.iterrows()):\n",
    "        crop_path = crop_root / row[\"cropped_path\"]\n",
    "        if crop_path.exists():\n",
    "            img = cv2.imread(str(crop_path))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            ax.imshow(img)\n",
    "            label = f\"c{int(row['class_id'])} | area={row['roi_area_frac']:.2f}\"\n",
    "            if row[\"fallback_to_full\"]:\n",
    "                label += \" [FALLBACK]\"\n",
    "            ax.set_title(label, fontsize=9)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Not found\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for ax in axes[len(samples):]:\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show samples: fallbacks, smallest ROIs, by class\n",
    "crop_root = yolo_root  # Use the root determined in the audit cell\n",
    "\n",
    "# Fallback samples\n",
    "fallback_samples = df[df[\"fallback_to_full\"]]\n",
    "if len(fallback_samples) > 0:\n",
    "    show_sample_grid(fallback_samples, \"‚ö†Ô∏è Fallback to Full Frame Samples\", crop_root)\n",
    "\n",
    "# Smallest ROI samples (non-fallback)\n",
    "smallest = non_fallback.nsmallest(12, \"roi_area_frac\")\n",
    "if len(smallest) > 0:\n",
    "    show_sample_grid(smallest, \"üî¨ Smallest ROI Crops (non-fallback)\", crop_root)\n",
    "\n",
    "# Per-class samples (one random per class)\n",
    "print(\"\\nüè∑Ô∏è Sample crop per class:\")\n",
    "for class_id in sorted(df[\"class_id\"].unique()):\n",
    "    class_df = df[df[\"class_id\"] == class_id]\n",
    "    if len(class_df) > 0:\n",
    "        show_sample_grid(class_df, f\"Class {int(class_id)} Samples\", crop_root, n_samples=8, n_cols=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfab60d",
   "metadata": {},
   "source": [
    "## üîÄ Hybrid ROI Extraction (RetinaFace + MediaPipe Hands)\n",
    "\n",
    "**Best accuracy option!** Uses specialized models:\n",
    "- **RetinaFace**: State-of-the-art face detection (handles occlusion, angles)\n",
    "- **MediaPipe Hands**: Google's dedicated hand model (much better than Holistic)\n",
    "\n",
    "This typically gives the lowest fallback rate and best hand detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca39043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÄ Generate Hybrid ROI crops (InsightFace + MediaPipe Hands)\n",
    "# Best accuracy option - uses specialized models for face and hands.\n",
    "# InsightFace uses ONNX (no TensorFlow dependency!)\n",
    "!pip -q install insightface onnxruntime mediapipe\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "VARIANT = \"face_hands\"  # choose: face | hands | face_hands\n",
    "HYBRID_OUTPUT_ROOT = Path(OUT_ROOT) / \"hybrid\"\n",
    "manifest_csv = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "splits_root = Path(OUT_ROOT) / \"splits\"\n",
    "\n",
    "# ===== AUTO-DETECT LOCAL vs DRIVE IMAGES =====\n",
    "# If you ran the \"copy + compress dataset\" cells (46/47), images will be in /content/data\n",
    "# We auto-detect and use local images for faster I/O, falling back to Drive if not available\n",
    "LOCAL_DATASET_ROOT = Path(\"/content/data/auc.distracted.driver.dataset_v2\")\n",
    "DRIVE_DATASET_ROOT = Path(DATASET_ROOT)\n",
    "\n",
    "if LOCAL_DATASET_ROOT.exists() and any(LOCAL_DATASET_ROOT.iterdir()):\n",
    "    EFFECTIVE_DATASET_ROOT = LOCAL_DATASET_ROOT\n",
    "    print(f\"üöÄ FAST MODE: Using local images from {LOCAL_DATASET_ROOT}\")\n",
    "else:\n",
    "    EFFECTIVE_DATASET_ROOT = DRIVE_DATASET_ROOT\n",
    "    print(f\"üìÅ Using images from Drive: {DRIVE_DATASET_ROOT}\")\n",
    "    print(\"   üí° Tip: Run cells 46/47 first to copy images to /content for faster extraction!\")\n",
    "\n",
    "# ===== TEST MODE OPTIONS (toggle these!) =====\n",
    "# Option 1: Use train_small.csv for quick testing (~200 images)\n",
    "TEST_MODE = True  # Set False for full extraction\n",
    "SAMPLE_CSV = Path(OUT_ROOT) / \"splits\" / \"train_small.csv\"  # Small balanced subset\n",
    "\n",
    "# Option 2: Limit to first N images (even faster for debugging)\n",
    "LIMIT = None  # Set to e.g. 50 for super quick test, None for no limit\n",
    "\n",
    "# Build command\n",
    "sample_flag = f\"--sample-csv {SAMPLE_CSV}\" if TEST_MODE and SAMPLE_CSV.exists() else \"\"\n",
    "limit_flag = f\"--limit {LIMIT}\" if LIMIT else \"\"\n",
    "\n",
    "extract_cmd = f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.data.hybrid_extract \\\n",
    "  --manifest {manifest_csv} \\\n",
    "  --splits-root {splits_root} \\\n",
    "  --dataset-root {EFFECTIVE_DATASET_ROOT} \\\n",
    "  --output-root {HYBRID_OUTPUT_ROOT} \\\n",
    "  --variant {VARIANT} \\\n",
    "  --min-detection-area-frac 0.03 \\\n",
    "  --min-area-frac 0.08 \\\n",
    "  --min-aspect 0.20 \\\n",
    "  --pad-frac 0.20 \\\n",
    "  {sample_flag} \\\n",
    "  {limit_flag} \\\n",
    "  --overwrite\n",
    "\"\"\"\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\"‚ö° TEST MODE: Using small sample for quick testing\")\n",
    "    print(f\"   Sample CSV: {SAMPLE_CSV}\")\n",
    "    if LIMIT:\n",
    "        print(f\"   Limit: {LIMIT} images\")\n",
    "else:\n",
    "    print(\"ü™µ FULL MODE: Processing all images\")\n",
    "\n",
    "print(f\"\\nRunning Hybrid extraction (InsightFace + MediaPipe Hands) for variant: {VARIANT}\")\n",
    "print(extract_cmd)\n",
    "proc = subprocess.Popen(\n",
    "    extract_cmd,\n",
    "    shell=True,\n",
    "    text=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "if proc.stdout is None:\n",
    "    raise RuntimeError(\"Extraction process has no stdout pipe.\")\n",
    "for line in proc.stdout:\n",
    "    print(line, end=\"\")\n",
    "proc.wait()\n",
    "if proc.returncode != 0:\n",
    "    raise RuntimeError(\"Hybrid extraction failed. Check logs above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798703bc",
   "metadata": {},
   "source": [
    "## üöö Copy Hybrid crops to /content (optional, faster I/O)\n",
    "\n",
    "Use this if training/audit from Drive is slow. Copies crops to local SSD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fec68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöö Copy Hybrid crops to /content for faster I/O\n",
    "import os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your variant to the one you extracted already\n",
    "HYBRID_VARIANT = \"face_hands\"  # face | hands | face_hands\n",
    "SRC_ROOT = Path(OUT_ROOT) / \"hybrid\"\n",
    "SRC_VARIANT_DIR = SRC_ROOT / HYBRID_VARIANT\n",
    "DST_ROOT = Path(\"/content/data/hybrid\") / HYBRID_VARIANT\n",
    "\n",
    "if not SRC_VARIANT_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Source Hybrid folder not found: {SRC_VARIANT_DIR}\\nRun the Hybrid extraction cell first.\")\n",
    "\n",
    "print(f\"Copying Hybrid crops from {SRC_VARIANT_DIR} -> {DST_ROOT}\")\n",
    "DST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy images directory tree\n",
    "file_count = 0\n",
    "for src_dir, _, files in os.walk(SRC_VARIANT_DIR):\n",
    "    rel_dir = Path(src_dir).relative_to(SRC_VARIANT_DIR)\n",
    "    dst_dir = DST_ROOT / rel_dir\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for fname in files:\n",
    "        src_path = Path(src_dir) / fname\n",
    "        dst_path = dst_dir / fname\n",
    "        if not dst_path.exists():\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            file_count += 1\n",
    "print(f\"   Copied {file_count} image files\")\n",
    "\n",
    "# Copy manifest/split CSVs that live one level above the variant folder\n",
    "csv_names = [\n",
    "    f\"manifest_{HYBRID_VARIANT}.csv\",\n",
    "    f\"train_{HYBRID_VARIANT}.csv\",\n",
    "    f\"val_{HYBRID_VARIANT}.csv\",\n",
    "    f\"test_{HYBRID_VARIANT}.csv\",\n",
    "    f\"detection_metadata_{HYBRID_VARIANT}.csv\",  # for auditing\n",
    "]\n",
    "for fname in csv_names:\n",
    "    src_csv = SRC_ROOT / fname\n",
    "    if not src_csv.exists():\n",
    "        print(f\"   ‚ö†Ô∏è Skipping {fname} (not found)\")\n",
    "        continue\n",
    "    dst_csv = DST_ROOT.parent / fname\n",
    "    shutil.copy2(src_csv, dst_csv)\n",
    "    print(f\"   ‚úÖ Copied {fname}\")\n",
    "\n",
    "# Point env vars for this runtime to the local Hybrid copy\n",
    "os.environ[\"HYBRID_ROOT_LOCAL\"] = str(DST_ROOT.parent)\n",
    "print(f\"\\n‚úÖ Copy complete! Set USE_HYBRID=True in training cell.\")\n",
    "print(f\"   HYBRID_ROOT_LOCAL = {os.environ['HYBRID_ROOT_LOCAL']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f546d61d",
   "metadata": {},
   "source": [
    "## üîç Hybrid Crop Quality Audit\n",
    "\n",
    "Quick stats and visual inspection of Hybrid crops (InsightFace + MediaPipe Hands).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa2df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Hybrid Crop Quality Audit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "VARIANT = \"face_hands\"  # must match the variant you extracted\n",
    "\n",
    "# Auto-detect: prefer local copy if available\n",
    "hybrid_root_local = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", \"\"))\n",
    "if hybrid_root_local.exists():\n",
    "    hybrid_root = hybrid_root_local\n",
    "else:\n",
    "    hybrid_root = Path(OUT_ROOT) / \"hybrid\"\n",
    "\n",
    "metadata_csv = hybrid_root / f\"detection_metadata_{VARIANT}.csv\"\n",
    "if not metadata_csv.exists():\n",
    "    raise FileNotFoundError(f\"Detection metadata not found: {metadata_csv}\\nRun Hybrid extraction first.\")\n",
    "\n",
    "print(f\"üìÅ Loading metadata from: {metadata_csv}\")\n",
    "df = pd.read_csv(metadata_csv)\n",
    "\n",
    "# Summary stats\n",
    "n_total = len(df)\n",
    "n_fallback = df[\"fallback_to_full\"].sum()\n",
    "n_face = (df[\"face_count\"] > 0).sum()\n",
    "n_left_hand = df[\"left_hand_detected\"].sum()\n",
    "n_right_hand = df[\"right_hand_detected\"].sum()\n",
    "n_both_hands = ((df[\"left_hand_detected\"]) & (df[\"right_hand_detected\"])).sum()\n",
    "n_any_hands = ((df[\"left_hand_detected\"]) | (df[\"right_hand_detected\"])).sum()\n",
    "n_face_and_hands = ((df[\"face_count\"] > 0) & (n_any_hands > 0)).sum()\n",
    "avg_face_conf = df.loc[df[\"face_confidence\"] > 0, \"face_confidence\"].mean()\n",
    "avg_left_conf = df.loc[df[\"left_hand_confidence\"] > 0, \"left_hand_confidence\"].mean()\n",
    "avg_right_conf = df.loc[df[\"right_hand_confidence\"] > 0, \"right_hand_confidence\"].mean()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä HYBRID DETECTION SUMMARY (RetinaFace + MediaPipe Hands)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total images: {n_total}\")\n",
    "print(f\"\\nüéØ Detection rates:\")\n",
    "print(f\"   Face detected (RetinaFace): {n_face} ({100*n_face/n_total:.1f}%)\")\n",
    "print(f\"   Left hand (MediaPipe): {n_left_hand} ({100*n_left_hand/n_total:.1f}%)\")\n",
    "print(f\"   Right hand (MediaPipe): {n_right_hand} ({100*n_right_hand/n_total:.1f}%)\")\n",
    "print(f\"   Both hands: {n_both_hands} ({100*n_both_hands/n_total:.1f}%)\")\n",
    "print(f\"   Any hand: {n_any_hands} ({100*n_any_hands/n_total:.1f}%)\")\n",
    "print(f\"\\nüìà Confidence scores:\")\n",
    "print(f\"   Avg face confidence: {avg_face_conf:.3f}\" if not np.isnan(avg_face_conf) else \"   Avg face confidence: N/A\")\n",
    "print(f\"   Avg left hand confidence: {avg_left_conf:.3f}\" if not np.isnan(avg_left_conf) else \"   Avg left hand confidence: N/A\")\n",
    "print(f\"   Avg right hand confidence: {avg_right_conf:.3f}\" if not np.isnan(avg_right_conf) else \"   Avg right hand confidence: N/A\")\n",
    "print(f\"\\n‚ö†Ô∏è  Fallback to full frame: {n_fallback} ({100*n_fallback/n_total:.1f}%)\")\n",
    "\n",
    "# Fallback reason breakdown\n",
    "fallback_df = df[df[\"fallback_to_full\"]]\n",
    "if len(fallback_df) > 0:\n",
    "    print(f\"   Fallback reasons:\")\n",
    "    for reason, count in fallback_df[\"fallback_reason\"].value_counts().items():\n",
    "        print(f\"      - {reason}: {count} ({100*count/n_total:.1f}%)\")\n",
    "\n",
    "# ROI stats (for non-fallback images)\n",
    "non_fallback = df[~df[\"fallback_to_full\"]]\n",
    "if len(non_fallback) > 0:\n",
    "    print(f\"\\nüìê ROI statistics (non-fallback only, n={len(non_fallback)}):\")\n",
    "    print(f\"   Raw detection area: mean={non_fallback['raw_detection_area_frac'].mean():.3f}, \"\n",
    "          f\"std={non_fallback['raw_detection_area_frac'].std():.3f}\")\n",
    "    print(f\"   Final ROI area: mean={non_fallback['roi_area_frac'].mean():.3f}, \"\n",
    "          f\"std={non_fallback['roi_area_frac'].std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Hybrid Breakdown by Camera and Class\n",
    "print(\"\\nüìã BREAKDOWN BY CAMERA\")\n",
    "print(\"-\" * 80)\n",
    "camera_stats = df.groupby(\"camera\").agg({\n",
    "    \"fallback_to_full\": [\"sum\", \"mean\"],\n",
    "    \"roi_area_frac\": \"mean\",\n",
    "    \"face_count\": lambda x: (x > 0).mean(),\n",
    "    \"left_hand_detected\": \"mean\",\n",
    "    \"right_hand_detected\": \"mean\",\n",
    "}).round(3)\n",
    "camera_stats.columns = [\"fallback_count\", \"fallback_pct\", \"mean_roi_area\", \"face_rate\", \"left_hand_rate\", \"right_hand_rate\"]\n",
    "camera_stats[\"fallback_pct\"] = (camera_stats[\"fallback_pct\"] * 100).round(1)\n",
    "camera_stats[\"face_rate\"] = (camera_stats[\"face_rate\"] * 100).round(1)\n",
    "camera_stats[\"left_hand_rate\"] = (camera_stats[\"left_hand_rate\"] * 100).round(1)\n",
    "camera_stats[\"right_hand_rate\"] = (camera_stats[\"right_hand_rate\"] * 100).round(1)\n",
    "print(camera_stats.to_string())\n",
    "\n",
    "print(\"\\nüìã BREAKDOWN BY CLASS\")\n",
    "print(\"-\" * 80)\n",
    "class_stats = df.groupby(\"class_id\").agg({\n",
    "    \"fallback_to_full\": [\"sum\", \"mean\"],\n",
    "    \"roi_area_frac\": \"mean\",\n",
    "    \"face_count\": lambda x: (x > 0).mean(),\n",
    "    \"left_hand_detected\": \"mean\",\n",
    "    \"right_hand_detected\": \"mean\",\n",
    "}).round(3)\n",
    "class_stats.columns = [\"fallback_count\", \"fallback_pct\", \"mean_roi_area\", \"face_rate\", \"left_hand_rate\", \"right_hand_rate\"]\n",
    "class_stats[\"fallback_pct\"] = (class_stats[\"fallback_pct\"] * 100).round(1)\n",
    "class_stats[\"face_rate\"] = (class_stats[\"face_rate\"] * 100).round(1)\n",
    "class_stats[\"left_hand_rate\"] = (class_stats[\"left_hand_rate\"] * 100).round(1)\n",
    "class_stats[\"right_hand_rate\"] = (class_stats[\"right_hand_rate\"] * 100).round(1)\n",
    "print(class_stats.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e011f7",
   "metadata": {},
   "source": [
    "## üöÇ 11.1 Train a model (adjust these knobs)\n",
    "\n",
    "- Choose a `RUN_TAG` so logs/checkpoints go into `TFM/checkpoints/runs/<tag>/...`\n",
    "- Set epochs/batch size to something small for a dry run (1 epoch, 16 batch)\n",
    "- This command uses the CLI helper (`python -m src.ddriver.cli.train ...`)\n",
    "- Logs + checkpoints are saved automatically to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess, textwrap, json, time, threading\n",
    "from pathlib import Path\n",
    "\n",
    "# ConvNeXt-Tiny baseline run (change RUN_TAG for each experiment)\n",
    "RUN_TAG = \"effb0_noLabelSmoothingCORRECTED\"   # change me for each experiment\n",
    "MODEL_NAME = \"efficientnet_b0\"                  # must be registered above (timm)\n",
    "\n",
    "# Training hyperparameters (EfficientNet-B0)\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_SIZE = 224\n",
    "LR = 3e-4                        # per provided hyperparams\n",
    "LR_DROP_EPOCH = None             # no LR drop\n",
    "LR_DROP_FACTOR = 0.1\n",
    "LABEL_SMOOTHING = 0.0\n",
    "USE_TINY_SPLIT = False\n",
    "\n",
    "# ROI crop pipeline selection (pick ONE, set others to False)\n",
    "USE_MEDIAPIPE = False             # set True to use MediaPipe ROI crops\n",
    "USE_YOLO = False                  # set True to use YOLO-World ROI crops\n",
    "USE_HYBRID = True                 # set True to use Hybrid (RetinaFace + MediaPipe Hands) crops\n",
    "ROI_VARIANT = \"face_hands\"        # face | hands | face_hands\n",
    "\n",
    "# Validate only one pipeline is selected\n",
    "active_pipelines = sum([USE_MEDIAPIPE, USE_YOLO, USE_HYBRID])\n",
    "if active_pipelines > 1:\n",
    "    raise ValueError(\"Pick ONE pipeline: set only one of USE_MEDIAPIPE, USE_YOLO, USE_HYBRID to True.\")\n",
    "\n",
    "if USE_HYBRID:\n",
    "    hybrid_root = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", Path(OUT_ROOT) / \"hybrid\"))\n",
    "    manifest_csv = hybrid_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    train_split = f\"train_{ROI_VARIANT}.csv\" if not USE_TINY_SPLIT else f\"train_small_{ROI_VARIANT}.csv\"\n",
    "    val_csv = hybrid_root / f\"val_{ROI_VARIANT}.csv\"\n",
    "    test_csv = hybrid_root / f\"test_{ROI_VARIANT}.csv\"\n",
    "    train_csv = hybrid_root / train_split\n",
    "    print(f\"üîÄ Using Hybrid (RetinaFace + MediaPipe Hands) ROI variant: {ROI_VARIANT}\")\n",
    "    print(f\"   hybrid_root = {hybrid_root}\")\n",
    "elif USE_YOLO:\n",
    "    yolo_root = Path(os.environ.get(\"YOLO_ROOT_LOCAL\", Path(OUT_ROOT) / \"yolo\"))\n",
    "    manifest_csv = yolo_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    train_split = f\"train_{ROI_VARIANT}.csv\" if not USE_TINY_SPLIT else f\"train_small_{ROI_VARIANT}.csv\"\n",
    "    val_csv = yolo_root / f\"val_{ROI_VARIANT}.csv\"\n",
    "    test_csv = yolo_root / f\"test_{ROI_VARIANT}.csv\"\n",
    "    train_csv = yolo_root / train_split\n",
    "    print(f\"üéØ Using YOLO-World ROI variant: {ROI_VARIANT}\")\n",
    "    print(f\"   yolo_root = {yolo_root}\")\n",
    "elif USE_MEDIAPIPE:\n",
    "    mp_root = Path(os.environ.get(\"MEDIAPIPE_ROOT_LOCAL\", Path(OUT_ROOT) / \"mediapipe\"))\n",
    "    manifest_csv = mp_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    train_split = f\"train_{ROI_VARIANT}.csv\" if not USE_TINY_SPLIT else f\"train_small_{ROI_VARIANT}.csv\"\n",
    "    val_csv = mp_root / f\"val_{ROI_VARIANT}.csv\"\n",
    "    test_csv = mp_root / f\"test_{ROI_VARIANT}.csv\"\n",
    "    train_csv = mp_root / train_split\n",
    "    print(f\"üß≠ Using MediaPipe ROI variant: {ROI_VARIANT}\")\n",
    "    print(f\"   mp_root = {mp_root}\")\n",
    "else:\n",
    "    manifest_csv = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "    train_split = \"train_small.csv\" if USE_TINY_SPLIT else \"train.csv\"\n",
    "    train_csv = Path(OUT_ROOT) / \"splits\" / train_split\n",
    "    val_csv = Path(OUT_ROOT) / \"splits\" / \"val.csv\"\n",
    "    test_csv = Path(OUT_ROOT) / \"splits\" / \"test.csv\"\n",
    "    print(\"üì∑ Using full-frame images (no ROI cropping)\")\n",
    "\n",
    "if USE_TINY_SPLIT:\n",
    "    print(\"‚ö° Using train_small.csv (20 imgs/class) for a quick smoke test.\")\n",
    "else:\n",
    "    print(\"ü™µ Using full train.csv for a proper run.\")\n",
    "\n",
    "train_cmd = textwrap.dedent(f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.cli.train \\\n",
    "    --model-name {MODEL_NAME} \\\n",
    "    --epochs {EPOCHS} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --num-workers {NUM_WORKERS} \\\n",
    "    --image-size {IMAGE_SIZE} \\\n",
    "    --lr {LR} \\\n",
    "    --weight-decay 0.0 \\\n",
    "    --optimizer adam \\\n",
    "    --label-smoothing {LABEL_SMOOTHING} \\\n",
    "    --out-tag {RUN_TAG} \\\n",
    "    --manifest-csv {manifest_csv} \\\n",
    "    --train-csv {train_csv} \\\n",
    "    --val-csv {val_csv} \\\n",
    "    --test-csv {test_csv}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Running training command and streaming logs:\\n\", train_cmd)\n",
    "\n",
    "proc = subprocess.Popen(\n",
    "    train_cmd,\n",
    "    shell=True,\n",
    "    text=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "\n",
    "# Background GPU monitor (prints every 5 seconds)\n",
    "def _gpu_monitor():\n",
    "    while proc.poll() is None:\n",
    "        try:\n",
    "            stats = subprocess.check_output(\n",
    "                \"nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total \"\n",
    "                \"--format=csv,nounits,noheader\",\n",
    "                shell=True,\n",
    "            ).decode(\"utf-8\").strip()\n",
    "            print(f\"[GPU] util%, mem_used, mem_total :: {stats}\")\n",
    "        except Exception as exc:\n",
    "            print(\"[GPU] Could not query nvidia-smi:\", exc)\n",
    "        time.sleep(5)\n",
    "\n",
    "monitor_thread = threading.Thread(target=_gpu_monitor, daemon=True)\n",
    "monitor_thread.start()\n",
    "\n",
    "# Stream CLI stdout live\n",
    "if proc.stdout is None:\n",
    "    raise RuntimeError(\"Training process has no stdout pipe.\")\n",
    "\n",
    "for line in proc.stdout:\n",
    "    print(line, end=\"\")\n",
    "\n",
    "proc.wait()\n",
    "monitor_thread.join(timeout=0)\n",
    "\n",
    "print(\"\\n‚úÖ Training run complete!\\n\")\n",
    "\n",
    "# --- Display every epoch's metrics so the notebook shows the learning curve ---\n",
    "run_base = Path(CKPT_ROOT) / \"runs\" / RUN_TAG\n",
    "all_runs = sorted(run_base.glob(\"*/\"))\n",
    "if not all_runs:\n",
    "    raise FileNotFoundError(f\"No run folders found under {run_base}\")\n",
    "latest_run = all_runs[-1]\n",
    "\n",
    "history_path = latest_run / \"history.json\"\n",
    "if not history_path.exists():\n",
    "    raise FileNotFoundError(f\"history.json not found in {latest_run}\")\n",
    "\n",
    "history = json.loads(history_path.read_text()).get(\"history\", [])\n",
    "print(f\"üìä Epoch metrics for run: {latest_run.name}\")\n",
    "for record in history:\n",
    "    train_metrics = record.get(\"train\", {})\n",
    "    val_metrics = record.get(\"val\", {}) or {}\n",
    "    train_loss = train_metrics.get(\"loss\")\n",
    "    train_acc = train_metrics.get(\"accuracy\")\n",
    "    val_loss = val_metrics.get(\"loss\")\n",
    "    val_acc = val_metrics.get(\"accuracy\")\n",
    "    val_str = (\n",
    "        f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\"\n",
    "        if val_loss is not None and val_acc is not None\n",
    "        else \"val_loss=‚Äî val_acc=‚Äî\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Epoch {record['epoch']:>2}: \"\n",
    "        f\"train_loss={train_loss:.4f} acc={train_acc:.4f}  \"\n",
    "        f\"{val_str}\"\n",
    "    )\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b93ef9",
   "metadata": {},
   "source": [
    "## üìù 11.1a Log training summary to Google Sheet\n",
    "Run this right after the training cell finishes. It looks up the newest run under `CKPT_ROOT/runs/<RUN_TAG>`, grabs the best/final train + val accuracies, and logs the model/hyperparams so you can compare experiments before doing predictions or metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75995de",
   "metadata": {},
   "source": [
    "## üöö Copy MediaPipe crops to /content (optional, faster I/O)\n",
    "Use this if training from Drive is slow. It copies the generated MediaPipe crops/CSVs into `/content/data/mediapipe/<variant>` and updates paths for the current runtime. Does not affect the original full-image copy cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your variant to the one you extracted already\n",
    "MEDIAPIPE_VARIANT = \"face_hands\"  # face | hands | face_hands\n",
    "SRC_ROOT = Path(OUT_ROOT) / \"mediapipe\"\n",
    "SRC_VARIANT_DIR = SRC_ROOT / MEDIAPIPE_VARIANT\n",
    "DST_ROOT = Path(\"/content/data/mediapipe\") / MEDIAPIPE_VARIANT\n",
    "\n",
    "if not SRC_VARIANT_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Source mediapipe folder not found: {SRC_VARIANT_DIR}\\nRun the extraction cell first.\")\n",
    "\n",
    "print(f\"Copying MediaPipe crops from {SRC_VARIANT_DIR} -> {DST_ROOT}\")\n",
    "DST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy images directory tree\n",
    "for src_dir, _, files in os.walk(SRC_VARIANT_DIR):\n",
    "    rel_dir = Path(src_dir).relative_to(SRC_VARIANT_DIR)\n",
    "    dst_dir = DST_ROOT / rel_dir\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for fname in files:\n",
    "        src_path = Path(src_dir) / fname\n",
    "        dst_path = dst_dir / fname\n",
    "        if not dst_path.exists():\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "# Copy manifest/split CSVs that live one level above the variant folder\n",
    "csv_names = [\n",
    "    f\"manifest_{MEDIAPIPE_VARIANT}.csv\",\n",
    "    f\"train_{MEDIAPIPE_VARIANT}.csv\",\n",
    "    f\"val_{MEDIAPIPE_VARIANT}.csv\",\n",
    "    f\"test_{MEDIAPIPE_VARIANT}.csv\",\n",
    "]\n",
    "for fname in csv_names:\n",
    "    src_csv = SRC_ROOT / fname\n",
    "    if not src_csv.exists():\n",
    "        raise FileNotFoundError(src_csv)\n",
    "    dst_csv = DST_ROOT.parent / fname\n",
    "    shutil.copy2(src_csv, dst_csv)\n",
    "    print(f\"Copied {src_csv} -> {dst_csv}\")\n",
    "\n",
    "# Point env vars for this runtime to the local mediapipe copy\n",
    "os.environ[\"MEDIAPIPE_ROOT_LOCAL\"] = str(DST_ROOT.parent)\n",
    "os.environ[\"MEDIAPIPE_VARIANT\"] = MEDIAPIPE_VARIANT\n",
    "print(\"\\n‚úÖ Copy complete. Set USE_MEDIAPIPE=True and point mp_root to MEDIAPIPE_ROOT_LOCAL in the training cell:\")\n",
    "print(\"  mp_root = Path(os.environ['MEDIAPIPE_ROOT_LOCAL'])\")\n",
    "print(\"  manifest_csv = mp_root / f'manifest_{MEDIAPIPE_VARIANT}.csv'\")\n",
    "print(\"  train_csv    = mp_root / f'train_{MEDIAPIPE_VARIANT}.csv'\")\n",
    "print(\"  val_csv      = mp_root / f'val_{MEDIAPIPE_VARIANT}.csv'\")\n",
    "print(\"  test_csv     = mp_root / f'test_{MEDIAPIPE_VARIANT}.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Training summary ‚Üí Google Sheet\n",
    "!pip -q install gspread\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import gspread\n",
    "from google.colab import auth\n",
    "import google.auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "creds, _ = google.auth.default()\n",
    "gc = gspread.authorize(creds)\n",
    "\n",
    "TRAIN_SHEET_NAME = \"TFM Train Logs\"   # create this sheet/tab ahead of time\n",
    "TRAIN_WORKSHEET = \"Sheet1\"\n",
    "\n",
    "run_base = Path(CKPT_ROOT) / \"runs\" / RUN_TAG\n",
    "all_runs = sorted(run_base.glob(\"*/\"))\n",
    "if not all_runs:\n",
    "    raise FileNotFoundError(f\"No run folders found under {run_base}\")\n",
    "latest_run = all_runs[-1]\n",
    "print(f\"Logging training summary for run folder: {latest_run}\")\n",
    "\n",
    "history_path = latest_run / \"history.json\"\n",
    "if not history_path.exists():\n",
    "    raise FileNotFoundError(f\"history.json not found under {latest_run}\")\n",
    "\n",
    "history_records = json.loads(history_path.read_text()).get(\"history\", [])\n",
    "if not history_records:\n",
    "    raise ValueError(f\"history.json under {latest_run} has no records.\")\n",
    "\n",
    "params_path = latest_run / \"params.json\"\n",
    "params = json.loads(params_path.read_text()) if params_path.exists() else {}\n",
    "\n",
    "model_name = params.get(\"model_name\", MODEL_NAME)\n",
    "epochs_cfg = params.get(\"epochs\", EPOCHS)\n",
    "batch_cfg = params.get(\"batch_size\", BATCH_SIZE)\n",
    "lr_cfg = params.get(\"lr\", LR)\n",
    "lr_drop_epoch_cfg = params.get(\"lr_drop_epoch\", LR_DROP_EPOCH)\n",
    "lr_drop_factor_cfg = params.get(\"lr_drop_factor\", LR_DROP_FACTOR)\n",
    "image_size_cfg = params.get(\"image_size\", IMAGE_SIZE)\n",
    "num_workers_cfg = params.get(\"num_workers\", NUM_WORKERS)\n",
    "use_tiny_cfg = params.get(\"use_tiny_split\", USE_TINY_SPLIT)\n",
    "\n",
    "\n",
    "def _best_metric(records, split: str) -> tuple[dict, float | None]:\n",
    "    best_epoch = None\n",
    "    best_acc = None\n",
    "    for rec in records:\n",
    "        split_metrics = rec.get(split) or {}\n",
    "        acc = split_metrics.get(\"accuracy\")\n",
    "        if acc is None:\n",
    "            continue\n",
    "        if best_acc is None or acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_epoch = rec.get(\"epoch\")\n",
    "    final_metrics = records[-1].get(split) or {}\n",
    "    final_acc = final_metrics.get(\"accuracy\")\n",
    "    return {\"epoch\": best_epoch, \"accuracy\": best_acc}, final_acc\n",
    "\n",
    "\n",
    "best_train, final_train = _best_metric(history_records, \"train\")\n",
    "best_val, final_val = _best_metric(history_records, \"val\")\n",
    "\n",
    "row = [\n",
    "    RUN_TAG,\n",
    "    latest_run.name,\n",
    "    model_name,\n",
    "    epochs_cfg,\n",
    "    batch_cfg,\n",
    "    lr_cfg,\n",
    "    lr_drop_epoch_cfg,\n",
    "    lr_drop_factor_cfg,\n",
    "    image_size_cfg,\n",
    "    num_workers_cfg,\n",
    "    use_tiny_cfg,\n",
    "    best_train[\"epoch\"] if best_train[\"epoch\"] is not None else \"\",\n",
    "    round(best_train[\"accuracy\"], 4) if best_train[\"accuracy\"] is not None else \"\",\n",
    "    best_val[\"epoch\"] if best_val[\"epoch\"] is not None else \"\",\n",
    "    round(best_val[\"accuracy\"], 4) if best_val[\"accuracy\"] is not None else \"\",\n",
    "    round(final_train, 4) if final_train is not None else \"\",\n",
    "    round(final_val, 4) if final_val is not None else \"\",\n",
    "]\n",
    "\n",
    "ws = gc.open(TRAIN_SHEET_NAME).worksheet(TRAIN_WORKSHEET)\n",
    "ws.append_row(row, value_input_option=\"USER_ENTERED\")\n",
    "print(f\"Appended training summary for {latest_run.name} ‚úÖ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c73e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Optional: copy + compress dataset subset ‚Üí fast local SSD (/content/data)\n",
    "# Re-encodes JPEGs once (quality 80, short side 320px) before landing in /content/data.\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from ddriver.data.fastcopy import CompressionSpec, copy_splits_with_compression\n",
    "\n",
    "SRC_ROOT = Path(DRIVE_DATA_ROOT) / \"auc.distracted.driver.dataset_v2\"\n",
    "DST_ROOT = Path(FAST_DATA) / \"auc.distracted.driver.dataset_v2\"\n",
    "\n",
    "split_csvs = {\n",
    "    \"train\": Path(OUT_ROOT) / \"splits\" / \"train.csv\",\n",
    "    \"val\": Path(OUT_ROOT) / \"splits\" / \"val.csv\",\n",
    "    \"train_small\": Path(OUT_ROOT) / \"splits\" / \"train_small.csv\",\n",
    "}\n",
    "\n",
    "compression_spec = CompressionSpec(\n",
    "    target_short_side=320,  # still >= image_size + resize margin for training\n",
    "    jpeg_quality=80,        # ImageNet-level compression, visually lossless\n",
    ")\n",
    "\n",
    "summary = copy_splits_with_compression(\n",
    "    split_csvs=split_csvs,\n",
    "    src_root=SRC_ROOT,\n",
    "    dst_root=DST_ROOT,\n",
    "    compression=compression_spec,\n",
    "    skip_existing=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nüìâ FAST_DATA copy stats: processed {summary['processed']} of {summary['total']} files \"\n",
    "    f\"(skipped {summary['skipped']} already present).\"\n",
    ")\n",
    "print(f\"Compressed dataset root: {summary['dst_root']}\")\n",
    "\n",
    "DATASET_ROOT = FAST_DATA\n",
    "os.environ[\"DATASET_ROOT\"] = str(DATASET_ROOT)\n",
    "try:\n",
    "    from ddriver import config as _ddriver_config\n",
    "    importlib.reload(_ddriver_config)\n",
    "    print(\"\\n‚ö° Copy complete. DATASET_ROOT now points to the local FAST_DATA copy for this runtime:\")\n",
    "    print(\"   ddriver.config.DATASET_ROOT =\", _ddriver_config.DATASET_ROOT)\n",
    "except Exception as exc:\n",
    "    print(\"\\n‚ö° Copy complete and DATASET_ROOT env updated, but could not reload ddriver.config:\", exc)\n",
    "print(\"   (Re-run env summary if you want to rewrite .env, but training now uses /content/data.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb9779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Optional: copy + compress TEST split ‚Üí /content/data (same settings as train/val)\n",
    "\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "from ddriver.data.fastcopy import CompressionSpec, copy_splits_with_compression\n",
    "\n",
    "SRC_ROOT = Path(DRIVE_DATA_ROOT) / \"auc.distracted.driver.dataset_v2\"\n",
    "DST_ROOT = Path(FAST_DATA) / \"auc.distracted.driver.dataset_v2\"\n",
    "\n",
    "split_csvs = {\n",
    "    \"test\": Path(OUT_ROOT) / \"splits\" / \"test.csv\",\n",
    "}\n",
    "\n",
    "compression_spec = CompressionSpec(\n",
    "    target_short_side=320,\n",
    "    jpeg_quality=80,\n",
    ")\n",
    "\n",
    "summary = copy_splits_with_compression(\n",
    "    split_csvs=split_csvs,\n",
    "    src_root=SRC_ROOT,\n",
    "    dst_root=DST_ROOT,\n",
    "    compression=compression_spec,\n",
    "    skip_existing=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nüìâ FAST_DATA test copy stats: processed {summary['processed']} of {summary['total']} \"\n",
    "    f\"(skipped {summary['skipped']} already present).\"\n",
    ")\n",
    "print(f\"Compressed dataset root: {summary['dst_root']}\")\n",
    "\n",
    "# DATASET_ROOT is already pointing at FAST_DATA from the earlier cell, but reload config just in case\n",
    "try:\n",
    "    from ddriver import config as _ddriver_config\n",
    "    importlib.reload(_ddriver_config)\n",
    "    print(\"\\n‚ö° Test copy complete. ddriver.config now sees:\")\n",
    "    print(\"   ddriver.config.DATASET_ROOT =\", _ddriver_config.DATASET_ROOT)\n",
    "except Exception as exc:\n",
    "    print(\"\\n‚ö° Test copy complete; config reload optional:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c429a0e",
   "metadata": {},
   "source": [
    "## üì¶ 11.2 Pick the latest checkpoint file\n",
    "\n",
    "This cell looks inside `CKPT_ROOT/runs/<RUN_TAG>/` and grabs the newest `epoch_*.pt`. Use this path in the prediction step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RUN_TAG = \"convnext_tiny_full_v1\"  # pick the tag you want to inspect\n",
    "#RUN_TAG = globals().get(\"RUN_TAG\", \"convnext_tiny_full_v1\")  # reuse your latest training tag by default\n",
    "\n",
    "run_base = Path(CKPT_ROOT) / \"runs\" / RUN_TAG\n",
    "runs = sorted(run_base.glob(\"*/\"))\n",
    "if not runs:\n",
    "    raise FileNotFoundError(f\"No run folders found under {run_base}\")\n",
    "\n",
    "# ---- choose which run folder to use ----\n",
    "RUN_IDX = -1          # -1 = newest, 0 = oldest, or any index from the printout below\n",
    "print(\"Available runs:\")\n",
    "for idx, run_dir in enumerate(runs):\n",
    "    print(f\"  [{idx}] {run_dir.name}\")\n",
    "target_run = runs[RUN_IDX]\n",
    "print(f\"\\nSelected run: {target_run}\\n\")\n",
    "\n",
    "# ---- choose which checkpoint (epoch) inside that run ----\n",
    "checkpoint_patterns = [\"epoch_*.pt\", \"best.pt\", \"last.pt\"]\n",
    "checkpoints = []\n",
    "for pattern in checkpoint_patterns:\n",
    "       matches = sorted(target_run.glob(pattern))\n",
    "       if matches:\n",
    "           checkpoints.extend(matches)\n",
    "\n",
    "if not checkpoints:\n",
    "       raise FileNotFoundError(f\"No checkpoints found under {target_run}\")\n",
    "\n",
    "CHECKPOINT_NAME = \"best.pt\"  # or \"last.pt\", or None to take the last match\n",
    "if CHECKPOINT_NAME:\n",
    "       chosen_ckpt = target_run / CHECKPOINT_NAME\n",
    "       if not chosen_ckpt.exists():\n",
    "           raise FileNotFoundError(chosen_ckpt)\n",
    "else:\n",
    "       chosen_ckpt = checkpoints[-1]\n",
    "\n",
    "LATEST_CKPT = chosen_ckpt\n",
    "print(\"Using checkpoint:\", LATEST_CKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d7e3f8",
   "metadata": {},
   "source": [
    "## üîÆ 11.3 Generate predictions CSV\n",
    "\n",
    "- Uses the checkpoint above\n",
    "- Choose which split to predict on (`val` or `test`)\n",
    "- Saves CSV under `OUT_ROOT/preds/<split>/<out_tag>.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PRED_SPLIT = \"test\"           # or \"val\"\n",
    "PRED_TAG = f\"{RUN_TAG}_{PRED_SPLIT}\"\n",
    "\n",
    "if \"USE_HYBRID\" in globals() and USE_HYBRID:\n",
    "    hybrid_root = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", Path(OUT_ROOT) / \"hybrid\"))\n",
    "    manifest_pred = hybrid_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    split_pred = hybrid_root / f\"{PRED_SPLIT}_{ROI_VARIANT}.csv\"\n",
    "    split_flag = \"--val-csv\" if PRED_SPLIT == \"val\" else \"--test-csv\"\n",
    "    manifest_flag = f\"--manifest-csv {manifest_pred}\"\n",
    "    split_flag_str = f\"{split_flag} {split_pred}\"\n",
    "elif \"USE_YOLO\" in globals() and USE_YOLO:\n",
    "    yolo_root = Path(os.environ.get(\"YOLO_ROOT_LOCAL\", Path(OUT_ROOT) / \"yolo\"))\n",
    "    manifest_pred = yolo_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    split_pred = yolo_root / f\"{PRED_SPLIT}_{ROI_VARIANT}.csv\"\n",
    "    split_flag = \"--val-csv\" if PRED_SPLIT == \"val\" else \"--test-csv\"\n",
    "    manifest_flag = f\"--manifest-csv {manifest_pred}\"\n",
    "    split_flag_str = f\"{split_flag} {split_pred}\"\n",
    "elif \"USE_MEDIAPIPE\" in globals() and USE_MEDIAPIPE:\n",
    "    mp_root = Path(os.environ.get(\"MEDIAPIPE_ROOT_LOCAL\", Path(OUT_ROOT) / \"mediapipe\"))\n",
    "    manifest_pred = mp_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    split_pred = mp_root / f\"{PRED_SPLIT}_{ROI_VARIANT}.csv\"\n",
    "    split_flag = \"--val-csv\" if PRED_SPLIT == \"val\" else \"--test-csv\"\n",
    "    manifest_flag = f\"--manifest-csv {manifest_pred}\"\n",
    "    split_flag_str = f\"{split_flag} {split_pred}\"\n",
    "else:\n",
    "    manifest_pred = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "    split_pred = Path(OUT_ROOT) / \"splits\" / f\"{PRED_SPLIT}.csv\"\n",
    "    manifest_flag = f\"--manifest-csv {manifest_pred}\"\n",
    "    split_flag_str = f\"--{PRED_SPLIT}-csv {split_pred}\"\n",
    "\n",
    "predict_cmd = textwrap.dedent(f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.cli.predict \\\n",
    "    --model-name {MODEL_NAME} \\\n",
    "    --checkpoint {LATEST_CKPT} \\\n",
    "    --split {PRED_SPLIT} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --num-workers {NUM_WORKERS} \\\n",
    "    --image-size {IMAGE_SIZE} \\\n",
    "    --out-tag {PRED_TAG} \\\n",
    "    {manifest_flag} \\\n",
    "    {split_flag_str}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Running prediction command:\\n\", predict_cmd)\n",
    "result = subprocess.run(predict_cmd, shell=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"Prediction command failed. See logs above.\")\n",
    "print(\"\\n‚úÖ Predictions completed! Check OUT_ROOT/preds/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ca361",
   "metadata": {},
   "source": [
    "## üìä 11.4 Evaluate metrics\n",
    "\n",
    "- Uses `src/ddriver/metrics.py`\n",
    "- Reads the manifest + split CSV + predictions CSV\n",
    "- Saves results under `OUT_ROOT/metrics/<tag>/<timestamp>/`\n",
    "- Shows accuracy + macro F1 + per-driver/camera (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if \"USE_HYBRID\" in globals() and USE_HYBRID:\n",
    "    hybrid_root = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", Path(OUT_ROOT) / \"hybrid\"))\n",
    "    manifest_path = hybrid_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    split_csv_path = hybrid_root / f\"{PRED_SPLIT}_{ROI_VARIANT}.csv\"\n",
    "    preds_csv_path = Path(OUT_ROOT) / \"preds\" / PRED_SPLIT / f\"{PRED_TAG}.csv\"\n",
    "elif \"USE_YOLO\" in globals() and USE_YOLO:\n",
    "    yolo_root = Path(os.environ.get(\"YOLO_ROOT_LOCAL\", Path(OUT_ROOT) / \"yolo\"))\n",
    "    manifest_path = yolo_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    split_csv_path = yolo_root / f\"{PRED_SPLIT}_{ROI_VARIANT}.csv\"\n",
    "    preds_csv_path = Path(OUT_ROOT) / \"preds\" / PRED_SPLIT / f\"{PRED_TAG}.csv\"\n",
    "elif \"USE_MEDIAPIPE\" in globals() and USE_MEDIAPIPE:\n",
    "    mp_root = Path(os.environ.get(\"MEDIAPIPE_ROOT_LOCAL\", Path(OUT_ROOT) / \"mediapipe\"))\n",
    "    manifest_path = mp_root / f\"manifest_{ROI_VARIANT}.csv\"\n",
    "    split_csv_path = mp_root / f\"{PRED_SPLIT}_{ROI_VARIANT}.csv\"\n",
    "    preds_csv_path = Path(OUT_ROOT) / \"preds\" / PRED_SPLIT / f\"{PRED_TAG}.csv\"\n",
    "else:\n",
    "    manifest_path = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "    split_csv_path = Path(OUT_ROOT) / \"splits\" / f\"{PRED_SPLIT}.csv\"\n",
    "    preds_csv_path = Path(OUT_ROOT) / \"preds\" / PRED_SPLIT / f\"{PRED_TAG}.csv\"\n",
    "METRICS_TAG = PRED_TAG\n",
    "\n",
    "metrics_cmd = textwrap.dedent(f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.eval.metrics \\\n",
    "    --manifest {manifest_path} \\\n",
    "    --split-csv {split_csv_path} \\\n",
    "    --predictions {preds_csv_path} \\\n",
    "    --out-tag {METRICS_TAG} \\\n",
    "    --per-driver \\\n",
    "    --per-camera\n",
    "\"\"\")\n",
    "\n",
    "print(\"Running metrics command:\\n\", metrics_cmd)\n",
    "result = subprocess.run(metrics_cmd, shell=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"Metrics command failed. See logs above.\")\n",
    "print(\"\\n‚úÖ Metrics saved under OUT_ROOT/metrics/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1047bf",
   "metadata": {},
   "source": [
    "### ‚úÖ You're all set!\n",
    "\n",
    "**What just happened:**\n",
    "1. ‚úÖ Mounted Google Drive\n",
    "2. ‚úÖ Cloned/updated your repo\n",
    "3. ‚úÖ Installed the package\n",
    "4. ‚úÖ Set up paths (works on Colab and Mac!)\n",
    "5. ‚úÖ Generated manifest.csv and train/val/test split CSVs\n",
    "6. ‚úÖ Tested that dataset.py can load images\n",
    "7. ‚úÖ Tested that datamod.py can create data loaders\n",
    "8. ‚úÖ (Optional) Registered a model + ran training ‚Üí prediction ‚Üí metrics pipeline\n",
    "\n",
    "**Your CSVs are saved in Google Drive:**\n",
    "- `OUT_ROOT/manifests/manifest.csv` - Big list of all images\n",
    "- `OUT_ROOT/splits/train.csv` - Training images\n",
    "- `OUT_ROOT/splits/val.csv` - Validation images (with driver IDs!)\n",
    "- `OUT_ROOT/splits/test.csv` - Test images\n",
    "\n",
    "**Next steps:**\n",
    "- Adjust the training/prediction cells (epochs, batch size, tags) to run bigger experiments\n",
    "- All paths use `ddriver.config` so it works on Colab and Mac\n",
    "- Re-run **Clone/Update** cell after pushing new commits\n",
    "- Optional: copy some data into `/content/data` to use `FAST_DATA` for speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Colab cell: append metrics + params to Google Sheet ----\n",
    "!pip -q install gspread\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import gspread\n",
    "from google.colab import auth\n",
    "import google.auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "creds, _ = google.auth.default()\n",
    "gc = gspread.authorize(creds)\n",
    "\n",
    "EVAL_SHEET_NAME = \"TFM Eval Logs\"   # create this sheet/tab ahead of time\n",
    "EVAL_WORKSHEET = \"Sheet1\"\n",
    "\n",
    "METRICS_TAG = (\n",
    "    globals().get(\"METRICS_TAG\")\n",
    "    or globals().get(\"PRED_TAG\")\n",
    "    or \"convnext_tiny_full_v1_val\"\n",
    ")  # match the --out-tag you used\n",
    "metrics_root = Path(OUT_ROOT) / \"metrics\" / METRICS_TAG\n",
    "runs = sorted(metrics_root.glob(\"*/\"))\n",
    "if not runs:\n",
    "    raise FileNotFoundError(f\"No metrics runs found under {metrics_root}\")\n",
    "latest_metrics = runs[-1]\n",
    "print(\"Logging metrics folder:\", latest_metrics)\n",
    "\n",
    "def _read_json(path: Path, *, required: bool = True) -> dict:\n",
    "    if not path.exists():\n",
    "        if required:\n",
    "            raise FileNotFoundError(f\"Expected file missing: {path}\")\n",
    "        return {}\n",
    "    return json.loads(path.read_text())\n",
    "\n",
    "metrics = _read_json(latest_metrics / \"metrics.json\")\n",
    "inputs = _read_json(latest_metrics / \"inputs.json\", required=False)\n",
    "params = _read_json(latest_metrics / \"params.json\", required=False)\n",
    "\n",
    "overall = metrics.get(\"overall\", {})\n",
    "macro = overall.get(\"macro_avg\", {})\n",
    "\n",
    "row = [\n",
    "    str(latest_metrics),\n",
    "    inputs.get(\"predictions\", \"\"),\n",
    "    inputs.get(\"split_source\", \"\"),\n",
    "    metrics.get(\"num_examples\", \"\"),\n",
    "    round(overall.get(\"accuracy\", 0.0), 4),\n",
    "    round(macro.get(\"f1\", 0.0), 4),\n",
    "    json.dumps(params, sort_keys=True)[:500],\n",
    "]\n",
    "\n",
    "ws = gc.open(EVAL_SHEET_NAME).worksheet(EVAL_WORKSHEET)\n",
    "ws.append_row(row, value_input_option=\"USER_ENTERED\")\n",
    "print(f\"Appended metrics run {latest_metrics.name} to {EVAL_SHEET_NAME}/{EVAL_WORKSHEET} ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ddb94",
   "metadata": {},
   "source": [
    "### üìä 11.4a Visualize Confusion Matrix\n",
    "\n",
    "Quick peek at where the model confuses classes using the most recent metrics run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c65d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "METRICS_TAG = (\n",
    "    globals().get(\"METRICS_TAG\")\n",
    "    or globals().get(\"PRED_TAG\")\n",
    "    or \"convnext_tiny_full_v1_val\"\n",
    ")  # change if you used a different --out-tag\n",
    "metrics_root = Path(OUT_ROOT) / \"metrics\" / METRICS_TAG\n",
    "runs = sorted(metrics_root.glob(\"*/\"))\n",
    "if not runs:\n",
    "    raise FileNotFoundError(f\"No metrics runs found under {metrics_root}\")\n",
    "latest_metrics = runs[-1]\n",
    "print(\"Reading confusion matrix from:\", latest_metrics)\n",
    "\n",
    "metrics = json.loads((latest_metrics / \"metrics.json\").read_text())\n",
    "cm_info = metrics.get(\"confusion_matrix\")\n",
    "if not cm_info:\n",
    "    raise ValueError(\"confusion_matrix missing from metrics.json\")\n",
    "\n",
    "labels = cm_info[\"rows_cols_labels\"]\n",
    "cm_df = pd.DataFrame(cm_info[\"matrix\"], index=labels, columns=labels)\n",
    "\n",
    "counts_path = latest_metrics / \"confusion_matrix_counts.png\"\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(f\"Confusion matrix ‚Äì {METRICS_TAG}\")\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(counts_path)\n",
    "plt.show()\n",
    "print(\"Saved counts heatmap to\", counts_path)\n",
    "\n",
    "cm_norm = cm_df.div(cm_df.sum(axis=1).replace(0, 1), axis=0)\n",
    "norm_path = latest_metrics / \"confusion_matrix_normalized.png\"\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "plt.title(f\"Normalized confusion matrix ‚Äì {METRICS_TAG}\")\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(norm_path)\n",
    "plt.show()\n",
    "print(\"Saved normalized heatmap to\", norm_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c11b4c",
   "metadata": {},
   "source": [
    "## üî• 11.5 Grad-CAM Visualizations\n",
    "\n",
    "**Grad-CAM (Gradient-weighted Class Activation Mapping)** shows which regions of the image the model focuses on when making predictions.\n",
    "\n",
    "**Use cases:**\n",
    "- **Full image models:** Verify the model looks at face/hands, not background\n",
    "- **Hybrid crop models:** See which specific features (hand position, facial expression) matter most\n",
    "- **Thesis comparison:** Visual evidence of WHY ROI cropping helps\n",
    "\n",
    "This cell generates:\n",
    "1. Grad-CAM heatmaps for sample images\n",
    "2. Comparison of correct vs misclassified predictions\n",
    "3. Per-class attention patterns\n",
    "4. Saved visualizations for your thesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• Grad-CAM Visualization\n",
    "!pip -q install grad-cam\n",
    "\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision import transforms\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# Use the same model/checkpoint from your training run\n",
    "GRADCAM_TAG = globals().get(\"TRAIN_TAG\") or globals().get(\"PRED_TAG\") or \"effb0_hybrid_face_hands\"\n",
    "MODEL_NAME = globals().get(\"MODEL_NAME\") or \"efficientnet_b0\"  # efficientnet_b0, resnet18, convnext_tiny\n",
    "SPLIT_TO_ANALYZE = \"val\"  # val or test\n",
    "N_SAMPLES_PER_CLASS = 3  # How many samples per class to visualize\n",
    "N_MISCLASSIFIED = 6  # How many misclassified examples to show\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# ===== FIND CHECKPOINT =====\n",
    "run_base = Path(CKPT_ROOT) / \"runs\" / GRADCAM_TAG\n",
    "all_runs = sorted(run_base.glob(\"*/\"))\n",
    "if not all_runs:\n",
    "    raise FileNotFoundError(f\"No run folders found under {run_base}\")\n",
    "latest_run = all_runs[-1]\n",
    "ckpt_path = latest_run / \"best.pt\"\n",
    "if not ckpt_path.exists():\n",
    "    ckpt_path = latest_run / \"last.pt\"\n",
    "print(f\"üìÅ Using checkpoint: {ckpt_path}\")\n",
    "\n",
    "# ===== LOAD MODEL =====\n",
    "from ddriver.models.registry import get_model\n",
    "\n",
    "# Load checkpoint to get num_classes\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "num_classes = ckpt[\"model_state_dict\"][\"classifier.weight\"].shape[0] if \"classifier.weight\" in ckpt[\"model_state_dict\"] else 10\n",
    "\n",
    "model = get_model(MODEL_NAME, num_classes=num_classes, pretrained=False)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"‚úÖ Loaded model: {MODEL_NAME} with {num_classes} classes\")\n",
    "\n",
    "# ===== DETERMINE TARGET LAYER FOR GRAD-CAM =====\n",
    "# Different architectures have different final conv layers\n",
    "if \"efficientnet\" in MODEL_NAME:\n",
    "    target_layers = [model.backbone.features[-1]]\n",
    "elif \"resnet\" in MODEL_NAME:\n",
    "    target_layers = [model.backbone.layer4[-1]]\n",
    "elif \"convnext\" in MODEL_NAME:\n",
    "    target_layers = [model.backbone.features[-1]]\n",
    "else:\n",
    "    # Fallback: try to find last conv layer\n",
    "    target_layers = [list(model.backbone.children())[-2]]\n",
    "print(f\"üéØ Target layer for Grad-CAM: {target_layers[0].__class__.__name__}\")\n",
    "\n",
    "# ===== LOAD PREDICTIONS AND DATA =====\n",
    "# Find the predictions CSV and manifest\n",
    "preds_csv = Path(OUT_ROOT) / \"preds\" / SPLIT_TO_ANALYZE / f\"{GRADCAM_TAG}.csv\"\n",
    "if not preds_csv.exists():\n",
    "    raise FileNotFoundError(f\"Predictions not found: {preds_csv}\\nRun the prediction cell first!\")\n",
    "\n",
    "preds_df = pd.read_csv(preds_csv)\n",
    "print(f\"üìä Loaded {len(preds_df)} predictions from {preds_csv}\")\n",
    "\n",
    "# Determine data root (hybrid, yolo, mediapipe, or full images)\n",
    "if \"USE_HYBRID\" in globals() and USE_HYBRID:\n",
    "    data_root = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", Path(OUT_ROOT) / \"hybrid\"))\n",
    "elif \"USE_YOLO\" in globals() and USE_YOLO:\n",
    "    data_root = Path(os.environ.get(\"YOLO_ROOT_LOCAL\", Path(OUT_ROOT) / \"yolo\"))\n",
    "elif \"USE_MEDIAPIPE\" in globals() and USE_MEDIAPIPE:\n",
    "    data_root = Path(OUT_ROOT) / \"mediapipe\"\n",
    "else:\n",
    "    data_root = Path(DATASET_ROOT)\n",
    "print(f\"üìÅ Data root: {data_root}\")\n",
    "\n",
    "# ===== CLASS NAMES =====\n",
    "CLASS_NAMES = {\n",
    "    0: \"Safe driving\",\n",
    "    1: \"Texting (right)\",\n",
    "    2: \"Talking (right)\",\n",
    "    3: \"Texting (left)\",\n",
    "    4: \"Talking (left)\",\n",
    "    5: \"Operating radio\",\n",
    "    6: \"Drinking\",\n",
    "    7: \"Reaching behind\",\n",
    "    8: \"Hair/makeup\",\n",
    "    9: \"Talking to passenger\",\n",
    "}\n",
    "\n",
    "# ===== TRANSFORMS =====\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def load_image(path):\n",
    "    \"\"\"Load image and return both tensor and RGB numpy array.\"\"\"\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img_resized = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    img_np = np.array(img_resized) / 255.0  # Normalized 0-1 for overlay\n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "    return img_tensor, img_np\n",
    "\n",
    "# ===== GENERATE GRAD-CAM =====\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "\n",
    "def generate_gradcam(img_path, target_class=None):\n",
    "    \"\"\"Generate Grad-CAM visualization for an image.\"\"\"\n",
    "    img_tensor, img_np = load_image(img_path)\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    \n",
    "    # Generate CAM (None = use predicted class)\n",
    "    grayscale_cam = cam(input_tensor=img_tensor, targets=None)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    # Overlay on image\n",
    "    visualization = show_cam_on_image(img_np.astype(np.float32), grayscale_cam, use_rgb=True)\n",
    "    return visualization, grayscale_cam\n",
    "\n",
    "# ===== 1. SAMPLE CORRECT PREDICTIONS PER CLASS =====\n",
    "print(\"\\nüé® Generating Grad-CAM for correct predictions per class...\")\n",
    "\n",
    "correct_df = preds_df[preds_df[\"label\"] == preds_df[\"pred\"]]\n",
    "fig, axes = plt.subplots(num_classes, N_SAMPLES_PER_CLASS * 2, figsize=(N_SAMPLES_PER_CLASS * 6, num_classes * 3))\n",
    "\n",
    "for class_id in range(num_classes):\n",
    "    class_samples = correct_df[correct_df[\"label\"] == class_id]\n",
    "    samples = class_samples.sample(min(N_SAMPLES_PER_CLASS, len(class_samples)), random_state=42)\n",
    "    \n",
    "    for i, (_, row) in enumerate(samples.iterrows()):\n",
    "        if i >= N_SAMPLES_PER_CLASS:\n",
    "            break\n",
    "        \n",
    "        img_path = data_root / row[\"path\"] if not Path(row[\"path\"]).is_absolute() else Path(row[\"path\"])\n",
    "        if not img_path.exists():\n",
    "            img_path = Path(DATASET_ROOT) / row[\"path\"]\n",
    "        \n",
    "        if img_path.exists():\n",
    "            # Original image\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "            axes[class_id, i * 2].imshow(img)\n",
    "            axes[class_id, i * 2].set_title(f\"{CLASS_NAMES.get(class_id, f'Class {class_id}')}\", fontsize=8)\n",
    "            axes[class_id, i * 2].axis(\"off\")\n",
    "            \n",
    "            # Grad-CAM\n",
    "            viz, _ = generate_gradcam(img_path)\n",
    "            axes[class_id, i * 2 + 1].imshow(viz)\n",
    "            axes[class_id, i * 2 + 1].set_title(f\"Grad-CAM (conf: {row['confidence']:.2f})\", fontsize=8)\n",
    "            axes[class_id, i * 2 + 1].axis(\"off\")\n",
    "        else:\n",
    "            axes[class_id, i * 2].text(0.5, 0.5, \"Not found\", ha=\"center\", va=\"center\")\n",
    "            axes[class_id, i * 2].axis(\"off\")\n",
    "            axes[class_id, i * 2 + 1].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Grad-CAM: Correct Predictions per Class ({MODEL_NAME})\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "gradcam_dir = Path(OUT_ROOT) / \"gradcam\" / GRADCAM_TAG\n",
    "gradcam_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(gradcam_dir / \"correct_per_class.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"üíæ Saved to {gradcam_dir / 'correct_per_class.png'}\")\n",
    "\n",
    "# ===== 2. MISCLASSIFIED EXAMPLES =====\n",
    "print(\"\\nüî¥ Generating Grad-CAM for misclassified examples...\")\n",
    "\n",
    "misclassified_df = preds_df[preds_df[\"label\"] != preds_df[\"pred\"]]\n",
    "if len(misclassified_df) > 0:\n",
    "    samples = misclassified_df.sample(min(N_MISCLASSIFIED, len(misclassified_df)), random_state=42)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(samples), 3, figsize=(12, len(samples) * 3))\n",
    "    if len(samples) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, (_, row) in enumerate(samples.iterrows()):\n",
    "        img_path = data_root / row[\"path\"] if not Path(row[\"path\"]).is_absolute() else Path(row[\"path\"])\n",
    "        if not img_path.exists():\n",
    "            img_path = Path(DATASET_ROOT) / row[\"path\"]\n",
    "        \n",
    "        if img_path.exists():\n",
    "            # Original\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "            axes[i, 0].imshow(img)\n",
    "            axes[i, 0].set_title(f\"True: {CLASS_NAMES.get(int(row['label']), row['label'])}\", fontsize=9)\n",
    "            axes[i, 0].axis(\"off\")\n",
    "            \n",
    "            # Grad-CAM\n",
    "            viz, heatmap = generate_gradcam(img_path)\n",
    "            axes[i, 1].imshow(viz)\n",
    "            axes[i, 1].set_title(f\"Pred: {CLASS_NAMES.get(int(row['pred']), row['pred'])} ({row['confidence']:.2f})\", fontsize=9)\n",
    "            axes[i, 1].axis(\"off\")\n",
    "            \n",
    "            # Heatmap only\n",
    "            axes[i, 2].imshow(heatmap, cmap=\"jet\")\n",
    "            axes[i, 2].set_title(\"Attention heatmap\", fontsize=9)\n",
    "            axes[i, 2].axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(f\"Grad-CAM: Misclassified Examples ({MODEL_NAME})\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(gradcam_dir / \"misclassified.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"üíæ Saved to {gradcam_dir / 'misclassified.png'}\")\n",
    "else:\n",
    "    print(\"‚úÖ No misclassified examples found!\")\n",
    "\n",
    "# ===== 3. SUMMARY STATISTICS =====\n",
    "print(\"\\nüìä Grad-CAM Analysis Summary:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Checkpoint: {ckpt_path.name}\")\n",
    "print(f\"   Split analyzed: {SPLIT_TO_ANALYZE}\")\n",
    "print(f\"   Total predictions: {len(preds_df)}\")\n",
    "print(f\"   Correct: {len(correct_df)} ({100*len(correct_df)/len(preds_df):.1f}%)\")\n",
    "print(f\"   Misclassified: {len(misclassified_df)} ({100*len(misclassified_df)/len(preds_df):.1f}%)\")\n",
    "print(f\"\\nüìÅ Visualizations saved to: {gradcam_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64534ea4",
   "metadata": {},
   "source": [
    "## üî¨ 11.5a Grad-CAM Comparison: Full Image vs Hybrid Crops (Optional)\n",
    "\n",
    "If you've trained both a **full-image model** and a **hybrid crop model**, run this cell to generate side-by-side comparisons showing how cropping changes the model's attention.\n",
    "\n",
    "**Thesis insight:** This demonstrates WHY ROI cropping helps ‚Äî the full-image model may attend to irrelevant regions while the crop model focuses on meaningful features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32584ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ Grad-CAM Comparison: Full Image vs Hybrid Crops\n",
    "# Requires: trained models for BOTH full images AND hybrid crops\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision import transforms\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# Tags for your trained models (must have run predictions for both)\n",
    "FULL_IMAGE_TAG = \"effb0_full_images\"      # Your full-image model tag\n",
    "HYBRID_TAG = \"effb0_hybrid_face_hands\"    # Your hybrid crop model tag\n",
    "MODEL_NAME = \"efficientnet_b0\"            # Must be same architecture for both\n",
    "SPLIT = \"val\"\n",
    "N_COMPARISON_SAMPLES = 6\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# ===== HELPER FUNCTIONS =====\n",
    "def load_model_and_cam(tag, model_name):\n",
    "    \"\"\"Load a model and create GradCAM for it.\"\"\"\n",
    "    from ddriver.models.registry import get_model\n",
    "    \n",
    "    run_base = Path(CKPT_ROOT) / \"runs\" / tag\n",
    "    all_runs = sorted(run_base.glob(\"*/\"))\n",
    "    if not all_runs:\n",
    "        return None, None, None\n",
    "    latest_run = all_runs[-1]\n",
    "    ckpt_path = latest_run / \"best.pt\"\n",
    "    if not ckpt_path.exists():\n",
    "        ckpt_path = latest_run / \"last.pt\"\n",
    "    \n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    num_classes = ckpt[\"model_state_dict\"].get(\"classifier.weight\", torch.zeros(10, 1)).shape[0]\n",
    "    \n",
    "    model = get_model(model_name, num_classes=num_classes, pretrained=False)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    if \"efficientnet\" in model_name:\n",
    "        target_layers = [model.backbone.features[-1]]\n",
    "    elif \"resnet\" in model_name:\n",
    "        target_layers = [model.backbone.layer4[-1]]\n",
    "    else:\n",
    "        target_layers = [model.backbone.features[-1]]\n",
    "    \n",
    "    cam = GradCAM(model=model, target_layers=target_layers)\n",
    "    return model, cam, ckpt_path\n",
    "\n",
    "# Load both models\n",
    "print(\"Loading models...\")\n",
    "full_model, full_cam, full_ckpt = load_model_and_cam(FULL_IMAGE_TAG, MODEL_NAME)\n",
    "hybrid_model, hybrid_cam, hybrid_ckpt = load_model_and_cam(HYBRID_TAG, MODEL_NAME)\n",
    "\n",
    "if full_model is None:\n",
    "    print(f\"‚ö†Ô∏è Full image model not found: {FULL_IMAGE_TAG}\")\n",
    "    print(\"   Train a full-image model first, or update FULL_IMAGE_TAG\")\n",
    "if hybrid_model is None:\n",
    "    print(f\"‚ö†Ô∏è Hybrid model not found: {HYBRID_TAG}\")\n",
    "    print(\"   Train a hybrid model first, or update HYBRID_TAG\")\n",
    "\n",
    "if full_model is None or hybrid_model is None:\n",
    "    raise RuntimeError(\"Both models required for comparison. Check tags above.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "full_model = full_model.to(device)\n",
    "hybrid_model = hybrid_model.to(device)\n",
    "print(f\"‚úÖ Loaded both models\")\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load predictions to find common samples\n",
    "full_preds = pd.read_csv(Path(OUT_ROOT) / \"preds\" / SPLIT / f\"{FULL_IMAGE_TAG}.csv\")\n",
    "hybrid_preds = pd.read_csv(Path(OUT_ROOT) / \"preds\" / SPLIT / f\"{HYBRID_TAG}.csv\")\n",
    "\n",
    "# Full images path and hybrid crops path\n",
    "full_data_root = Path(DATASET_ROOT)\n",
    "hybrid_data_root = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", Path(OUT_ROOT) / \"hybrid\"))\n",
    "\n",
    "# Sample images\n",
    "samples = full_preds.sample(min(N_COMPARISON_SAMPLES, len(full_preds)), random_state=42)\n",
    "\n",
    "# Generate comparison\n",
    "fig, axes = plt.subplots(len(samples), 4, figsize=(16, len(samples) * 3.5))\n",
    "\n",
    "CLASS_NAMES = {\n",
    "    0: \"Safe\", 1: \"Text-R\", 2: \"Talk-R\", 3: \"Text-L\", 4: \"Talk-L\",\n",
    "    5: \"Radio\", 6: \"Drink\", 7: \"Reach\", 8: \"Hair\", 9: \"Passenger\"\n",
    "}\n",
    "\n",
    "for i, (_, row) in enumerate(samples.iterrows()):\n",
    "    # Find corresponding hybrid path\n",
    "    full_path = full_data_root / row[\"path\"]\n",
    "    \n",
    "    # Match in hybrid predictions by finding same original image\n",
    "    # The hybrid manifest has \"original_path\" column\n",
    "    hybrid_manifest = pd.read_csv(hybrid_data_root / f\"manifest_face_hands.csv\")\n",
    "    match = hybrid_manifest[hybrid_manifest[\"original_path\"].str.contains(Path(row[\"path\"]).name, na=False)]\n",
    "    \n",
    "    if len(match) == 0:\n",
    "        continue\n",
    "    hybrid_path = hybrid_data_root / match.iloc[0][\"path\"]\n",
    "    \n",
    "    # Load and process full image\n",
    "    if full_path.exists():\n",
    "        full_img = Image.open(full_path).convert(\"RGB\")\n",
    "        full_img_resized = full_img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        full_np = np.array(full_img_resized) / 255.0\n",
    "        full_tensor = transform(full_img).unsqueeze(0).to(device)\n",
    "        \n",
    "        full_cam_result = full_cam(input_tensor=full_tensor, targets=None)[0]\n",
    "        full_viz = show_cam_on_image(full_np.astype(np.float32), full_cam_result, use_rgb=True)\n",
    "        \n",
    "        axes[i, 0].imshow(full_img_resized)\n",
    "        axes[i, 0].set_title(f\"Full Image\\nTrue: {CLASS_NAMES.get(int(row['label']), row['label'])}\", fontsize=9)\n",
    "        axes[i, 0].axis(\"off\")\n",
    "        \n",
    "        axes[i, 1].imshow(full_viz)\n",
    "        axes[i, 1].set_title(f\"Full Grad-CAM\\nPred: {CLASS_NAMES.get(int(row['pred']), row['pred'])}\", fontsize=9)\n",
    "        axes[i, 1].axis(\"off\")\n",
    "    \n",
    "    # Load and process hybrid crop\n",
    "    if hybrid_path.exists():\n",
    "        hybrid_img = Image.open(hybrid_path).convert(\"RGB\")\n",
    "        hybrid_img_resized = hybrid_img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        hybrid_np = np.array(hybrid_img_resized) / 255.0\n",
    "        hybrid_tensor = transform(hybrid_img).unsqueeze(0).to(device)\n",
    "        \n",
    "        hybrid_cam_result = hybrid_cam(input_tensor=hybrid_tensor, targets=None)[0]\n",
    "        hybrid_viz = show_cam_on_image(hybrid_np.astype(np.float32), hybrid_cam_result, use_rgb=True)\n",
    "        \n",
    "        axes[i, 2].imshow(hybrid_img_resized)\n",
    "        axes[i, 2].set_title(f\"Hybrid Crop\", fontsize=9)\n",
    "        axes[i, 2].axis(\"off\")\n",
    "        \n",
    "        axes[i, 3].imshow(hybrid_viz)\n",
    "        hybrid_row = hybrid_preds[hybrid_preds[\"path\"].str.contains(Path(hybrid_path).name, na=False)]\n",
    "        pred_label = hybrid_row.iloc[0][\"pred\"] if len(hybrid_row) > 0 else \"?\"\n",
    "        axes[i, 3].set_title(f\"Hybrid Grad-CAM\\nPred: {CLASS_NAMES.get(int(pred_label), pred_label)}\", fontsize=9)\n",
    "        axes[i, 3].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Grad-CAM Comparison: Full Image vs Hybrid Crop ({MODEL_NAME})\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "comparison_path = Path(OUT_ROOT) / \"gradcam\" / \"comparison_full_vs_hybrid.png\"\n",
    "comparison_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(comparison_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"\\nüíæ Saved comparison to: {comparison_path}\")\n",
    "print(\"\\nüìù Thesis talking point:\")\n",
    "print(\"   'The full-image model attends to [background/seat/etc] while the\")\n",
    "print(\"    hybrid-crop model focuses on [hand position/facial features/etc],\")\n",
    "print(\"    demonstrating the value of ROI-based preprocessing.'\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
