{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7296b001",
   "metadata": {},
   "source": [
    "\n",
    "# üöÄ Colab Setup ‚Äî **CNNs-distracted-driving** (hardcoded + config-aware)\n",
    "\n",
    "This version is **simplified and hardcoded** for your repo and URL, and it **respects your `src/ddriver/config.py`**.\n",
    "- Repo name fixed to **`CNNs-distracted-driving`**\n",
    "- Repo URL fixed to **`https://github.com/ClaudiaCPach/CNNs-distracted-driving`**\n",
    "- Uses your `config.py` convention: when running in Colab, we **set env vars** (`DRIVE_PATH`, `DATASET_ROOT`, `OUT_ROOT`, `CKPT_ROOT`, `FAST_DATA`) so your code reads correct paths via `ddriver.config`.\n",
    "- Optional `FAST_DATA` at `/content/data` for faster I/O (if you later copy data there).\n",
    "\n",
    "> Run cells **top ‚Üí bottom** the first time. Re-run **Update repo** to pull new commits after you push.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0fd729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîß 0) (Optional) quick GPU check\n",
    "!nvidia-smi || echo \"No GPU detected ‚Äî CPU runtime is okay for setup steps.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîß 1) Fixed config for your repo + Drive layout\n",
    "REPO_URL       = \"https://github.com/ClaudiaCPach/CNNs-distracted-driving\"\n",
    "REPO_DIRNAME   = \"CNNs-distracted-driving\"   # hardcoded\n",
    "BRANCH         = \"main\"\n",
    "PROJECT_ROOT   = f\"/content/{REPO_DIRNAME}\"  # where the repo will live in Colab\n",
    "\n",
    "# Your persistent Google Drive base folder (matches your project docs):\n",
    "DRIVE_PATH     = \"/content/drive/MyDrive/TFM\"\n",
    "\n",
    "# Your dataset lives under TFM/data/auc.distracted.driver.dataset_v2 (as per your structure)\n",
    "DATASET_ROOT   = f\"{DRIVE_PATH}/data\"   # not .../data/auc.distracted.driver.dataset_v2OUT_ROOT       = f\"{DRIVE_PATH}/outputs\"\n",
    "CKPT_ROOT      = f\"{DRIVE_PATH}/checkpoints\"\n",
    "\n",
    "# Optional: a fast, ephemeral workspace inside the VM\n",
    "FAST_DATA      = \"/content/data\"   # leave as-is; you can rsync into this later for speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîå 2) Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "print(\"‚úÖ Drive mounted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df094a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìÅ 3) Clone or update the repo (no name inference ‚Äî all hardcoded)\n",
    "import os, subprocess\n",
    "\n",
    "def sh(cmd):\n",
    "    print(f\"\\n$ {cmd}\")\n",
    "    rc = subprocess.call(cmd, shell=True, executable=\"/bin/bash\")\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {rc}: {cmd}\")\n",
    "\n",
    "if os.path.isdir(PROJECT_ROOT):\n",
    "    print(f\"üìÅ Repo already present at {PROJECT_ROOT}. Pulling latest on branch {BRANCH}...\")\n",
    "    sh(f\"cd {PROJECT_ROOT} && git fetch origin {BRANCH} && git checkout {BRANCH} && git pull --rebase origin {BRANCH}\")\n",
    "else:\n",
    "    print(f\"‚¨áÔ∏è Cloning {REPO_URL} ‚Üí {PROJECT_ROOT}\")\n",
    "    sh(f\"git clone --branch {BRANCH} {REPO_URL} {PROJECT_ROOT}\")\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üì¶ 4) Install the repo (editable) + requirements (uses pyproject.toml if present)\n",
    "import os, subprocess\n",
    "\n",
    "def sh(cmd):\n",
    "    print(f\"\\n$ {cmd}\")\n",
    "    rc = subprocess.call(cmd, shell=True, executable=\"/bin/bash\")\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {rc}: {cmd}\")\n",
    "\n",
    "print(\"üîÑ Upgrading pip/setuptools/wheel...\")\n",
    "sh(\"python -m pip install --upgrade pip setuptools wheel\")\n",
    "\n",
    "has_pyproject = os.path.exists(os.path.join(PROJECT_ROOT, \"pyproject.toml\"))\n",
    "if has_pyproject:\n",
    "    print(\"üì¶ Editable install from pyproject.toml ...\")\n",
    "    sh(f\"cd {PROJECT_ROOT} && pip install -e .\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No pyproject.toml found. Skipping editable install.\")\n",
    "\n",
    "req_path = os.path.join(PROJECT_ROOT, \"requirements.txt\")\n",
    "if os.path.exists(req_path):\n",
    "    print(\"üìù Installing requirements.txt...\")\n",
    "    sh(f\"pip install -r {req_path}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No requirements.txt found ‚Äî continuing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accfc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üå≥ 5) Configure environment for your ddriver.config (Colab branch)\n",
    "# Your config.py reads env vars and falls back to sensible defaults when in Colab.\n",
    "import os\n",
    "\n",
    "os.environ[\"DRIVE_PATH\"]   = DRIVE_PATH\n",
    "os.environ[\"DATASET_ROOT\"] = DATASET_ROOT\n",
    "os.environ[\"OUT_ROOT\"]     = OUT_ROOT\n",
    "os.environ[\"CKPT_ROOT\"]    = CKPT_ROOT\n",
    "os.environ[\"FAST_DATA\"]    = FAST_DATA\n",
    "\n",
    "# Also write a .env (harmless in Colab; helpful if code calls load_dotenv())\n",
    "env_text = f\"\"\"DRIVE_PATH={DRIVE_PATH}\n",
    "DATASET_ROOT={DATASET_ROOT}\n",
    "OUT_ROOT={OUT_ROOT}\n",
    "CKPT_ROOT={CKPT_ROOT}\n",
    "FAST_DATA={FAST_DATA}\n",
    "\"\"\"\n",
    "with open(os.path.join(PROJECT_ROOT, \".env\"), \"w\") as f:\n",
    "    f.write(env_text)\n",
    "\n",
    "print(\"‚úÖ Environment variables set for ddriver.config\")\n",
    "print(\"\\nSummary:\")\n",
    "for k in [\"DRIVE_PATH\",\"DATASET_ROOT\",\"OUT_ROOT\",\"CKPT_ROOT\",\"FAST_DATA\"]:\n",
    "    print(f\"{k} = {os.environ[k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27df24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîó 6) (Optional) Symlink dataset into repo for familiar paths (scripts that assume PROJECT_ROOT/data/...)\n",
    "# Not required when using ddriver.config, but convenient for ad-hoc browsing.\n",
    "import os\n",
    "\n",
    "LOCAL_DATA_DIR = f\"{PROJECT_ROOT}/data\"\n",
    "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
    "\n",
    "dataset_link = os.path.join(LOCAL_DATA_DIR, \"auc.distracted.driver.dataset_v2\")\n",
    "if not os.path.islink(dataset_link) and not os.path.exists(dataset_link):\n",
    "    try:\n",
    "        os.symlink(DATASET_ROOT, dataset_link)\n",
    "        print(f\"üîó Symlinked {dataset_link} ‚Üí {DATASET_ROOT}\")\n",
    "    except OSError as e:\n",
    "        print(f\"‚ÑπÔ∏è Symlink skipped or failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Dataset link already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf8bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîç 7) Quick sanity checks\n",
    "import os, glob\n",
    "\n",
    "def preview_dir(path, n=10):\n",
    "    print(f\"Listing up to {n} items under: {path}\")\n",
    "    try:\n",
    "        for i, name in enumerate(sorted(os.listdir(path))):\n",
    "            print(\"  -\", name)\n",
    "            if i+1 >= n:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(\"Could not list:\", e)\n",
    "\n",
    "print(\"\\nTop-level DATASET_ROOT:\")\n",
    "preview_dir(os.environ[\"DATASET_ROOT\"], n=10)\n",
    "\n",
    "cam1_train = os.path.join(os.environ[\"DATASET_ROOT\"], \"v2_cam1_cam2_ split_by_driver\", \"Camera 1\", \"train\")\n",
    "print(\"\\nCamera 1/train class folders (first 10):\")\n",
    "preview_dir(cam1_train, n=10)\n",
    "\n",
    "for cls in [\"c0\",\"c1\",\"c2\"]:\n",
    "    cls_dir = os.path.join(cam1_train, cls)\n",
    "    if os.path.isdir(cls_dir):\n",
    "        num_imgs = len([p for p in glob.glob(os.path.join(cls_dir, \"*\")) if os.path.isfile(p)])\n",
    "        print(f\"  ‚Ä¢ {cls}: {num_imgs} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ 8) Import smoke test (uses your package + config.py)\n",
    "import sys, os\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "sys.path.append(os.path.join(PROJECT_ROOT, \"src\"))  # <‚Äî lets Python find src/ddriver\n",
    "\n",
    "try:\n",
    "    import ddriver\n",
    "    print(\"ddriver imported OK from:\", ddriver.__file__)\n",
    "    # Confirm config picks up Colab env:\n",
    "    try:\n",
    "        from ddriver import config\n",
    "        print(\"Loaded ddriver.config successfully.\")\n",
    "        # Echo the resolved paths from config (they are pathlib.Path objects)\n",
    "        print(\"config.DATASET_ROOT =\", config.DATASET_ROOT)\n",
    "        print(\"config.OUT_ROOT     =\", config.OUT_ROOT)\n",
    "        print(\"config.CKPT_ROOT    =\", config.CKPT_ROOT)\n",
    "        print(\"config.FAST_DATA    =\", config.FAST_DATA)\n",
    "    except Exception as e:\n",
    "        print(\"Note: ddriver.config not imported:\", e)\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Import failed ‚Äî check package name/setup.\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374139f5",
   "metadata": {},
   "source": [
    "# üìã 9) Generate Manifest and Split CSVs\n",
    "\n",
    "This step creates the CSV files that tell your code where all the images are and which ones go to train/val/test.\n",
    "\n",
    "**What this does:**\n",
    "- Scans all your images in the dataset folder\n",
    "- Creates a big list (manifest.csv) with info about every image\n",
    "- Creates three smaller lists (train.csv, val.csv, test.csv) that say which images belong where\n",
    "- Saves everything to your Google Drive so it's permanent\n",
    "\n",
    "**Why we need this:**\n",
    "- Your training code needs to know which images to use\n",
    "- The manifest remembers which driver each image belongs to (for VAL split)\n",
    "- The split CSVs organize images into train/val/test groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8883386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the manifest generator\n",
    "# This is like asking a librarian to catalog all your books and create reading lists\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Make sure we can import ddriver\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Run the manifest script\n",
    "# --write-split-lists means \"also create train.csv, val.csv, test.csv files\"\n",
    "manifest_cmd = f\"cd {PROJECT_ROOT} && python -m ddriver.data.manifest --write-split-lists\"\n",
    "\n",
    "print(\"üî® Generating manifest and split CSVs...\")\n",
    "print(f\"Running: {manifest_cmd}\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    manifest_cmd,\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Show what happened\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Warnings/Errors:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n‚úÖ Manifest and split CSVs generated successfully!\")\n",
    "    print(f\"   Manifest: {os.environ['OUT_ROOT']}/manifests/manifest.csv\")\n",
    "    print(f\"   Train split: {os.environ['OUT_ROOT']}/splits/train.csv\")\n",
    "    print(f\"   Val split: {os.environ['OUT_ROOT']}/splits/val.csv\")\n",
    "    print(f\"   Test split: {os.environ['OUT_ROOT']}/splits/test.csv\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Error generating manifest (exit code {result.returncode})\")\n",
    "    raise RuntimeError(\"Manifest generation failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: Did the CSVs get created?\n",
    "# This is like checking that the librarian actually wrote down all the book lists\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "manifest_path = Path(os.environ['OUT_ROOT']) / \"manifests\" / \"manifest.csv\"\n",
    "train_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"train.csv\"\n",
    "val_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"val.csv\"\n",
    "test_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"test.csv\"\n",
    "\n",
    "print(\"üìä Checking CSV files...\\n\")\n",
    "\n",
    "for name, path in [(\"Manifest\", manifest_path), (\"Train\", train_path), (\"Val\", val_path), (\"Test\", test_path)]:\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"‚úÖ {name}: {len(df)} rows, columns: {list(df.columns)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name}: File not found at {path}\")\n",
    "\n",
    "# Show a sample from the manifest\n",
    "if manifest_path.exists():\n",
    "    print(\"\\nüìÑ Sample from manifest (first 3 rows):\")\n",
    "    sample = pd.read_csv(manifest_path).head(3)\n",
    "    print(sample[['path', 'class_id', 'driver_id', 'camera', 'split']].to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041552b4",
   "metadata": {},
   "source": [
    "# üß™ 10) Test dataset.py and datamod.py\n",
    "\n",
    "Now let's make sure the code that loads images actually works!\n",
    "\n",
    "**What we're testing:**\n",
    "1. **dataset.py** - Can it load a single image and give us the right info?\n",
    "2. **datamod.py** - Can it create data loaders that give us batches of images?\n",
    "\n",
    "**Why test this:**\n",
    "- If these don't work, training will fail\n",
    "- Better to catch problems now than later\n",
    "- We want to see that images load correctly and labels are right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6044f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Can dataset.py load a single image?\n",
    "# This is like testing if a worker can fetch one book from the library\n",
    "\n",
    "from ddriver.data.dataset import AucDriverDataset\n",
    "from torchvision import transforms as T\n",
    "from pathlib import Path\n",
    "\n",
    "# Get paths from config\n",
    "manifest_csv = Path(os.environ['OUT_ROOT']) / \"manifests\" / \"manifest.csv\"\n",
    "val_split_csv = Path(os.environ['OUT_ROOT']) / \"splits\" / \"val.csv\"\n",
    "\n",
    "print(\"üß™ Test 1: Testing AucDriverDataset (dataset.py)\")\n",
    "print(f\"   Manifest: {manifest_csv}\")\n",
    "print(f\"   Using Val split: {val_split_csv}\\n\")\n",
    "\n",
    "try:\n",
    "    # Create a simple dataset (no fancy transforms, just load the image)\n",
    "    simple_transforms = T.ToTensor()  # Just convert to tensor, no augmentation\n",
    "    \n",
    "    val_dataset = AucDriverDataset(\n",
    "        manifest_csv=manifest_csv,\n",
    "        split_csv=val_split_csv,\n",
    "        transforms=simple_transforms\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset created! It has {len(val_dataset)} images in VAL split\")\n",
    "    \n",
    "    # Try to load the first image\n",
    "    print(\"\\nüìñ Loading first image from VAL split...\")\n",
    "    sample = val_dataset[0]\n",
    "    \n",
    "    print(f\"‚úÖ Image loaded successfully!\")\n",
    "    print(f\"   Image shape: {sample['image'].shape} (should be [3, height, width])\")\n",
    "    print(f\"   Label: {sample['label']} (should be 0-9)\")\n",
    "    print(f\"   Driver ID: {sample['driver_id']} (VAL should have driver IDs)\")\n",
    "    print(f\"   Camera: {sample['camera']} (should be 'cam1' or 'cam2')\")\n",
    "    print(f\"   Path: {sample['path'][:80]}...\")  # Show first 80 chars\n",
    "    \n",
    "    # Check that label is valid (0-9)\n",
    "    if 0 <= sample['label'] <= 9:\n",
    "        print(f\"   ‚úÖ Label is valid (0-9)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Label {sample['label']} is NOT in range 0-9!\")\n",
    "    \n",
    "    # Check that VAL has driver IDs\n",
    "    if sample['driver_id'] is not None:\n",
    "        print(f\"   ‚úÖ VAL split has driver ID (as expected)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  VAL split missing driver ID (might be okay if this image wasn't in your DRIVER_RANGES)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Test 1 PASSED: dataset.py works!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 1 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286fdbe",
   "metadata": {},
   "source": [
    "# üßµ 11) Full pipeline (train ‚Üí predict ‚Üí metrics)\n",
    "\n",
    "Now that data loading is working, these next cells show how to:\n",
    "1. Register the model you want (e.g., `resnet18` from timm)\n",
    "2. Run training from the command line helper\n",
    "3. Generate predictions from a checkpoint\n",
    "4. Evaluate metrics and save all logs to Drive\n",
    "\n",
    "> You can change the `RUN_TAG`, model name, epochs, etc. in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register models you want to use (run once per runtime)\n",
    "# This example uses timm's resnet18.\n",
    "\n",
    "!pip -q install timm\n",
    "\n",
    "from ddriver.models import registry\n",
    "\n",
    "registry.register_timm_backbone(\"resnet18\")\n",
    "print(\"Available models:\", registry.available_models()[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e011f7",
   "metadata": {},
   "source": [
    "## üöÇ 11.1 Train a model (adjust these knobs)\n",
    "\n",
    "- Choose a `RUN_TAG` so logs/checkpoints go into `TFM/checkpoints/runs/<tag>/...`\n",
    "- Set epochs/batch size to something small for a dry run (1 epoch, 16 batch)\n",
    "- This command uses the CLI helper (`python -m src.ddriver.cli.train ...`)\n",
    "- Logs + checkpoints are saved automatically to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, textwrap\n",
    "\n",
    "RUN_TAG = \"cam_mix_dryrun\"      # change me for each experiment\n",
    "MODEL_NAME = \"resnet18\"         # must be registered above\n",
    "EPOCHS = 1                       # start tiny to make sure it works\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_SIZE = 224\n",
    "LR = 1e-3\n",
    "\n",
    "train_cmd = textwrap.dedent(f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.cli.train \\\n",
    "    --model-name {MODEL_NAME} \\\n",
    "    --epochs {EPOCHS} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --num-workers {NUM_WORKERS} \\\n",
    "    --image-size {IMAGE_SIZE} \\\n",
    "    --lr {LR} \\\n",
    "    --out-tag {RUN_TAG}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Running training command:\\n\", train_cmd)\n",
    "result = subprocess.run(train_cmd, shell=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"Training command failed. See logs above.\")\n",
    "print(\"\\n‚úÖ Training run complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c429a0e",
   "metadata": {},
   "source": [
    "## üì¶ 11.2 Pick the latest checkpoint file\n",
    "\n",
    "This cell looks inside `CKPT_ROOT/runs/<RUN_TAG>/` and grabs the newest `epoch_*.pt`. Use this path in the prediction step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "run_base = Path(CKPT_ROOT) / \"runs\" / RUN_TAG\n",
    "all_runs = sorted(run_base.glob(\"*/\"))\n",
    "if not all_runs:\n",
    "    raise FileNotFoundError(f\"No run folders found under {run_base}\")\n",
    "latest_run = all_runs[-1]\n",
    "\n",
    "checkpoints = sorted(latest_run.glob(\"epoch_*.pt\"))\n",
    "if not checkpoints:\n",
    "    raise FileNotFoundError(f\"No checkpoints found under {latest_run}\")\n",
    "LATEST_CKPT = checkpoints[-1]\n",
    "\n",
    "print(\"Latest run folder:\", latest_run)\n",
    "print(\"Using checkpoint:\", LATEST_CKPT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d7e3f8",
   "metadata": {},
   "source": [
    "## üîÆ 11.3 Generate predictions CSV\n",
    "\n",
    "- Uses the checkpoint above\n",
    "- Choose which split to predict on (`val` or `test`)\n",
    "- Saves CSV under `OUT_ROOT/preds/<split>/<out_tag>.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_SPLIT = \"val\"           # or \"test\"\n",
    "PRED_TAG = f\"{RUN_TAG}_{PRED_SPLIT}\"\n",
    "\n",
    "predict_cmd = textwrap.dedent(f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.cli.predict \\\n",
    "    --model-name {MODEL_NAME} \\\n",
    "    --checkpoint {LATEST_CKPT} \\\n",
    "    --split {PRED_SPLIT} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --num-workers {NUM_WORKERS} \\\n",
    "    --image-size {IMAGE_SIZE} \\\n",
    "    --out-tag {PRED_TAG}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Running prediction command:\\n\", predict_cmd)\n",
    "result = subprocess.run(predict_cmd, shell=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"Prediction command failed. See logs above.\")\n",
    "print(\"\\n‚úÖ Predictions completed! Check OUT_ROOT/preds/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ca361",
   "metadata": {},
   "source": [
    "## üìä 11.4 Evaluate metrics\n",
    "\n",
    "- Uses `src/ddriver/metrics.py`\n",
    "- Reads the manifest + split CSV + predictions CSV\n",
    "- Saves results under `OUT_ROOT/metrics/<tag>/<timestamp>/`\n",
    "- Shows accuracy + macro F1 + per-driver/camera (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "manifest_path = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "split_csv_path = Path(OUT_ROOT) / \"splits\" / f\"{PRED_SPLIT}.csv\"\n",
    "preds_csv_path = Path(OUT_ROOT) / \"preds\" / PRED_SPLIT / f\"{PRED_TAG}.csv\"\n",
    "METRICS_TAG = PRED_TAG\n",
    "\n",
    "metrics_cmd = textwrap.dedent(f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.metrics \\\n",
    "    --manifest {manifest_path} \\\n",
    "    --split-csv {split_csv_path} \\\n",
    "    --predictions {preds_csv_path} \\\n",
    "    --out-tag {METRICS_TAG} \\\n",
    "    --per-driver \\\n",
    "    --per-camera\n",
    "\"\"\")\n",
    "\n",
    "print(\"Running metrics command:\\n\", metrics_cmd)\n",
    "result = subprocess.run(metrics_cmd, shell=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"Metrics command failed. See logs above.\")\n",
    "print(\"\\n‚úÖ Metrics saved under OUT_ROOT/metrics/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c13864",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dec2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Can datamod.py create data loaders and load batches?\n",
    "# This is like testing if the teacher can organize students into groups and give them work\n",
    "\n",
    "from ddriver.data.datamod import build_dataloaders, make_cfg_from_config\n",
    "import torch\n",
    "\n",
    "print(\"üß™ Test 2: Testing build_dataloaders (datamod.py)\\n\")\n",
    "\n",
    "try:\n",
    "    # Create config using the helper that uses ddriver.config paths\n",
    "    # This is the easy way - it automatically finds your CSVs!\n",
    "    cfg = make_cfg_from_config(\n",
    "        batch_size=4,  # Small batch for testing (faster)\n",
    "        num_workers=2,  # Use 2 workers (Colab might have limited CPUs)\n",
    "        image_size=224,  # Standard image size\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Config created using ddriver.config paths:\")\n",
    "    print(f\"   Manifest: {cfg.manifest_csv}\")\n",
    "    print(f\"   Train: {cfg.train_split_csv}\")\n",
    "    print(f\"   Val: {cfg.val_split_csv}\")\n",
    "    print(f\"   Test: {cfg.test_split_csv}\\n\")\n",
    "    \n",
    "    # Build the data loaders\n",
    "    print(\"üî® Building data loaders...\")\n",
    "    loaders = build_dataloaders(cfg)\n",
    "    \n",
    "    print(\"‚úÖ Data loaders created!\")\n",
    "    print(f\"   Available splits: {list(loaders.keys())}\\n\")\n",
    "    \n",
    "    # Test train loader\n",
    "    print(\"üì¶ Testing TRAIN loader...\")\n",
    "    train_loader = loaders[\"train\"]\n",
    "    train_batch = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"   ‚úÖ Train batch loaded!\")\n",
    "    print(f\"   Batch size: {train_batch['image'].shape[0]} images\")\n",
    "    print(f\"   Image shape: {train_batch['image'].shape} (should be [batch_size, 3, 224, 224])\")\n",
    "    print(f\"   Labels: {train_batch['label'].tolist()} (should be list of 0-9)\")\n",
    "    print(f\"   Driver IDs: {train_batch['driver_id']} (train should mostly be None)\")\n",
    "    print(f\"   Cameras: {train_batch['camera']}\")\n",
    "    \n",
    "    # Check image shape is correct\n",
    "    expected_shape = (cfg.batch_size, 3, cfg.image_size, cfg.image_size)\n",
    "    if train_batch['image'].shape == expected_shape:\n",
    "        print(f\"   ‚úÖ Image shape is correct: {train_batch['image'].shape}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Image shape wrong! Got {train_batch['image'].shape}, expected {expected_shape}\")\n",
    "    \n",
    "    # Test val loader\n",
    "    print(\"\\nüì¶ Testing VAL loader...\")\n",
    "    val_loader = loaders[\"val\"]\n",
    "    val_batch = next(iter(val_loader))\n",
    "    \n",
    "    print(f\"   ‚úÖ Val batch loaded!\")\n",
    "    print(f\"   Batch size: {val_batch['image'].shape[0]} images\")\n",
    "    print(f\"   Image shape: {val_batch['image'].shape}\")\n",
    "    print(f\"   Labels: {val_batch['label'].tolist()}\")\n",
    "    print(f\"   Driver IDs: {val_batch['driver_id']} (VAL should have driver IDs!)\")\n",
    "    \n",
    "    # Check that VAL has driver IDs\n",
    "    val_has_ids = any(did is not None for did in val_batch['driver_id'])\n",
    "    if val_has_ids:\n",
    "        print(f\"   ‚úÖ VAL batch has driver IDs (as expected)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  VAL batch missing driver IDs (check your DRIVER_RANGES in manifest.py)\")\n",
    "    \n",
    "    # Test that images are normalized (should be in range roughly -2 to 2 after ImageNet normalization)\n",
    "    img_min, img_max = train_batch['image'].min().item(), train_batch['image'].max().item()\n",
    "    print(f\"\\n   Image value range: [{img_min:.3f}, {img_max:.3f}]\")\n",
    "    print(f\"   (Should be roughly -2 to 2 after ImageNet normalization)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Test 2 PASSED: datamod.py works! Data loaders are ready for training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 2 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1047bf",
   "metadata": {},
   "source": [
    "### ‚úÖ You're all set!\n",
    "\n",
    "**What just happened:**\n",
    "1. ‚úÖ Mounted Google Drive\n",
    "2. ‚úÖ Cloned/updated your repo\n",
    "3. ‚úÖ Installed the package\n",
    "4. ‚úÖ Set up paths (works on Colab and Mac!)\n",
    "5. ‚úÖ Generated manifest.csv and train/val/test split CSVs\n",
    "6. ‚úÖ Tested that dataset.py can load images\n",
    "7. ‚úÖ Tested that datamod.py can create data loaders\n",
    "8. ‚úÖ (Optional) Registered a model + ran training ‚Üí prediction ‚Üí metrics pipeline\n",
    "\n",
    "**Your CSVs are saved in Google Drive:**\n",
    "- `OUT_ROOT/manifests/manifest.csv` - Big list of all images\n",
    "- `OUT_ROOT/splits/train.csv` - Training images\n",
    "- `OUT_ROOT/splits/val.csv` - Validation images (with driver IDs!)\n",
    "- `OUT_ROOT/splits/test.csv` - Test images\n",
    "\n",
    "**Next steps:**\n",
    "- Adjust the training/prediction cells (epochs, batch size, tags) to run bigger experiments\n",
    "- All paths use `ddriver.config` so it works on Colab and Mac\n",
    "- Re-run **Clone/Update** cell after pushing new commits\n",
    "- Optional: copy some data into `/content/data` to use `FAST_DATA` for speed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1814de",
   "metadata": {},
   "source": [
    "\n",
    "### ‚úÖ You‚Äôre set!\n",
    "- Your repo + URL are **hardcoded**.\n",
    "- `ddriver.config` will see the Colab env vars and resolve paths there.\n",
    "- Re-run **Clone/Update** after pushing new commits.\n",
    "- Optional: copy some data into `/content/data` to use `FAST_DATA` for speed, then call `ddriver.config.dataset_dir(prefer_fast=True)` in your scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Colab cell: append metrics + params to Google Sheet ----\n",
    "!pip -q install gspread\n",
    "\n",
    "import json, os\n",
    "from pathlib import Path\n",
    "import gspread\n",
    "from ddriver import config\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "gc = gspread.authorize(gspread.auth.default())\n",
    "\n",
    "import gspread\n",
    "from google.colab import auth\n",
    "import google.auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "creds, _ = google.auth.default()\n",
    "gc = gspread.authorize(creds)\n",
    "\n",
    "SHEET_NAME = \"TFM Logs\"   # change if needed\n",
    "WORKSHEET  = \"Sheet1\"     # or whatever tab name\n",
    "\n",
    "# Point to your latest run folder (paste it from the console printout)\n",
    "run_dir = config.OUT_ROOT / \"runs\" / \"val_run_test\" / \"2025-11-23_2-50\"\n",
    "\n",
    "metrics = json.loads((run_dir / \"metrics.json\").read_text())\n",
    "inputs  = json.loads((run_dir / \"inputs.json\").read_text())\n",
    "params_path = run_dir / \"params.json\"\n",
    "params = json.loads(params_path.read_text()) if params_path.exists() else {}\n",
    "\n",
    "ws = gc.open(SHEET_NAME).worksheet(WORKSHEET)\n",
    "\n",
    "row = [\n",
    "  str(run_dir),                        # Run folder\n",
    "  inputs.get(\"predictions\",\"\"),        # Predictions file\n",
    "  inputs.get(\"split_source\",\"\"),       # Split source\n",
    "  metrics[\"num_examples\"],             # Support\n",
    "  round(metrics[\"overall\"][\"accuracy\"], 4),\n",
    "  round(metrics[\"overall\"][\"macro_avg\"][\"f1\"], 4),\n",
    "  json.dumps(params, sort_keys=True)[:500],  # params preview (trim)\n",
    "]\n",
    "ws.append_row(row, value_input_option=\"USER_ENTERED\")\n",
    "print(\"Appended to Google Sheet ‚úÖ\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
