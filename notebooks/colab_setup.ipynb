{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7296b001",
   "metadata": {},
   "source": [
    "\n",
    "# üöÄ Colab Setup ‚Äî **CNNs-distracted-driving** (hardcoded + config-aware)\n",
    "\n",
    "This version is **simplified and hardcoded** for your repo and URL, and it **respects your `src/ddriver/config.py`**.\n",
    "- Repo name fixed to **`CNNs-distracted-driving`**\n",
    "- Repo URL fixed to **`https://github.com/ClaudiaCPach/CNNs-distracted-driving`**\n",
    "- Uses your `config.py` convention: when running in Colab, we **set env vars** (`DRIVE_PATH`, `DATASET_ROOT`, `OUT_ROOT`, `CKPT_ROOT`, `FAST_DATA`) so your code reads correct paths via `ddriver.config`.\n",
    "- Optional `FAST_DATA` at `/content/data` for faster I/O (if you later copy data there).\n",
    "\n",
    "> Run cells **top ‚Üí bottom** the first time. Re-run **Update repo** to pull new commits after you push.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0fd729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîß 0) (Optional) quick GPU check\n",
    "!nvidia-smi || echo \"No GPU detected ‚Äî CPU runtime is okay for setup steps.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîß 1) Fixed config for your repo + Drive layout\n",
    "import os\n",
    "\n",
    "REPO_URL       = \"https://github.com/ClaudiaCPach/CNNs-distracted-driving\"\n",
    "REPO_DIRNAME   = \"CNNs-distracted-driving\"   # hardcoded\n",
    "BRANCH         = \"main\"\n",
    "PROJECT_ROOT   = f\"/content/{REPO_DIRNAME}\"  # where the repo will live in Colab\n",
    "\n",
    "# Your persistent Google Drive base folder (matches your project docs):\n",
    "DRIVE_PATH       = \"/content/drive/MyDrive/TFM\"\n",
    "DRIVE_DATA_ROOT  = f\"{DRIVE_PATH}/data\"      # contains auc.distracted.driver.dataset_v2\n",
    "\n",
    "# Optional: a fast, ephemeral workspace inside the VM\n",
    "FAST_DATA        = \"/content/data\"           # rsync target for faster I/O (lives on the VM SSD)\n",
    "\n",
    "# Start with Drive as the canonical dataset root; later cells can switch to FAST_DATA\n",
    "DATASET_ROOT     = DRIVE_DATA_ROOT\n",
    "OUT_ROOT         = f\"{DRIVE_PATH}/outputs\"\n",
    "CKPT_ROOT        = f\"{DRIVE_PATH}/checkpoints\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîå 2) Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "print(\"‚úÖ Drive mounted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df094a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìÅ 3) Clone or update the repo (no name inference ‚Äî all hardcoded)\n",
    "import os, subprocess\n",
    "\n",
    "def sh(cmd):\n",
    "    print(f\"\\n$ {cmd}\")\n",
    "    rc = subprocess.call(cmd, shell=True, executable=\"/bin/bash\")\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {rc}: {cmd}\")\n",
    "\n",
    "if os.path.isdir(PROJECT_ROOT):\n",
    "    print(f\"üìÅ Repo already present at {PROJECT_ROOT}. Pulling latest on branch {BRANCH}...\")\n",
    "    sh(f\"cd {PROJECT_ROOT} && git fetch origin {BRANCH} && git checkout {BRANCH} && git pull --rebase origin {BRANCH}\")\n",
    "else:\n",
    "    print(f\"‚¨áÔ∏è Cloning {REPO_URL} ‚Üí {PROJECT_ROOT}\")\n",
    "    sh(f\"git clone --branch {BRANCH} {REPO_URL} {PROJECT_ROOT}\")\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üì¶ 4) Install the repo (editable) + requirements (uses pyproject.toml if present)\n",
    "import os, subprocess\n",
    "\n",
    "def sh(cmd):\n",
    "    print(f\"\\n$ {cmd}\")\n",
    "    rc = subprocess.call(cmd, shell=True, executable=\"/bin/bash\")\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {rc}: {cmd}\")\n",
    "\n",
    "print(\"üîÑ Upgrading pip/setuptools/wheel...\")\n",
    "sh(\"python -m pip install --upgrade pip setuptools wheel\")\n",
    "\n",
    "has_pyproject = os.path.exists(os.path.join(PROJECT_ROOT, \"pyproject.toml\"))\n",
    "if has_pyproject:\n",
    "    print(\"üì¶ Editable install from pyproject.toml ...\")\n",
    "    sh(f\"cd {PROJECT_ROOT} && pip install -e .\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No pyproject.toml found. Skipping editable install.\")\n",
    "\n",
    "req_path = os.path.join(PROJECT_ROOT, \"requirements.txt\")\n",
    "if os.path.exists(req_path):\n",
    "    print(\"üìù Installing requirements.txt...\")\n",
    "    sh(f\"pip install -r {req_path}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No requirements.txt found ‚Äî continuing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accfc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üå≥ 5) Configure environment for your ddriver.config (Colab branch)\n",
    "# Your config.py reads env vars and falls back to sensible defaults when in Colab.\n",
    "import os\n",
    "\n",
    "os.environ[\"DRIVE_PATH\"]   = DRIVE_PATH\n",
    "os.environ[\"DATASET_ROOT\"] = DATASET_ROOT\n",
    "os.environ[\"OUT_ROOT\"]     = OUT_ROOT\n",
    "os.environ[\"CKPT_ROOT\"]    = CKPT_ROOT\n",
    "os.environ[\"FAST_DATA\"]    = FAST_DATA\n",
    "\n",
    "# Also write a .env (harmless in Colab; helpful if code calls load_dotenv())\n",
    "env_text = f\"\"\"DRIVE_PATH={DRIVE_PATH}\n",
    "DATASET_ROOT={DATASET_ROOT}\n",
    "OUT_ROOT={OUT_ROOT}\n",
    "CKPT_ROOT={CKPT_ROOT}\n",
    "FAST_DATA={FAST_DATA}\n",
    "\"\"\"\n",
    "with open(os.path.join(PROJECT_ROOT, \".env\"), \"w\") as f:\n",
    "    f.write(env_text)\n",
    "\n",
    "print(\"‚úÖ Environment variables set for ddriver.config\")\n",
    "print(\"\\nSummary:\")\n",
    "for k in [\"DRIVE_PATH\",\"DATASET_ROOT\",\"OUT_ROOT\",\"CKPT_ROOT\",\"FAST_DATA\"]:\n",
    "    print(f\"{k} = {os.environ[k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27df24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîó 6) (Optional) Symlink dataset into repo for familiar paths (scripts that assume PROJECT_ROOT/data/...)\n",
    "# Not required when using ddriver.config, but convenient for ad-hoc browsing.\n",
    "import os\n",
    "\n",
    "LOCAL_DATA_DIR = f\"{PROJECT_ROOT}/data\"\n",
    "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
    "\n",
    "dataset_link = os.path.join(LOCAL_DATA_DIR, \"auc.distracted.driver.dataset_v2\")\n",
    "if not os.path.islink(dataset_link) and not os.path.exists(dataset_link):\n",
    "    try:\n",
    "        os.symlink(DATASET_ROOT, dataset_link)\n",
    "        print(f\"üîó Symlinked {dataset_link} ‚Üí {DATASET_ROOT}\")\n",
    "    except OSError as e:\n",
    "        print(f\"‚ÑπÔ∏è Symlink skipped or failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Dataset link already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf8bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîç 7) Quick sanity checks\n",
    "import os, glob\n",
    "\n",
    "def preview_dir(path, n=10):\n",
    "    print(f\"Listing up to {n} items under: {path}\")\n",
    "    try:\n",
    "        for i, name in enumerate(sorted(os.listdir(path))):\n",
    "            print(\"  -\", name)\n",
    "            if i+1 >= n:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(\"Could not list:\", e)\n",
    "\n",
    "print(\"\\nTop-level DATASET_ROOT:\")\n",
    "preview_dir(os.environ[\"DATASET_ROOT\"], n=10)\n",
    "\n",
    "cam1_train = os.path.join(os.environ[\"DATASET_ROOT\"], \"v2_cam1_cam2_ split_by_driver\", \"Camera 1\", \"train\")\n",
    "print(\"\\nCamera 1/train class folders (first 10):\")\n",
    "preview_dir(cam1_train, n=10)\n",
    "\n",
    "for cls in [\"c0\",\"c1\",\"c2\"]:\n",
    "    cls_dir = os.path.join(cam1_train, cls)\n",
    "    if os.path.isdir(cls_dir):\n",
    "        num_imgs = len([p for p in glob.glob(os.path.join(cls_dir, \"*\")) if os.path.isfile(p)])\n",
    "        print(f\"  ‚Ä¢ {cls}: {num_imgs} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ 8) Import smoke test (uses your package + config.py)\n",
    "import sys, os\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "sys.path.append(os.path.join(PROJECT_ROOT, \"src\"))  # <‚Äî lets Python find src/ddriver\n",
    "\n",
    "try:\n",
    "    import ddriver\n",
    "    print(\"ddriver imported OK from:\", ddriver.__file__)\n",
    "    # Confirm config picks up Colab env:\n",
    "    try:\n",
    "        from ddriver import config\n",
    "        print(\"Loaded ddriver.config successfully.\")\n",
    "        # Echo the resolved paths from config (they are pathlib.Path objects)\n",
    "        print(\"config.DATASET_ROOT =\", config.DATASET_ROOT)\n",
    "        print(\"config.OUT_ROOT     =\", config.OUT_ROOT)\n",
    "        print(\"config.CKPT_ROOT    =\", config.CKPT_ROOT)\n",
    "        print(\"config.FAST_DATA    =\", config.FAST_DATA)\n",
    "    except Exception as e:\n",
    "        print(\"Note: ddriver.config not imported:\", e)\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Import failed ‚Äî check package name/setup.\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374139f5",
   "metadata": {},
   "source": [
    "# üìã 9) Generate Manifest and Split CSVs\n",
    "\n",
    "This step creates the CSV files that tell your code where all the images are and which ones go to train/val/test.\n",
    "\n",
    "**What this does:**\n",
    "- Scans all your images in the dataset folder\n",
    "- Creates a big list (manifest.csv) with info about every image\n",
    "- Creates three smaller lists (train.csv, val.csv, test.csv) that say which images belong where\n",
    "- Saves everything to your Google Drive so it's permanent\n",
    "\n",
    "**Why we need this:**\n",
    "- Your training code needs to know which images to use\n",
    "- The manifest remembers which driver each image belongs to (for VAL split)\n",
    "- The split CSVs organize images into train/val/test groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8883386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the manifest generator\n",
    "# This is like asking a librarian to catalog all your books and create reading lists\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Make sure we can import ddriver\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Run the manifest script\n",
    "# --write-split-lists means \"also create train.csv, val.csv, test.csv files\"\n",
    "manifest_cmd = f\"cd {PROJECT_ROOT} && python -m ddriver.data.manifest --write-split-lists\"\n",
    "\n",
    "print(\"üî® Generating manifest and split CSVs...\")\n",
    "print(f\"Running: {manifest_cmd}\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    manifest_cmd,\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Show what happened\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Warnings/Errors:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n‚úÖ Manifest and split CSVs generated successfully!\")\n",
    "    print(f\"   Manifest: {os.environ['OUT_ROOT']}/manifests/manifest.csv\")\n",
    "    print(f\"   Train split: {os.environ['OUT_ROOT']}/splits/train.csv\")\n",
    "    print(f\"   Val split: {os.environ['OUT_ROOT']}/splits/val.csv\")\n",
    "    print(f\"   Test split: {os.environ['OUT_ROOT']}/splits/test.csv\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Error generating manifest (exit code {result.returncode})\")\n",
    "    raise RuntimeError(\"Manifest generation failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: Did the CSVs get created?\n",
    "# This is like checking that the librarian actually wrote down all the book lists\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "manifest_path = Path(os.environ['OUT_ROOT']) / \"manifests\" / \"manifest.csv\"\n",
    "train_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"train.csv\"\n",
    "val_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"val.csv\"\n",
    "test_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"test.csv\"\n",
    "\n",
    "print(\"üìä Checking CSV files...\\n\")\n",
    "\n",
    "for name, path in [(\"Manifest\", manifest_path), (\"Train\", train_path), (\"Val\", val_path), (\"Test\", test_path)]:\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"‚úÖ {name}: {len(df)} rows, columns: {list(df.columns)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name}: File not found at {path}\")\n",
    "\n",
    "# Show a sample from the manifest\n",
    "if manifest_path.exists():\n",
    "    print(\"\\nüìÑ Sample from manifest (first 3 rows):\")\n",
    "    sample = pd.read_csv(manifest_path).head(3)\n",
    "    print(sample[['path', 'class_id', 'driver_id', 'camera', 'split']].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc2b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny balanced subset for quick testing\n",
    "# Run this cell ONCE to create train_small.csv, then use it for fast experiments\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from ddriver import config\n",
    "\n",
    "train_csv = Path(config.OUT_ROOT) / \"splits\" / \"train.csv\"\n",
    "train_small_csv = Path(config.OUT_ROOT) / \"splits\" / \"train_small.csv\"\n",
    "\n",
    "print(f\"Reading {train_csv}...\")\n",
    "df = pd.read_csv(train_csv)\n",
    "\n",
    "# Get 20 images per class (balanced)\n",
    "small = df.groupby(\"class_id\").head(20)\n",
    "\n",
    "print(f\"Original train.csv: {len(df)} images\")\n",
    "print(f\"Small subset: {len(small)} images ({len(small) // 10} per class)\")\n",
    "print(f\"\\nClass distribution in small subset:\")\n",
    "print(small[\"class_id\"].value_counts().sort_index())\n",
    "\n",
    "small.to_csv(train_small_csv, index=False)\n",
    "print(f\"\\n‚úÖ Saved to {train_small_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c222e9",
   "metadata": {},
   "source": [
    "### ‚ö°Ô∏è Tiny-train option\n",
    "\n",
    "Set `USE_TINY_SPLIT = True` in the training cell below to replace the heavy\n",
    "`train.csv` with the quick `train_small.csv` (20 images per class). Validation\n",
    "and test splits stay full so you still see realistic metrics.\n",
    "\n",
    "Run the \"Create a tiny balanced subset\" cell once per Drive setup before\n",
    "enabling this flag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041552b4",
   "metadata": {},
   "source": [
    "# üß™ 10) Test dataset.py and datamod.py\n",
    "\n",
    "Now let's make sure the code that loads images actually works!\n",
    "\n",
    "**What we're testing:**\n",
    "1. **dataset.py** - Can it load a single image and give us the right info?\n",
    "2. **datamod.py** - Can it create data loaders that give us batches of images?\n",
    "\n",
    "**Why test this:**\n",
    "- If these don't work, training will fail\n",
    "- Better to catch problems now than later\n",
    "- We want to see that images load correctly and labels are right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6044f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Can dataset.py load a single image?\n",
    "# This is like testing if a worker can fetch one book from the library\n",
    "\n",
    "from ddriver.data.dataset import AucDriverDataset\n",
    "from torchvision import transforms as T\n",
    "from pathlib import Path\n",
    "\n",
    "# Get paths from config\n",
    "manifest_csv = Path(os.environ['OUT_ROOT']) / \"manifests\" / \"manifest.csv\"\n",
    "val_split_csv = Path(os.environ['OUT_ROOT']) / \"splits\" / \"val.csv\"\n",
    "\n",
    "print(\"üß™ Test 1: Testing AucDriverDataset (dataset.py)\")\n",
    "print(f\"   Manifest: {manifest_csv}\")\n",
    "print(f\"   Using Val split: {val_split_csv}\\n\")\n",
    "\n",
    "try:\n",
    "    # Create a simple dataset (no fancy transforms, just load the image)\n",
    "    simple_transforms = T.ToTensor()  # Just convert to tensor, no augmentation\n",
    "    \n",
    "    val_dataset = AucDriverDataset(\n",
    "        manifest_csv=manifest_csv,\n",
    "        split_csv=val_split_csv,\n",
    "        transforms=simple_transforms\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset created! It has {len(val_dataset)} images in VAL split\")\n",
    "    \n",
    "    # Try to load the first image\n",
    "    print(\"\\nüìñ Loading first image from VAL split...\")\n",
    "    sample = val_dataset[0]\n",
    "    \n",
    "    print(f\"‚úÖ Image loaded successfully!\")\n",
    "    print(f\"   Image shape: {sample['image'].shape} (should be [3, height, width])\")\n",
    "    print(f\"   Label: {sample['label']} (should be 0-9)\")\n",
    "    print(f\"   Driver ID: {sample['driver_id']} (VAL should have driver IDs)\")\n",
    "    print(f\"   Camera: {sample['camera']} (should be 'cam1' or 'cam2')\")\n",
    "    print(f\"   Path: {sample['path'][:80]}...\")  # Show first 80 chars\n",
    "    \n",
    "    # Check that label is valid (0-9)\n",
    "    if 0 <= sample['label'] <= 9:\n",
    "        print(f\"   ‚úÖ Label is valid (0-9)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Label {sample['label']} is NOT in range 0-9!\")\n",
    "    \n",
    "    # Check that VAL has driver IDs\n",
    "    if sample['driver_id'] is not None:\n",
    "        print(f\"   ‚úÖ VAL split has driver ID (as expected)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  VAL split missing driver ID (might be okay if this image wasn't in your DRIVER_RANGES)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Test 1 PASSED: dataset.py works!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 1 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286fdbe",
   "metadata": {},
   "source": [
    "# üßµ 11) Full pipeline (train ‚Üí predict ‚Üí metrics)\n",
    "\n",
    "Now that data loading is working, these next cells show how to:\n",
    "1. Register the model you want (e.g., `resnet18` from timm)\n",
    "2. Run training from the command line helper\n",
    "3. Generate predictions from a checkpoint\n",
    "4. Evaluate metrics and save all logs to Drive\n",
    "\n",
    "> You can change the `RUN_TAG`, model name, epochs, etc. in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register models you want to use (run once per runtime)\n",
    "# This example uses timm's resnet18.\n",
    "\n",
    "!pip -q install timm\n",
    "\n",
    "from ddriver.models import registry\n",
    "\n",
    "registry.register_timm_backbone(\"resnet18\")\n",
    "print(\"Available models:\", registry.available_models()[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e011f7",
   "metadata": {},
   "source": [
    "## üöÇ 11.1 Train a model (adjust these knobs)\n",
    "\n",
    "- Choose a `RUN_TAG` so logs/checkpoints go into `TFM/checkpoints/runs/<tag>/...`\n",
    "- Set epochs/batch size to something small for a dry run (1 epoch, 16 batch)\n",
    "- This command uses the CLI helper (`python -m src.ddriver.cli.train ...`)\n",
    "- Logs + checkpoints are saved automatically to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, textwrap, json, time, threading\n",
    "from pathlib import Path\n",
    "\n",
    "# ResNet-18 baseline run (change RUN_TAG for each experiment)\n",
    "RUN_TAG = \"resnet18_full_v1\"     # change me for each experiment\n",
    "MODEL_NAME = \"resnet18\"          # must be registered above (timm)\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCHS = 15                      # for real runs; use 1‚Äì2 for quick smoke tests\n",
    "BATCH_SIZE = 32                  # try 64 later if VRAM allows\n",
    "NUM_WORKERS = 2                  # how many background data-loading workers\n",
    "IMAGE_SIZE = 224\n",
    "LR = 1e-3                        # good starting LR for fine-tuning ResNet-18\n",
    "LR_DROP_EPOCH = None             # drop LR after this epoch (1-based)\n",
    "LR_DROP_FACTOR = 0.1             # multiply LR by this factor after drop\n",
    "USE_TINY_SPLIT = False           # True = train_small.csv (debug); False = full train.csv\n",
    "\n",
    "manifest_csv = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "train_split = \"train_small.csv\" if USE_TINY_SPLIT else \"train.csv\"\n",
    "train_csv = Path(OUT_ROOT) / \"splits\" / train_split\n",
    "val_csv = Path(OUT_ROOT) / \"splits\" / \"val.csv\"\n",
    "test_csv = Path(OUT_ROOT) / \"splits\" / \"test.csv\"\n",
    "\n",
    "if USE_TINY_SPLIT:\n",
    "    print(\"‚ö° Using train_small.csv (20 imgs/class) for a quick smoke test.\")\n",
    "else:\n",
    "    print(\"ü™µ Using full train.csv for a proper run.\")\n",
    "\n",
    "train_cmd = textwrap.dedent(f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.cli.train \\\n",
    "    --model-name {MODEL_NAME} \\\n",
    "    --epochs {EPOCHS} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --num-workers {NUM_WORKERS} \\\n",
    "    --image-size {IMAGE_SIZE} \\\n",
    "    --lr {LR} \\\n",
    "    --label-smoothing 0.05 \\\n",
    "    --out-tag {RUN_TAG} \\\n",
    "    --manifest-csv {manifest_csv} \\\n",
    "    --train-csv {train_csv} \\\n",
    "    --val-csv {val_csv} \\\n",
    "    --test-csv {test_csv}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Running training command and streaming logs:\\n\", train_cmd)\n",
    "\n",
    "proc = subprocess.Popen(\n",
    "    train_cmd,\n",
    "    shell=True,\n",
    "    text=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "\n",
    "# Background GPU monitor (prints every 5 seconds)\n",
    "def _gpu_monitor():\n",
    "    while proc.poll() is None:\n",
    "        try:\n",
    "            stats = subprocess.check_output(\n",
    "                \"nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total \"\n",
    "                \"--format=csv,nounits,noheader\",\n",
    "                shell=True,\n",
    "            ).decode(\"utf-8\").strip()\n",
    "            print(f\"[GPU] util%, mem_used, mem_total :: {stats}\")\n",
    "        except Exception as exc:\n",
    "            print(\"[GPU] Could not query nvidia-smi:\", exc)\n",
    "        time.sleep(5)\n",
    "\n",
    "monitor_thread = threading.Thread(target=_gpu_monitor, daemon=True)\n",
    "monitor_thread.start()\n",
    "\n",
    "# Stream CLI stdout live\n",
    "if proc.stdout is None:\n",
    "    raise RuntimeError(\"Training process has no stdout pipe.\")\n",
    "\n",
    "for line in proc.stdout:\n",
    "    print(line, end=\"\")\n",
    "\n",
    "proc.wait()\n",
    "monitor_thread.join(timeout=0)\n",
    "\n",
    "print(\"\\n‚úÖ Training run complete!\\n\")\n",
    "\n",
    "# --- Display every epoch's metrics so the notebook shows the learning curve ---\n",
    "run_base = Path(CKPT_ROOT) / \"runs\" / RUN_TAG\n",
    "all_runs = sorted(run_base.glob(\"*/\"))\n",
    "if not all_runs:\n",
    "    raise FileNotFoundError(f\"No run folders found under {run_base}\")\n",
    "latest_run = all_runs[-1]\n",
    "\n",
    "history_path = latest_run / \"history.json\"\n",
    "if not history_path.exists():\n",
    "    raise FileNotFoundError(f\"history.json not found in {latest_run}\")\n",
    "\n",
    "history = json.loads(history_path.read_text()).get(\"history\", [])\n",
    "print(f\"üìä Epoch metrics for run: {latest_run.name}\")\n",
    "for record in history:\n",
    "    train_metrics = record.get(\"train\", {})\n",
    "    val_metrics = record.get(\"val\", {}) or {}\n",
    "    train_loss = train_metrics.get(\"loss\")\n",
    "    train_acc = train_metrics.get(\"accuracy\")\n",
    "    val_loss = val_metrics.get(\"loss\")\n",
    "    val_acc = val_metrics.get(\"accuracy\")\n",
    "    val_str = (\n",
    "        f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\"\n",
    "        if val_loss is not None and val_acc is not None\n",
    "        else \"val_loss=‚Äî val_acc=‚Äî\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Epoch {record['epoch']:>2}: \"\n",
    "        f\"train_loss={train_loss:.4f} acc={train_acc:.4f}  \"\n",
    "        f\"{val_str}\"\n",
    "    )\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b93ef9",
   "metadata": {},
   "source": [
    "## üìù 11.1a Log training summary to Google Sheet\n",
    "Run this right after the training cell finishes. It looks up the newest run under `CKPT_ROOT/runs/<RUN_TAG>`, grabs the best/final train + val accuracies, and logs the model/hyperparams so you can compare experiments before doing predictions or metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Training summary ‚Üí Google Sheet\n",
    "!pip -q install gspread\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import gspread\n",
    "from google.colab import auth\n",
    "import google.auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "creds, _ = google.auth.default()\n",
    "gc = gspread.authorize(creds)\n",
    "\n",
    "TRAIN_SHEET_NAME = \"TFM Train Logs\"   # create this sheet/tab ahead of time\n",
    "TRAIN_WORKSHEET = \"Sheet1\"\n",
    "\n",
    "run_base = Path(CKPT_ROOT) / \"runs\" / RUN_TAG\n",
    "all_runs = sorted(run_base.glob(\"*/\"))\n",
    "if not all_runs:\n",
    "    raise FileNotFoundError(f\"No run folders found under {run_base}\")\n",
    "latest_run = all_runs[-1]\n",
    "print(f\"Logging training summary for run folder: {latest_run}\")\n",
    "\n",
    "history_path = latest_run / \"history.json\"\n",
    "if not history_path.exists():\n",
    "    raise FileNotFoundError(f\"history.json not found under {latest_run}\")\n",
    "\n",
    "history_records = json.loads(history_path.read_text()).get(\"history\", [])\n",
    "if not history_records:\n",
    "    raise ValueError(f\"history.json under {latest_run} has no records.\")\n",
    "\n",
    "params_path = latest_run / \"params.json\"\n",
    "params = json.loads(params_path.read_text()) if params_path.exists() else {}\n",
    "\n",
    "model_name = params.get(\"model_name\", MODEL_NAME)\n",
    "epochs_cfg = params.get(\"epochs\", EPOCHS)\n",
    "batch_cfg = params.get(\"batch_size\", BATCH_SIZE)\n",
    "lr_cfg = params.get(\"lr\", LR)\n",
    "lr_drop_epoch_cfg = params.get(\"lr_drop_epoch\", LR_DROP_EPOCH)\n",
    "lr_drop_factor_cfg = params.get(\"lr_drop_factor\", LR_DROP_FACTOR)\n",
    "image_size_cfg = params.get(\"image_size\", IMAGE_SIZE)\n",
    "num_workers_cfg = params.get(\"num_workers\", NUM_WORKERS)\n",
    "use_tiny_cfg = params.get(\"use_tiny_split\", USE_TINY_SPLIT)\n",
    "\n",
    "\n",
    "def _best_metric(records, split: str) -> tuple[dict, float | None]:\n",
    "    best_epoch = None\n",
    "    best_acc = None\n",
    "    for rec in records:\n",
    "        split_metrics = rec.get(split) or {}\n",
    "        acc = split_metrics.get(\"accuracy\")\n",
    "        if acc is None:\n",
    "            continue\n",
    "        if best_acc is None or acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_epoch = rec.get(\"epoch\")\n",
    "    final_metrics = records[-1].get(split) or {}\n",
    "    final_acc = final_metrics.get(\"accuracy\")\n",
    "    return {\"epoch\": best_epoch, \"accuracy\": best_acc}, final_acc\n",
    "\n",
    "\n",
    "best_train, final_train = _best_metric(history_records, \"train\")\n",
    "best_val, final_val = _best_metric(history_records, \"val\")\n",
    "\n",
    "row = [\n",
    "    RUN_TAG,\n",
    "    latest_run.name,\n",
    "    model_name,\n",
    "    epochs_cfg,\n",
    "    batch_cfg,\n",
    "    lr_cfg,\n",
    "    lr_drop_epoch_cfg,\n",
    "    lr_drop_factor_cfg,\n",
    "    image_size_cfg,\n",
    "    num_workers_cfg,\n",
    "    use_tiny_cfg,\n",
    "    best_train[\"epoch\"] if best_train[\"epoch\"] is not None else \"\",\n",
    "    round(best_train[\"accuracy\"], 4) if best_train[\"accuracy\"] is not None else \"\",\n",
    "    best_val[\"epoch\"] if best_val[\"epoch\"] is not None else \"\",\n",
    "    round(best_val[\"accuracy\"], 4) if best_val[\"accuracy\"] is not None else \"\",\n",
    "    round(final_train, 4) if final_train is not None else \"\",\n",
    "    round(final_val, 4) if final_val is not None else \"\",\n",
    "]\n",
    "\n",
    "ws = gc.open(TRAIN_SHEET_NAME).worksheet(TRAIN_WORKSHEET)\n",
    "ws.append_row(row, value_input_option=\"USER_ENTERED\")\n",
    "print(f\"Appended training summary for {latest_run.name} ‚úÖ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c73e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Optional: copy + compress dataset subset ‚Üí fast local SSD (/content/data)\n",
    "# Re-encodes JPEGs once (quality 80, short side 320px) before landing in /content/data.\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from ddriver.data.fastcopy import CompressionSpec, copy_splits_with_compression\n",
    "\n",
    "SRC_ROOT = Path(DRIVE_DATA_ROOT) / \"auc.distracted.driver.dataset_v2\"\n",
    "DST_ROOT = Path(FAST_DATA) / \"auc.distracted.driver.dataset_v2\"\n",
    "\n",
    "split_csvs = {\n",
    "    \"train\": Path(OUT_ROOT) / \"splits\" / \"train.csv\",\n",
    "    \"val\": Path(OUT_ROOT) / \"splits\" / \"val.csv\",\n",
    "    \"train_small\": Path(OUT_ROOT) / \"splits\" / \"train_small.csv\",\n",
    "}\n",
    "\n",
    "compression_spec = CompressionSpec(\n",
    "    target_short_side=320,  # still >= image_size + resize margin for training\n",
    "    jpeg_quality=80,        # ImageNet-level compression, visually lossless\n",
    ")\n",
    "\n",
    "summary = copy_splits_with_compression(\n",
    "    split_csvs=split_csvs,\n",
    "    src_root=SRC_ROOT,\n",
    "    dst_root=DST_ROOT,\n",
    "    compression=compression_spec,\n",
    "    skip_existing=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nüìâ FAST_DATA copy stats: processed {summary['processed']} of {summary['total']} files \"\n",
    "    f\"(skipped {summary['skipped']} already present).\"\n",
    ")\n",
    "print(f\"Compressed dataset root: {summary['dst_root']}\")\n",
    "\n",
    "DATASET_ROOT = FAST_DATA\n",
    "os.environ[\"DATASET_ROOT\"] = str(DATASET_ROOT)\n",
    "try:\n",
    "    from ddriver import config as _ddriver_config\n",
    "    importlib.reload(_ddriver_config)\n",
    "    print(\"\\n‚ö° Copy complete. DATASET_ROOT now points to the local FAST_DATA copy for this runtime:\")\n",
    "    print(\"   ddriver.config.DATASET_ROOT =\", _ddriver_config.DATASET_ROOT)\n",
    "except Exception as exc:\n",
    "    print(\"\\n‚ö° Copy complete and DATASET_ROOT env updated, but could not reload ddriver.config:\", exc)\n",
    "print(\"   (Re-run env summary if you want to rewrite .env, but training now uses /content/data.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb9779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Optional: copy + compress TEST split ‚Üí /content/data (same settings as train/val)\n",
    "\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "from ddriver.data.fastcopy import CompressionSpec, copy_splits_with_compression\n",
    "\n",
    "SRC_ROOT = Path(DRIVE_DATA_ROOT) / \"auc.distracted.driver.dataset_v2\"\n",
    "DST_ROOT = Path(FAST_DATA) / \"auc.distracted.driver.dataset_v2\"\n",
    "\n",
    "split_csvs = {\n",
    "    \"test\": Path(OUT_ROOT) / \"splits\" / \"test.csv\",\n",
    "}\n",
    "\n",
    "compression_spec = CompressionSpec(\n",
    "    target_short_side=320,\n",
    "    jpeg_quality=80,\n",
    ")\n",
    "\n",
    "summary = copy_splits_with_compression(\n",
    "    split_csvs=split_csvs,\n",
    "    src_root=SRC_ROOT,\n",
    "    dst_root=DST_ROOT,\n",
    "    compression=compression_spec,\n",
    "    skip_existing=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nüìâ FAST_DATA test copy stats: processed {summary['processed']} of {summary['total']} \"\n",
    "    f\"(skipped {summary['skipped']} already present).\"\n",
    ")\n",
    "print(f\"Compressed dataset root: {summary['dst_root']}\")\n",
    "\n",
    "# DATASET_ROOT is already pointing at FAST_DATA from the earlier cell, but reload config just in case\n",
    "try:\n",
    "    from ddriver import config as _ddriver_config\n",
    "    importlib.reload(_ddriver_config)\n",
    "    print(\"\\n‚ö° Test copy complete. ddriver.config now sees:\")\n",
    "    print(\"   ddriver.config.DATASET_ROOT =\", _ddriver_config.DATASET_ROOT)\n",
    "except Exception as exc:\n",
    "    print(\"\\n‚ö° Test copy complete; config reload optional:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f61ab",
   "metadata": {},
   "source": [
    "## üëÄ 11.1b Optional: sanity-check a few images\n",
    "Run this right after the copy+compress cell to view originals from Drive next to their compressed FAST_DATA twins. You can change `NUM_SAMPLES` or switch which split to inspect if you want more spot checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce48c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëÄ Visual sanity check: Drive vs FAST_DATA\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "NUM_SAMPLES = 3          # how many images to compare\n",
    "SPLIT_FOR_CHECK = \"val\"  # choose 'train', 'val', or 'train_small'\n",
    "\n",
    "split_csv = Path(OUT_ROOT) / \"splits\" / f\"{SPLIT_FOR_CHECK}.csv\"\n",
    "if not split_csv.exists():\n",
    "    raise FileNotFoundError(f\"Split CSV not found: {split_csv}. Run manifest generation first.\")\n",
    "\n",
    "df = pd.read_csv(split_csv)\n",
    "if df.empty:\n",
    "    raise ValueError(f\"No rows in {split_csv}; cannot sample images.\")\n",
    "\n",
    "marker = \"auc.distracted.driver.dataset_v2\"\n",
    "marker_lower = marker.lower()\n",
    "\n",
    "def _relative_path(path_str: str) -> Path:\n",
    "    path_str = str(path_str)\n",
    "    path = Path(path_str)\n",
    "    if path.is_absolute():\n",
    "        lowered = path_str.lower()\n",
    "        idx = lowered.find(marker_lower)\n",
    "        if idx == -1:\n",
    "            raise ValueError(f\"Could not locate dataset marker '{marker}' inside: {path_str}\")\n",
    "        rel = Path(path_str[idx:])\n",
    "    else:\n",
    "        rel = path\n",
    "    parts = rel.parts\n",
    "    if parts and parts[0].lower() == marker_lower:\n",
    "        rel = Path(*parts[1:])\n",
    "    return rel\n",
    "\n",
    "sample_paths = df[\"path\"].sample(\n",
    "    n=min(NUM_SAMPLES, len(df)),\n",
    "    replace=False,\n",
    "    random_state=42,\n",
    ").tolist()\n",
    "\n",
    "for idx, sample_path in enumerate(sample_paths, 1):\n",
    "    rel = _relative_path(sample_path)\n",
    "    drive_img_path = Path(DRIVE_DATA_ROOT) / rel\n",
    "    fast_img_path = Path(FAST_DATA) / rel\n",
    "\n",
    "    if not drive_img_path.exists():\n",
    "        raise FileNotFoundError(f\"Drive image missing: {drive_img_path}\")\n",
    "    if not fast_img_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"FAST_DATA image missing: {fast_img_path}. Run the copy+compress cell first.\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\nSample {idx}: {rel}\")\n",
    "    print(\"Drive (original):\")\n",
    "    with Image.open(drive_img_path) as orig:\n",
    "        display(orig.copy())\n",
    "    print(\"FAST_DATA (compressed):\")\n",
    "    with Image.open(fast_img_path) as comp:\n",
    "        display(comp.copy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c429a0e",
   "metadata": {},
   "source": [
    "## üì¶ 11.2 Pick the latest checkpoint file\n",
    "\n",
    "This cell looks inside `CKPT_ROOT/runs/<RUN_TAG>/` and grabs the newest `epoch_*.pt`. Use this path in the prediction step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RUN_TAG = globals().get(\"RUN_TAG\", \"resnet18_full_v1\")  # reuse your latest training tag by default\n",
    "\n",
    "run_base = Path(CKPT_ROOT) / \"runs\" / RUN_TAG\n",
    "runs = sorted(run_base.glob(\"*/\"))\n",
    "if not runs:\n",
    "    raise FileNotFoundError(f\"No run folders found under {run_base}\")\n",
    "\n",
    "# ---- choose which run folder to use ----\n",
    "RUN_IDX = -1          # -1 = newest, 0 = oldest, or any index from the printout below\n",
    "print(\"Available runs:\")\n",
    "for idx, run_dir in enumerate(runs):\n",
    "    print(f\"  [{idx}] {run_dir.name}\")\n",
    "target_run = runs[RUN_IDX]\n",
    "print(f\"\\nSelected run: {target_run}\\n\")\n",
    "\n",
    "# ---- choose which checkpoint (epoch) inside that run ----\n",
    "checkpoint_patterns = [\"epoch_*.pt\", \"best.pt\", \"last.pt\"]\n",
    "checkpoints = []\n",
    "for pattern in checkpoint_patterns:\n",
    "       matches = sorted(target_run.glob(pattern))\n",
    "       if matches:\n",
    "           checkpoints.extend(matches)\n",
    "\n",
    "if not checkpoints:\n",
    "       raise FileNotFoundError(f\"No checkpoints found under {target_run}\")\n",
    "\n",
    "CHECKPOINT_NAME = \"best.pt\"  # or \"last.pt\", or None to take the last match\n",
    "if CHECKPOINT_NAME:\n",
    "       chosen_ckpt = target_run / CHECKPOINT_NAME\n",
    "       if not chosen_ckpt.exists():\n",
    "           raise FileNotFoundError(chosen_ckpt)\n",
    "else:\n",
    "       chosen_ckpt = checkpoints[-1]\n",
    "\n",
    "LATEST_CKPT = chosen_ckpt\n",
    "print(\"Using checkpoint:\", LATEST_CKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d7e3f8",
   "metadata": {},
   "source": [
    "## üîÆ 11.3 Generate predictions CSV\n",
    "\n",
    "- Uses the checkpoint above\n",
    "- Choose which split to predict on (`val` or `test`)\n",
    "- Saves CSV under `OUT_ROOT/preds/<split>/<out_tag>.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_SPLIT = \"val\"           # or \"test\"\n",
    "PRED_TAG = f\"{RUN_TAG}_{PRED_SPLIT}\"\n",
    "\n",
    "predict_cmd = textwrap.dedent(f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.cli.predict \\\n",
    "    --model-name {MODEL_NAME} \\\n",
    "    --checkpoint {LATEST_CKPT} \\\n",
    "    --split {PRED_SPLIT} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --num-workers {NUM_WORKERS} \\\n",
    "    --image-size {IMAGE_SIZE} \\\n",
    "    --out-tag {PRED_TAG}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Running prediction command:\\n\", predict_cmd)\n",
    "result = subprocess.run(predict_cmd, shell=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"Prediction command failed. See logs above.\")\n",
    "print(\"\\n‚úÖ Predictions completed! Check OUT_ROOT/preds/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ca361",
   "metadata": {},
   "source": [
    "## üìä 11.4 Evaluate metrics\n",
    "\n",
    "- Uses `src/ddriver/metrics.py`\n",
    "- Reads the manifest + split CSV + predictions CSV\n",
    "- Saves results under `OUT_ROOT/metrics/<tag>/<timestamp>/`\n",
    "- Shows accuracy + macro F1 + per-driver/camera (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "manifest_path = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
    "split_csv_path = Path(OUT_ROOT) / \"splits\" / f\"{PRED_SPLIT}.csv\"\n",
    "preds_csv_path = Path(OUT_ROOT) / \"preds\" / PRED_SPLIT / f\"{PRED_TAG}.csv\"\n",
    "METRICS_TAG = PRED_TAG\n",
    "\n",
    "metrics_cmd = textwrap.dedent(f\"\"\"\n",
    "cd {PROJECT_ROOT}\n",
    "python -m src.ddriver.eval.metrics \\\n",
    "    --manifest {manifest_path} \\\n",
    "    --split-csv {split_csv_path} \\\n",
    "    --predictions {preds_csv_path} \\\n",
    "    --out-tag {METRICS_TAG} \\\n",
    "    --per-driver \\\n",
    "    --per-camera\n",
    "\"\"\")\n",
    "\n",
    "print(\"Running metrics command:\\n\", metrics_cmd)\n",
    "result = subprocess.run(metrics_cmd, shell=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"Metrics command failed. See logs above.\")\n",
    "print(\"\\n‚úÖ Metrics saved under OUT_ROOT/metrics/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c13864",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dec2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Can datamod.py create data loaders and load batches?\n",
    "# This is like testing if the teacher can organize students into groups and give them work\n",
    "\n",
    "from ddriver.data.datamod import build_dataloaders, make_cfg_from_config\n",
    "import torch\n",
    "\n",
    "print(\"üß™ Test 2: Testing build_dataloaders (datamod.py)\\n\")\n",
    "\n",
    "try:\n",
    "    # Create config using the helper that uses ddriver.config paths\n",
    "    # This is the easy way - it automatically finds your CSVs!\n",
    "    cfg = make_cfg_from_config(\n",
    "        batch_size=4,  # Small batch for testing (faster)\n",
    "        num_workers=2,  # Use 2 workers (Colab might have limited CPUs)\n",
    "        image_size=224,  # Standard image size\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Config created using ddriver.config paths:\")\n",
    "    print(f\"   Manifest: {cfg.manifest_csv}\")\n",
    "    print(f\"   Train: {cfg.train_split_csv}\")\n",
    "    print(f\"   Val: {cfg.val_split_csv}\")\n",
    "    print(f\"   Test: {cfg.test_split_csv}\\n\")\n",
    "    \n",
    "    # Build the data loaders\n",
    "    print(\"üî® Building data loaders...\")\n",
    "    loaders = build_dataloaders(cfg)\n",
    "    \n",
    "    print(\"‚úÖ Data loaders created!\")\n",
    "    print(f\"   Available splits: {list(loaders.keys())}\\n\")\n",
    "    \n",
    "    # Test train loader\n",
    "    print(\"üì¶ Testing TRAIN loader...\")\n",
    "    train_loader = loaders[\"train\"]\n",
    "    train_batch = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"   ‚úÖ Train batch loaded!\")\n",
    "    print(f\"   Batch size: {train_batch['image'].shape[0]} images\")\n",
    "    print(f\"   Image shape: {train_batch['image'].shape} (should be [batch_size, 3, 224, 224])\")\n",
    "    print(f\"   Labels: {train_batch['label'].tolist()} (should be list of 0-9)\")\n",
    "    print(f\"   Driver IDs: {train_batch['driver_id']} (train should mostly be None)\")\n",
    "    print(f\"   Cameras: {train_batch['camera']}\")\n",
    "    \n",
    "    # Check image shape is correct\n",
    "    expected_shape = (cfg.batch_size, 3, cfg.image_size, cfg.image_size)\n",
    "    if train_batch['image'].shape == expected_shape:\n",
    "        print(f\"   ‚úÖ Image shape is correct: {train_batch['image'].shape}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Image shape wrong! Got {train_batch['image'].shape}, expected {expected_shape}\")\n",
    "    \n",
    "    # Test val loader\n",
    "    print(\"\\nüì¶ Testing VAL loader...\")\n",
    "    val_loader = loaders[\"val\"]\n",
    "    val_batch = next(iter(val_loader))\n",
    "    \n",
    "    print(f\"   ‚úÖ Val batch loaded!\")\n",
    "    print(f\"   Batch size: {val_batch['image'].shape[0]} images\")\n",
    "    print(f\"   Image shape: {val_batch['image'].shape}\")\n",
    "    print(f\"   Labels: {val_batch['label'].tolist()}\")\n",
    "    print(f\"   Driver IDs: {val_batch['driver_id']} (VAL should have driver IDs!)\")\n",
    "    \n",
    "    # Check that VAL has driver IDs\n",
    "    val_has_ids = any(did is not None for did in val_batch['driver_id'])\n",
    "    if val_has_ids:\n",
    "        print(f\"   ‚úÖ VAL batch has driver IDs (as expected)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  VAL batch missing driver IDs (check your DRIVER_RANGES in manifest.py)\")\n",
    "    \n",
    "    # Test that images are normalized (should be in range roughly -2 to 2 after ImageNet normalization)\n",
    "    img_min, img_max = train_batch['image'].min().item(), train_batch['image'].max().item()\n",
    "    print(f\"\\n   Image value range: [{img_min:.3f}, {img_max:.3f}]\")\n",
    "    print(f\"   (Should be roughly -2 to 2 after ImageNet normalization)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Test 2 PASSED: datamod.py works! Data loaders are ready for training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 2 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1047bf",
   "metadata": {},
   "source": [
    "### ‚úÖ You're all set!\n",
    "\n",
    "**What just happened:**\n",
    "1. ‚úÖ Mounted Google Drive\n",
    "2. ‚úÖ Cloned/updated your repo\n",
    "3. ‚úÖ Installed the package\n",
    "4. ‚úÖ Set up paths (works on Colab and Mac!)\n",
    "5. ‚úÖ Generated manifest.csv and train/val/test split CSVs\n",
    "6. ‚úÖ Tested that dataset.py can load images\n",
    "7. ‚úÖ Tested that datamod.py can create data loaders\n",
    "8. ‚úÖ (Optional) Registered a model + ran training ‚Üí prediction ‚Üí metrics pipeline\n",
    "\n",
    "**Your CSVs are saved in Google Drive:**\n",
    "- `OUT_ROOT/manifests/manifest.csv` - Big list of all images\n",
    "- `OUT_ROOT/splits/train.csv` - Training images\n",
    "- `OUT_ROOT/splits/val.csv` - Validation images (with driver IDs!)\n",
    "- `OUT_ROOT/splits/test.csv` - Test images\n",
    "\n",
    "**Next steps:**\n",
    "- Adjust the training/prediction cells (epochs, batch size, tags) to run bigger experiments\n",
    "- All paths use `ddriver.config` so it works on Colab and Mac\n",
    "- Re-run **Clone/Update** cell after pushing new commits\n",
    "- Optional: copy some data into `/content/data` to use `FAST_DATA` for speed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1814de",
   "metadata": {},
   "source": [
    "\n",
    "### ‚úÖ You‚Äôre set!\n",
    "- Your repo + URL are **hardcoded**.\n",
    "- `ddriver.config` will see the Colab env vars and resolve paths there.\n",
    "- Re-run **Clone/Update** after pushing new commits.\n",
    "- Optional: copy some data into `/content/data` to use `FAST_DATA` for speed, then call `ddriver.config.dataset_dir(prefer_fast=True)` in your scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Colab cell: append metrics + params to Google Sheet ----\n",
    "!pip -q install gspread\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import gspread\n",
    "from google.colab import auth\n",
    "import google.auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "creds, _ = google.auth.default()\n",
    "gc = gspread.authorize(creds)\n",
    "\n",
    "EVAL_SHEET_NAME = \"TFM Eval Logs\"   # create this sheet/tab ahead of time\n",
    "EVAL_WORKSHEET = \"Sheet1\"\n",
    "\n",
    "METRICS_TAG = (\n",
    "    globals().get(\"METRICS_TAG\")\n",
    "    or globals().get(\"PRED_TAG\")\n",
    "    or \"resnet18_full_v1_val\"\n",
    ")  # match the --out-tag you used\n",
    "metrics_root = Path(OUT_ROOT) / \"metrics\" / METRICS_TAG\n",
    "runs = sorted(metrics_root.glob(\"*/\"))\n",
    "if not runs:\n",
    "    raise FileNotFoundError(f\"No metrics runs found under {metrics_root}\")\n",
    "latest_metrics = runs[-1]\n",
    "print(\"Logging metrics folder:\", latest_metrics)\n",
    "\n",
    "def _read_json(path: Path, *, required: bool = True) -> dict:\n",
    "    if not path.exists():\n",
    "        if required:\n",
    "            raise FileNotFoundError(f\"Expected file missing: {path}\")\n",
    "        return {}\n",
    "    return json.loads(path.read_text())\n",
    "\n",
    "metrics = _read_json(latest_metrics / \"metrics.json\")\n",
    "inputs = _read_json(latest_metrics / \"inputs.json\", required=False)\n",
    "params = _read_json(latest_metrics / \"params.json\", required=False)\n",
    "\n",
    "overall = metrics.get(\"overall\", {})\n",
    "macro = overall.get(\"macro_avg\", {})\n",
    "\n",
    "row = [\n",
    "    str(latest_metrics),\n",
    "    inputs.get(\"predictions\", \"\"),\n",
    "    inputs.get(\"split_source\", \"\"),\n",
    "    metrics.get(\"num_examples\", \"\"),\n",
    "    round(overall.get(\"accuracy\", 0.0), 4),\n",
    "    round(macro.get(\"f1\", 0.0), 4),\n",
    "    json.dumps(params, sort_keys=True)[:500],\n",
    "]\n",
    "\n",
    "ws = gc.open(EVAL_SHEET_NAME).worksheet(EVAL_WORKSHEET)\n",
    "ws.append_row(row, value_input_option=\"USER_ENTERED\")\n",
    "print(f\"Appended metrics run {latest_metrics.name} to {EVAL_SHEET_NAME}/{EVAL_WORKSHEET} ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ddb94",
   "metadata": {},
   "source": [
    "### üìä 11.4a Visualize Confusion Matrix\n",
    "\n",
    "Quick peek at where the model confuses classes using the most recent metrics run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c65d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "METRICS_TAG = (\n",
    "    globals().get(\"METRICS_TAG\")\n",
    "    or globals().get(\"PRED_TAG\")\n",
    "    or \"resnet18_full_v1_val\"\n",
    ")  # change if you used a different --out-tag\n",
    "metrics_root = Path(OUT_ROOT) / \"metrics\" / METRICS_TAG\n",
    "runs = sorted(metrics_root.glob(\"*/\"))\n",
    "if not runs:\n",
    "    raise FileNotFoundError(f\"No metrics runs found under {metrics_root}\")\n",
    "latest_metrics = runs[-1]\n",
    "print(\"Reading confusion matrix from:\", latest_metrics)\n",
    "\n",
    "metrics = json.loads((latest_metrics / \"metrics.json\").read_text())\n",
    "cm_info = metrics.get(\"confusion_matrix\")\n",
    "if not cm_info:\n",
    "    raise ValueError(\"confusion_matrix missing from metrics.json\")\n",
    "\n",
    "labels = cm_info[\"rows_cols_labels\"]\n",
    "cm_df = pd.DataFrame(cm_info[\"matrix\"], index=labels, columns=labels)\n",
    "\n",
    "counts_path = latest_metrics / \"confusion_matrix_counts.png\"\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(f\"Confusion matrix ‚Äì {METRICS_TAG}\")\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(counts_path)\n",
    "plt.show()\n",
    "print(\"Saved counts heatmap to\", counts_path)\n",
    "\n",
    "cm_norm = cm_df.div(cm_df.sum(axis=1).replace(0, 1), axis=0)\n",
    "norm_path = latest_metrics / \"confusion_matrix_normalized.png\"\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "plt.title(f\"Normalized confusion matrix ‚Äì {METRICS_TAG}\")\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(norm_path)\n",
    "plt.show()\n",
    "print(\"Saved normalized heatmap to\", norm_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
