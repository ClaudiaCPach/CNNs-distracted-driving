{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7296b001",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸš€ Colab Setup â€” **CNNs-distracted-driving** (hardcoded + config-aware)\n",
    "\n",
    "This version is **simplified and hardcoded** for your repo and URL, and it **respects your `src/ddriver/config.py`**.\n",
    "- Repo name fixed to **`CNNs-distracted-driving`**\n",
    "- Repo URL fixed to **`https://github.com/ClaudiaCPach/CNNs-distracted-driving`**\n",
    "- Uses your `config.py` convention: when running in Colab, we **set env vars** (`DRIVE_PATH`, `DATASET_ROOT`, `OUT_ROOT`, `CKPT_ROOT`, `FAST_DATA`) so your code reads correct paths via `ddriver.config`.\n",
    "- Optional `FAST_DATA` at `/content/data` for faster I/O (if you later copy data there).\n",
    "\n",
    "> Run cells **top â†’ bottom** the first time. Re-run **Update repo** to pull new commits after you push.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0fd729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ”§ 0) (Optional) quick GPU check\n",
    "!nvidia-smi || echo \"No GPU detected â€” CPU runtime is okay for setup steps.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ”§ 1) Fixed config for your repo + Drive layout\n",
    "REPO_URL       = \"https://github.com/ClaudiaCPach/CNNs-distracted-driving\"\n",
    "REPO_DIRNAME   = \"CNNs-distracted-driving\"   # hardcoded\n",
    "BRANCH         = \"main\"\n",
    "PROJECT_ROOT   = f\"/content/{REPO_DIRNAME}\"  # where the repo will live in Colab\n",
    "\n",
    "# Your persistent Google Drive base folder (matches your project docs):\n",
    "DRIVE_PATH     = \"/content/drive/MyDrive/TFM\"\n",
    "\n",
    "# Your dataset lives under TFM/data/auc.distracted.driver.dataset_v2 (as per your structure)\n",
    "DATASET_ROOT   = f\"{DRIVE_PATH}/data/auc.distracted.driver.dataset_v2\"\n",
    "OUT_ROOT       = f\"{DRIVE_PATH}/outputs\"\n",
    "CKPT_ROOT      = f\"{DRIVE_PATH}/checkpoints\"\n",
    "\n",
    "# Optional: a fast, ephemeral workspace inside the VM\n",
    "FAST_DATA      = \"/content/data\"   # leave as-is; you can rsync into this later for speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ”Œ 2) Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "print(\"âœ… Drive mounted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df094a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“ 3) Clone or update the repo (no name inference â€” all hardcoded)\n",
    "import os, subprocess\n",
    "\n",
    "def sh(cmd):\n",
    "    print(f\"\\n$ {cmd}\")\n",
    "    rc = subprocess.call(cmd, shell=True, executable=\"/bin/bash\")\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {rc}: {cmd}\")\n",
    "\n",
    "if os.path.isdir(PROJECT_ROOT):\n",
    "    print(f\"ðŸ“ Repo already present at {PROJECT_ROOT}. Pulling latest on branch {BRANCH}...\")\n",
    "    sh(f\"cd {PROJECT_ROOT} && git fetch origin {BRANCH} && git checkout {BRANCH} && git pull --rebase origin {BRANCH}\")\n",
    "else:\n",
    "    print(f\"â¬‡ï¸ Cloning {REPO_URL} â†’ {PROJECT_ROOT}\")\n",
    "    sh(f\"git clone --branch {BRANCH} {REPO_URL} {PROJECT_ROOT}\")\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“¦ 4) Install the repo (editable) + requirements (uses pyproject.toml if present)\n",
    "import os, subprocess\n",
    "\n",
    "def sh(cmd):\n",
    "    print(f\"\\n$ {cmd}\")\n",
    "    rc = subprocess.call(cmd, shell=True, executable=\"/bin/bash\")\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {rc}: {cmd}\")\n",
    "\n",
    "print(\"ðŸ”„ Upgrading pip/setuptools/wheel...\")\n",
    "sh(\"python -m pip install --upgrade pip setuptools wheel\")\n",
    "\n",
    "has_pyproject = os.path.exists(os.path.join(PROJECT_ROOT, \"pyproject.toml\"))\n",
    "if has_pyproject:\n",
    "    print(\"ðŸ“¦ Editable install from pyproject.toml ...\")\n",
    "    sh(f\"cd {PROJECT_ROOT} && pip install -e .\")\n",
    "else:\n",
    "    print(\"âš ï¸ No pyproject.toml found. Skipping editable install.\")\n",
    "\n",
    "req_path = os.path.join(PROJECT_ROOT, \"requirements.txt\")\n",
    "if os.path.exists(req_path):\n",
    "    print(\"ðŸ“ Installing requirements.txt...\")\n",
    "    sh(f\"pip install -r {req_path}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ No requirements.txt found â€” continuing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accfc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸŒ³ 5) Configure environment for your ddriver.config (Colab branch)\n",
    "# Your config.py reads env vars and falls back to sensible defaults when in Colab.\n",
    "import os\n",
    "\n",
    "os.environ[\"DRIVE_PATH\"]   = DRIVE_PATH\n",
    "os.environ[\"DATASET_ROOT\"] = DATASET_ROOT\n",
    "os.environ[\"OUT_ROOT\"]     = OUT_ROOT\n",
    "os.environ[\"CKPT_ROOT\"]    = CKPT_ROOT\n",
    "os.environ[\"FAST_DATA\"]    = FAST_DATA\n",
    "\n",
    "# Also write a .env (harmless in Colab; helpful if code calls load_dotenv())\n",
    "env_text = f\"\"\"DRIVE_PATH={DRIVE_PATH}\n",
    "DATASET_ROOT={DATASET_ROOT}\n",
    "OUT_ROOT={OUT_ROOT}\n",
    "CKPT_ROOT={CKPT_ROOT}\n",
    "FAST_DATA={FAST_DATA}\n",
    "\"\"\"\n",
    "with open(os.path.join(PROJECT_ROOT, \".env\"), \"w\") as f:\n",
    "    f.write(env_text)\n",
    "\n",
    "print(\"âœ… Environment variables set for ddriver.config\")\n",
    "print(\"\\nSummary:\")\n",
    "for k in [\"DRIVE_PATH\",\"DATASET_ROOT\",\"OUT_ROOT\",\"CKPT_ROOT\",\"FAST_DATA\"]:\n",
    "    print(f\"{k} = {os.environ[k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27df24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ”— 6) (Optional) Symlink dataset into repo for familiar paths (scripts that assume PROJECT_ROOT/data/...)\n",
    "# Not required when using ddriver.config, but convenient for ad-hoc browsing.\n",
    "import os\n",
    "\n",
    "LOCAL_DATA_DIR = f\"{PROJECT_ROOT}/data\"\n",
    "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
    "\n",
    "dataset_link = os.path.join(LOCAL_DATA_DIR, \"auc.distracted.driver.dataset_v2\")\n",
    "if not os.path.islink(dataset_link) and not os.path.exists(dataset_link):\n",
    "    try:\n",
    "        os.symlink(DATASET_ROOT, dataset_link)\n",
    "        print(f\"ðŸ”— Symlinked {dataset_link} â†’ {DATASET_ROOT}\")\n",
    "    except OSError as e:\n",
    "        print(f\"â„¹ï¸ Symlink skipped or failed: {e}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Dataset link already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf8bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ” 7) Quick sanity checks\n",
    "import os, glob\n",
    "\n",
    "def preview_dir(path, n=10):\n",
    "    print(f\"Listing up to {n} items under: {path}\")\n",
    "    try:\n",
    "        for i, name in enumerate(sorted(os.listdir(path))):\n",
    "            print(\"  -\", name)\n",
    "            if i+1 >= n:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(\"Could not list:\", e)\n",
    "\n",
    "print(\"\\nTop-level DATASET_ROOT:\")\n",
    "preview_dir(os.environ[\"DATASET_ROOT\"], n=10)\n",
    "\n",
    "cam1_train = os.path.join(os.environ[\"DATASET_ROOT\"], \"v2_cam1_cam2_ split_by_driver\", \"Camera 1\", \"train\")\n",
    "print(\"\\nCamera 1/train class folders (first 10):\")\n",
    "preview_dir(cam1_train, n=10)\n",
    "\n",
    "for cls in [\"c0\",\"c1\",\"c2\"]:\n",
    "    cls_dir = os.path.join(cam1_train, cls)\n",
    "    if os.path.isdir(cls_dir):\n",
    "        num_imgs = len([p for p in glob.glob(os.path.join(cls_dir, \"*\")) if os.path.isfile(p)])\n",
    "        print(f\"  â€¢ {cls}: {num_imgs} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… 8) Import smoke test (uses your package + config.py)\n",
    "import sys, os\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "sys.path.append(os.path.join(PROJECT_ROOT, \"src\"))  # <â€” lets Python find src/ddriver\n",
    "\n",
    "try:\n",
    "    import ddriver\n",
    "    print(\"ddriver imported OK from:\", ddriver.__file__)\n",
    "    # Confirm config picks up Colab env:\n",
    "    try:\n",
    "        from ddriver import config\n",
    "        print(\"Loaded ddriver.config successfully.\")\n",
    "        # Echo the resolved paths from config (they are pathlib.Path objects)\n",
    "        print(\"config.DATASET_ROOT =\", config.DATASET_ROOT)\n",
    "        print(\"config.OUT_ROOT     =\", config.OUT_ROOT)\n",
    "        print(\"config.CKPT_ROOT    =\", config.CKPT_ROOT)\n",
    "        print(\"config.FAST_DATA    =\", config.FAST_DATA)\n",
    "    except Exception as e:\n",
    "        print(\"Note: ddriver.config not imported:\", e)\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ Import failed â€” check package name/setup.\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374139f5",
   "metadata": {},
   "source": [
    "# ðŸ“‹ 9) Generate Manifest and Split CSVs\n",
    "\n",
    "This step creates the CSV files that tell your code where all the images are and which ones go to train/val/test.\n",
    "\n",
    "**What this does:**\n",
    "- Scans all your images in the dataset folder\n",
    "- Creates a big list (manifest.csv) with info about every image\n",
    "- Creates three smaller lists (train.csv, val.csv, test.csv) that say which images belong where\n",
    "- Saves everything to your Google Drive so it's permanent\n",
    "\n",
    "**Why we need this:**\n",
    "- Your training code needs to know which images to use\n",
    "- The manifest remembers which driver each image belongs to (for VAL split)\n",
    "- The split CSVs organize images into train/val/test groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8883386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the manifest generator\n",
    "# This is like asking a librarian to catalog all your books and create reading lists\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Make sure we can import ddriver\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Run the manifest script\n",
    "# --write-split-lists means \"also create train.csv, val.csv, test.csv files\"\n",
    "manifest_cmd = f\"cd {PROJECT_ROOT} && python -m ddriver.data.manifest --write-split-lists\"\n",
    "\n",
    "print(\"ðŸ”¨ Generating manifest and split CSVs...\")\n",
    "print(f\"Running: {manifest_cmd}\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    manifest_cmd,\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Show what happened\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Warnings/Errors:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\nâœ… Manifest and split CSVs generated successfully!\")\n",
    "    print(f\"   Manifest: {os.environ['OUT_ROOT']}/manifests/manifest.csv\")\n",
    "    print(f\"   Train split: {os.environ['OUT_ROOT']}/splits/train.csv\")\n",
    "    print(f\"   Val split: {os.environ['OUT_ROOT']}/splits/val.csv\")\n",
    "    print(f\"   Test split: {os.environ['OUT_ROOT']}/splits/test.csv\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Error generating manifest (exit code {result.returncode})\")\n",
    "    raise RuntimeError(\"Manifest generation failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: Did the CSVs get created?\n",
    "# This is like checking that the librarian actually wrote down all the book lists\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "manifest_path = Path(os.environ['OUT_ROOT']) / \"manifests\" / \"manifest.csv\"\n",
    "train_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"train.csv\"\n",
    "val_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"val.csv\"\n",
    "test_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"test.csv\"\n",
    "\n",
    "print(\"ðŸ“Š Checking CSV files...\\n\")\n",
    "\n",
    "for name, path in [(\"Manifest\", manifest_path), (\"Train\", train_path), (\"Val\", val_path), (\"Test\", test_path)]:\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"âœ… {name}: {len(df)} rows, columns: {list(df.columns)}\")\n",
    "    else:\n",
    "        print(f\"âŒ {name}: File not found at {path}\")\n",
    "\n",
    "# Show a sample from the manifest\n",
    "if manifest_path.exists():\n",
    "    print(\"\\nðŸ“„ Sample from manifest (first 3 rows):\")\n",
    "    sample = pd.read_csv(manifest_path).head(3)\n",
    "    print(sample[['path', 'class_id', 'driver_id', 'camera', 'split']].to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041552b4",
   "metadata": {},
   "source": [
    "# ðŸ§ª 10) Test dataset.py and datamod.py\n",
    "\n",
    "Now let's make sure the code that loads images actually works!\n",
    "\n",
    "**What we're testing:**\n",
    "1. **dataset.py** - Can it load a single image and give us the right info?\n",
    "2. **datamod.py** - Can it create data loaders that give us batches of images?\n",
    "\n",
    "**Why test this:**\n",
    "- If these don't work, training will fail\n",
    "- Better to catch problems now than later\n",
    "- We want to see that images load correctly and labels are right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6044f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Can dataset.py load a single image?\n",
    "# This is like testing if a worker can fetch one book from the library\n",
    "\n",
    "from ddriver.data.dataset import AucDriverDataset\n",
    "from torchvision import transforms as T\n",
    "from pathlib import Path\n",
    "\n",
    "# Get paths from config\n",
    "manifest_csv = Path(os.environ['OUT_ROOT']) / \"manifests\" / \"manifest.csv\"\n",
    "val_split_csv = Path(os.environ['OUT_ROOT']) / \"splits\" / \"val.csv\"\n",
    "\n",
    "print(\"ðŸ§ª Test 1: Testing AucDriverDataset (dataset.py)\")\n",
    "print(f\"   Manifest: {manifest_csv}\")\n",
    "print(f\"   Using Val split: {val_split_csv}\\n\")\n",
    "\n",
    "try:\n",
    "    # Create a simple dataset (no fancy transforms, just load the image)\n",
    "    simple_transforms = T.ToTensor()  # Just convert to tensor, no augmentation\n",
    "    \n",
    "    val_dataset = AucDriverDataset(\n",
    "        manifest_csv=manifest_csv,\n",
    "        split_csv=val_split_csv,\n",
    "        transforms=simple_transforms\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Dataset created! It has {len(val_dataset)} images in VAL split\")\n",
    "    \n",
    "    # Try to load the first image\n",
    "    print(\"\\nðŸ“– Loading first image from VAL split...\")\n",
    "    sample = val_dataset[0]\n",
    "    \n",
    "    print(f\"âœ… Image loaded successfully!\")\n",
    "    print(f\"   Image shape: {sample['image'].shape} (should be [3, height, width])\")\n",
    "    print(f\"   Label: {sample['label']} (should be 0-9)\")\n",
    "    print(f\"   Driver ID: {sample['driver_id']} (VAL should have driver IDs)\")\n",
    "    print(f\"   Camera: {sample['camera']} (should be 'cam1' or 'cam2')\")\n",
    "    print(f\"   Path: {sample['path'][:80]}...\")  # Show first 80 chars\n",
    "    \n",
    "    # Check that label is valid (0-9)\n",
    "    if 0 <= sample['label'] <= 9:\n",
    "        print(f\"   âœ… Label is valid (0-9)\")\n",
    "    else:\n",
    "        print(f\"   âŒ Label {sample['label']} is NOT in range 0-9!\")\n",
    "    \n",
    "    # Check that VAL has driver IDs\n",
    "    if sample['driver_id'] is not None:\n",
    "        print(f\"   âœ… VAL split has driver ID (as expected)\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  VAL split missing driver ID (might be okay if this image wasn't in your DRIVER_RANGES)\")\n",
    "    \n",
    "    print(\"\\nâœ… Test 1 PASSED: dataset.py works!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Test 1 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dec2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Can datamod.py create data loaders and load batches?\n",
    "# This is like testing if the teacher can organize students into groups and give them work\n",
    "\n",
    "from ddriver.data.datamod import build_dataloaders, make_cfg_from_config\n",
    "import torch\n",
    "\n",
    "print(\"ðŸ§ª Test 2: Testing build_dataloaders (datamod.py)\\n\")\n",
    "\n",
    "try:\n",
    "    # Create config using the helper that uses ddriver.config paths\n",
    "    # This is the easy way - it automatically finds your CSVs!\n",
    "    cfg = make_cfg_from_config(\n",
    "        batch_size=4,  # Small batch for testing (faster)\n",
    "        num_workers=2,  # Use 2 workers (Colab might have limited CPUs)\n",
    "        image_size=224,  # Standard image size\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Config created using ddriver.config paths:\")\n",
    "    print(f\"   Manifest: {cfg.manifest_csv}\")\n",
    "    print(f\"   Train: {cfg.train_split_csv}\")\n",
    "    print(f\"   Val: {cfg.val_split_csv}\")\n",
    "    print(f\"   Test: {cfg.test_split_csv}\\n\")\n",
    "    \n",
    "    # Build the data loaders\n",
    "    print(\"ðŸ”¨ Building data loaders...\")\n",
    "    loaders = build_dataloaders(cfg)\n",
    "    \n",
    "    print(\"âœ… Data loaders created!\")\n",
    "    print(f\"   Available splits: {list(loaders.keys())}\\n\")\n",
    "    \n",
    "    # Test train loader\n",
    "    print(\"ðŸ“¦ Testing TRAIN loader...\")\n",
    "    train_loader = loaders[\"train\"]\n",
    "    train_batch = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"   âœ… Train batch loaded!\")\n",
    "    print(f\"   Batch size: {train_batch['image'].shape[0]} images\")\n",
    "    print(f\"   Image shape: {train_batch['image'].shape} (should be [batch_size, 3, 224, 224])\")\n",
    "    print(f\"   Labels: {train_batch['label'].tolist()} (should be list of 0-9)\")\n",
    "    print(f\"   Driver IDs: {train_batch['driver_id']} (train should mostly be None)\")\n",
    "    print(f\"   Cameras: {train_batch['camera']}\")\n",
    "    \n",
    "    # Check image shape is correct\n",
    "    expected_shape = (cfg.batch_size, 3, cfg.image_size, cfg.image_size)\n",
    "    if train_batch['image'].shape == expected_shape:\n",
    "        print(f\"   âœ… Image shape is correct: {train_batch['image'].shape}\")\n",
    "    else:\n",
    "        print(f\"   âŒ Image shape wrong! Got {train_batch['image'].shape}, expected {expected_shape}\")\n",
    "    \n",
    "    # Test val loader\n",
    "    print(\"\\nðŸ“¦ Testing VAL loader...\")\n",
    "    val_loader = loaders[\"val\"]\n",
    "    val_batch = next(iter(val_loader))\n",
    "    \n",
    "    print(f\"   âœ… Val batch loaded!\")\n",
    "    print(f\"   Batch size: {val_batch['image'].shape[0]} images\")\n",
    "    print(f\"   Image shape: {val_batch['image'].shape}\")\n",
    "    print(f\"   Labels: {val_batch['label'].tolist()}\")\n",
    "    print(f\"   Driver IDs: {val_batch['driver_id']} (VAL should have driver IDs!)\")\n",
    "    \n",
    "    # Check that VAL has driver IDs\n",
    "    val_has_ids = any(did is not None for did in val_batch['driver_id'])\n",
    "    if val_has_ids:\n",
    "        print(f\"   âœ… VAL batch has driver IDs (as expected)\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  VAL batch missing driver IDs (check your DRIVER_RANGES in manifest.py)\")\n",
    "    \n",
    "    # Test that images are normalized (should be in range roughly -2 to 2 after ImageNet normalization)\n",
    "    img_min, img_max = train_batch['image'].min().item(), train_batch['image'].max().item()\n",
    "    print(f\"\\n   Image value range: [{img_min:.3f}, {img_max:.3f}]\")\n",
    "    print(f\"   (Should be roughly -2 to 2 after ImageNet normalization)\")\n",
    "    \n",
    "    print(\"\\nâœ… Test 2 PASSED: datamod.py works! Data loaders are ready for training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Test 2 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1047bf",
   "metadata": {},
   "source": [
    "### âœ… You're all set!\n",
    "\n",
    "**What just happened:**\n",
    "1. âœ… Mounted Google Drive\n",
    "2. âœ… Cloned/updated your repo\n",
    "3. âœ… Installed the package\n",
    "4. âœ… Set up paths (works on Colab and Mac!)\n",
    "5. âœ… Generated manifest.csv and train/val/test split CSVs\n",
    "6. âœ… Tested that dataset.py can load images\n",
    "7. âœ… Tested that datamod.py can create data loaders\n",
    "\n",
    "**Your CSVs are saved in Google Drive:**\n",
    "- `OUT_ROOT/manifests/manifest.csv` - Big list of all images\n",
    "- `OUT_ROOT/splits/train.csv` - Training images\n",
    "- `OUT_ROOT/splits/val.csv` - Validation images (with driver IDs!)\n",
    "- `OUT_ROOT/splits/test.csv` - Test images\n",
    "\n",
    "**Next steps:**\n",
    "- You can now use `build_dataloaders()` in your training code\n",
    "- All paths use `ddriver.config` so it works on Colab and Mac\n",
    "- Re-run **Clone/Update** cell after pushing new commits\n",
    "- Optional: copy some data into `/content/data` to use `FAST_DATA` for speed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1814de",
   "metadata": {},
   "source": [
    "\n",
    "### âœ… Youâ€™re set!\n",
    "- Your repo + URL are **hardcoded**.\n",
    "- `ddriver.config` will see the Colab env vars and resolve paths there.\n",
    "- Re-run **Clone/Update** after pushing new commits.\n",
    "- Optional: copy some data into `/content/data` to use `FAST_DATA` for speed, then call `ddriver.config.dataset_dir(prefer_fast=True)` in your scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Colab cell: append metrics + params to Google Sheet ----\n",
    "!pip -q install gspread\n",
    "\n",
    "import json, os\n",
    "from pathlib import Path\n",
    "import gspread\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "gc = gspread.authorize(gspread.auth.default())\n",
    "\n",
    "SHEET_NAME = \"TFM Logs\"   # change if needed\n",
    "WORKSHEET  = \"Sheet1\"     # or whatever tab name\n",
    "\n",
    "# Point to your latest run folder (paste it from the console printout)\n",
    "run_dir = Path(\"/Users/claudiapacheco/TFM/outputs/metrics/val_run1/2025-11-09_12-34-56\")\n",
    "\n",
    "metrics = json.loads((run_dir / \"metrics.json\").read_text())\n",
    "inputs  = json.loads((run_dir / \"inputs.json\").read_text())\n",
    "params_path = run_dir / \"params.json\"\n",
    "params = json.loads(params_path.read_text()) if params_path.exists() else {}\n",
    "\n",
    "ws = gc.open(SHEET_NAME).worksheet(WORKSHEET)\n",
    "\n",
    "row = [\n",
    "  str(run_dir),                        # Run folder\n",
    "  inputs.get(\"predictions\",\"\"),        # Predictions file\n",
    "  inputs.get(\"split_source\",\"\"),       # Split source\n",
    "  metrics[\"num_examples\"],             # Support\n",
    "  round(metrics[\"overall\"][\"accuracy\"], 4),\n",
    "  round(metrics[\"overall\"][\"macro_avg\"][\"f1\"], 4),\n",
    "  json.dumps(params, sort_keys=True)[:500],  # params preview (trim)\n",
    "]\n",
    "ws.append_row(row, value_input_option=\"USER_ENTERED\")\n",
    "print(\"Appended to Google Sheet âœ…\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
