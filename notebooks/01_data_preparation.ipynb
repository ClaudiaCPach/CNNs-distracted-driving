{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìã 01 ‚Äî Data Preparation\n",
        "\n",
        "**Purpose:** Generate manifests, extract hybrid ROI crops, create tar archives, and audit quality.\n",
        "\n",
        "**Sections:**\n",
        "1. Inline Setup (run if starting fresh)\n",
        "2. Manifest & Split Generation\n",
        "3. Hybrid ROI Extraction (face, face_hands)\n",
        "4. **Create Tar Archives (ONE-TIME)** ‚Äî for fast data loading in future sessions\n",
        "5. Quality Audit & Thesis Figures\n",
        "\n",
        "**Prerequisites:** Original images exist on Google Drive at `DRIVE_DATA_ROOT/auc.distracted.driver.dataset_v2/`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Section 1: Inline Setup\n",
        "\n",
        "Run these cells if starting this notebook fresh (not coming from 00_setup.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- INLINE SETUP (run if starting fresh) ---\n",
        "import os, subprocess, sys\n",
        "\n",
        "# Config\n",
        "REPO_URL       = \"https://github.com/ClaudiaCPach/CNNs-distracted-driving\"\n",
        "REPO_DIRNAME   = \"CNNs-distracted-driving\"\n",
        "BRANCH         = \"main\"\n",
        "PROJECT_ROOT   = f\"/content/{REPO_DIRNAME}\"\n",
        "DRIVE_PATH     = \"/content/drive/MyDrive/TFM\"\n",
        "DRIVE_DATA_ROOT = f\"{DRIVE_PATH}/data\"\n",
        "FAST_DATA      = \"/content/data\"\n",
        "DATASET_ROOT   = DRIVE_DATA_ROOT\n",
        "OUT_ROOT       = f\"{DRIVE_PATH}/outputs\"\n",
        "CKPT_ROOT      = f\"{DRIVE_PATH}/checkpoints\"\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Clone/update repo\n",
        "def sh(cmd):\n",
        "    print(f\"$ {cmd}\")\n",
        "    rc = subprocess.call(cmd, shell=True, executable=\"/bin/bash\")\n",
        "    if rc != 0:\n",
        "        raise RuntimeError(f\"Command failed: {cmd}\")\n",
        "\n",
        "if os.path.isdir(PROJECT_ROOT):\n",
        "    sh(f\"cd {PROJECT_ROOT} && git pull --rebase origin {BRANCH}\")\n",
        "else:\n",
        "    sh(f\"git clone --branch {BRANCH} {REPO_URL} {PROJECT_ROOT}\")\n",
        "\n",
        "# Install\n",
        "sh(f\"pip install -q -e {PROJECT_ROOT}\")\n",
        "\n",
        "# Set env vars\n",
        "os.environ[\"DRIVE_PATH\"] = DRIVE_PATH\n",
        "os.environ[\"DATASET_ROOT\"] = DATASET_ROOT\n",
        "os.environ[\"OUT_ROOT\"] = OUT_ROOT\n",
        "os.environ[\"CKPT_ROOT\"] = CKPT_ROOT\n",
        "os.environ[\"FAST_DATA\"] = FAST_DATA\n",
        "\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "sys.path.insert(0, os.path.join(PROJECT_ROOT, \"src\"))\n",
        "\n",
        "print(\"‚úÖ Inline setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Section 2: Manifest & Split Generation\n",
        "\n",
        "Generate manifest.csv and train/val/test split CSVs. **Run once** ‚Äî results persist on Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the manifest generator\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "manifest_cmd = f\"cd {PROJECT_ROOT} && python -m ddriver.data.manifest --write-split-lists\"\n",
        "\n",
        "print(\"üî® Generating manifest and split CSVs...\")\n",
        "print(f\"Running: {manifest_cmd}\\n\")\n",
        "\n",
        "result = subprocess.run(manifest_cmd, shell=True, capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "if result.stderr:\n",
        "    print(\"Warnings/Errors:\", result.stderr)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n‚úÖ Manifest and split CSVs generated successfully!\")\n",
        "    print(f\"   Manifest: {os.environ['OUT_ROOT']}/manifests/manifest.csv\")\n",
        "    print(f\"   Train: {os.environ['OUT_ROOT']}/splits/train.csv\")\n",
        "    print(f\"   Val: {os.environ['OUT_ROOT']}/splits/val.csv\")\n",
        "    print(f\"   Test: {os.environ['OUT_ROOT']}/splits/test.csv\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Error (exit code {result.returncode})\")\n",
        "    raise RuntimeError(\"Manifest generation failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify CSVs were created\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "manifest_path = Path(os.environ['OUT_ROOT']) / \"manifests\" / \"manifest.csv\"\n",
        "train_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"train.csv\"\n",
        "val_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"val.csv\"\n",
        "test_path = Path(os.environ['OUT_ROOT']) / \"splits\" / \"test.csv\"\n",
        "\n",
        "print(\"üìä Checking CSV files...\\n\")\n",
        "for name, path in [(\"Manifest\", manifest_path), (\"Train\", train_path), (\"Val\", val_path), (\"Test\", test_path)]:\n",
        "    if path.exists():\n",
        "        df = pd.read_csv(path)\n",
        "        print(f\"‚úÖ {name}: {len(df)} rows, columns: {list(df.columns)}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {name}: File not found at {path}\")\n",
        "\n",
        "if manifest_path.exists():\n",
        "    print(\"\\nüìÑ Sample from manifest (first 3 rows):\")\n",
        "    sample = pd.read_csv(manifest_path).head(3)\n",
        "    print(sample[['path', 'class_id', 'driver_id', 'camera', 'split']].to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a tiny balanced subset for quick testing (20 images per class)\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from ddriver import config\n",
        "\n",
        "train_csv = Path(config.OUT_ROOT) / \"splits\" / \"train.csv\"\n",
        "train_small_csv = Path(config.OUT_ROOT) / \"splits\" / \"train_small.csv\"\n",
        "\n",
        "print(f\"Reading {train_csv}...\")\n",
        "df = pd.read_csv(train_csv)\n",
        "\n",
        "small = df.groupby(\"class_id\").head(20)\n",
        "\n",
        "print(f\"Original train.csv: {len(df)} images\")\n",
        "print(f\"Small subset: {len(small)} images ({len(small) // 10} per class)\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(small[\"class_id\"].value_counts().sort_index())\n",
        "\n",
        "small.to_csv(train_small_csv, index=False)\n",
        "print(f\"\\n‚úÖ Saved to {train_small_csv}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÄ Section 3: Hybrid ROI Extraction (InsightFace + MediaPipe Hands)\n",
        "\n",
        "Extract face and face+hands crops using the hybrid pipeline. **Run once per variant** ‚Äî results persist on Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install extraction dependencies\n",
        "!pip -q install insightface onnxruntime mediapipe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÄ Hybrid ROI Extraction ‚Äî FACE variant\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "VARIANT = \"face\"  # <<<< CHANGE TO \"face_hands\" for second run\n",
        "\n",
        "# Output location (Drive for persistence)\n",
        "HYBRID_OUTPUT_ROOT = Path(OUT_ROOT) / \"hybrid\"\n",
        "\n",
        "manifest_csv = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
        "splits_root = Path(OUT_ROOT) / \"splits\"\n",
        "\n",
        "# Auto-detect local vs Drive images\n",
        "LOCAL_DATASET_ROOT = Path(\"/content/data/auc.distracted.driver.dataset_v2\")\n",
        "DRIVE_DATASET_ROOT = Path(DATASET_ROOT)\n",
        "\n",
        "if LOCAL_DATASET_ROOT.exists() and any(LOCAL_DATASET_ROOT.iterdir()):\n",
        "    EFFECTIVE_DATASET_ROOT = LOCAL_DATASET_ROOT\n",
        "    print(f\"üöÄ Using local images from {LOCAL_DATASET_ROOT}\")\n",
        "else:\n",
        "    EFFECTIVE_DATASET_ROOT = DRIVE_DATASET_ROOT\n",
        "    print(f\"üìÅ Using images from Drive: {DRIVE_DATASET_ROOT}\")\n",
        "\n",
        "# Test mode options\n",
        "TEST_MODE = False  # Set True for quick test\n",
        "LIMIT = None  # Set to e.g. 50 for debugging\n",
        "\n",
        "sample_flag = \"\"\n",
        "limit_flag = f\"--limit {LIMIT}\" if LIMIT else \"\"\n",
        "\n",
        "extract_cmd = f\"\"\"\n",
        "cd {PROJECT_ROOT}\n",
        "python -m src.ddriver.data.hybrid_extract \\\n",
        "  --manifest {manifest_csv} \\\n",
        "  --splits-root {splits_root} \\\n",
        "  --dataset-root {EFFECTIVE_DATASET_ROOT} \\\n",
        "  --output-root {HYBRID_OUTPUT_ROOT} \\\n",
        "  --variant {VARIANT} \\\n",
        "  --min-face-conf 0.4 \\\n",
        "  --min-detection-area-frac 0.005 \\\n",
        "  --min-area-frac 0.01 \\\n",
        "  --min-aspect 0.08 \\\n",
        "  --pad-frac 0.35 \\\n",
        "  --max-area-frac 0.40 \\\n",
        "  {limit_flag} \\\n",
        "  --overwrite\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Running Hybrid extraction for variant: {VARIANT}\")\n",
        "print(extract_cmd)\n",
        "proc = subprocess.Popen(extract_cmd, shell=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
        "for line in proc.stdout:\n",
        "    print(line, end=\"\")\n",
        "proc.wait()\n",
        "if proc.returncode != 0:\n",
        "    raise RuntimeError(\"Hybrid extraction failed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÅ Regenerate Hybrid CSVs (manifest + splits) for the extracted variant\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# VARIANT should match what you just extracted\n",
        "VARIANT = VARIANT if 'VARIANT' in globals() else 'face'\n",
        "HYBRID_OUTPUT_ROOT = HYBRID_OUTPUT_ROOT if 'HYBRID_OUTPUT_ROOT' in globals() else Path(OUT_ROOT) / 'hybrid'\n",
        "\n",
        "manifest_csv = Path(OUT_ROOT) / 'manifests' / 'manifest.csv'\n",
        "splits_root = Path(OUT_ROOT) / 'splits'\n",
        "crop_root = Path(HYBRID_OUTPUT_ROOT) / VARIANT\n",
        "meta_csv = Path(HYBRID_OUTPUT_ROOT) / f'detection_metadata_{VARIANT}.csv'\n",
        "\n",
        "def _extract_class(path_str):\n",
        "    for part in Path(path_str).parts:\n",
        "        if len(part) == 2 and part.startswith('c') and part[1].isdigit():\n",
        "            return part\n",
        "    return None\n",
        "\n",
        "def _extract_camera(path_str):\n",
        "    for part in Path(path_str).parts:\n",
        "        if part.lower().startswith('camera'):\n",
        "            return part\n",
        "    return None\n",
        "\n",
        "def _extract_filename(path_str):\n",
        "    return Path(path_str).name\n",
        "\n",
        "def _coerce_class_id(value):\n",
        "    if pd.isna(value):\n",
        "        return None\n",
        "    value_str = str(value)\n",
        "    if len(value_str) == 2 and value_str.startswith('c') and value_str[1].isdigit():\n",
        "        return value_str\n",
        "    if value_str.isdigit():\n",
        "        return f'c{int(value_str)}'\n",
        "    return None\n",
        "\n",
        "def _normalize_camera(cam):\n",
        "    if cam is None or pd.isna(cam):\n",
        "        return None\n",
        "    cam_str = str(cam).lower().replace(' ', '')\n",
        "    if cam_str in ['camera1', 'cam1']:\n",
        "        return 'cam1'\n",
        "    if cam_str in ['camera2', 'cam2']:\n",
        "        return 'cam2'\n",
        "    return cam_str\n",
        "\n",
        "print(f'üìÇ Loading original manifest: {manifest_csv}')\n",
        "orig_df = pd.read_csv(manifest_csv)\n",
        "orig_df = orig_df.rename(columns={'path': 'original_path'})\n",
        "orig_df['_filename'] = orig_df['original_path'].astype(str).map(_extract_filename)\n",
        "orig_df['_class'] = orig_df['original_path'].astype(str).map(_extract_class)\n",
        "orig_df['_camera'] = orig_df['original_path'].astype(str).map(_extract_camera).map(_normalize_camera)\n",
        "\n",
        "print(f'üîç Scanning crops: {crop_root}')\n",
        "crop_paths = list(crop_root.rglob('*.jpg'))\n",
        "crop_df = pd.DataFrame({'crop_path': [str(p) for p in crop_paths]})\n",
        "crop_df['path'] = crop_df['crop_path'].map(lambda p: str(Path(p).relative_to(HYBRID_OUTPUT_ROOT)))\n",
        "crop_df['_filename'] = crop_df['crop_path'].map(_extract_filename)\n",
        "crop_df['_class'] = crop_df['crop_path'].map(_extract_class)\n",
        "crop_df['_camera'] = crop_df['crop_path'].map(_extract_camera).map(_normalize_camera)\n",
        "\n",
        "fallback_paths = set()\n",
        "if meta_csv.exists():\n",
        "    meta_df = pd.read_csv(meta_csv)\n",
        "    meta_df = meta_df[meta_df['cropped_path'].astype(str).str.len() > 0]\n",
        "    meta_df['path'] = meta_df['cropped_path'].astype(str)\n",
        "    meta_df['_class'] = meta_df['class_id'].map(_coerce_class_id)\n",
        "    meta_df['_camera'] = meta_df['camera'].map(_normalize_camera)\n",
        "    crop_df = crop_df.merge(\n",
        "        meta_df[['path', '_class', '_camera', 'fallback_to_full']],\n",
        "        on='path', how='left', suffixes=('', '_meta'),\n",
        "    )\n",
        "    crop_df['_class'] = crop_df['_class'].fillna(crop_df['_class_meta'])\n",
        "    crop_df['_camera'] = crop_df['_camera'].fillna(crop_df['_camera_meta'])\n",
        "    crop_df = crop_df.drop(columns=['_class_meta', '_camera_meta'], errors='ignore')\n",
        "    fallback_paths = set(meta_df.loc[meta_df['fallback_to_full'] == True, 'path'].dropna().astype(str))\n",
        "    print(f'üö´ Excluding {len(fallback_paths)} fallback crops from splits')\n",
        "\n",
        "crop_df_all = crop_df.copy()\n",
        "crop_df = crop_df[~crop_df['path'].isin(fallback_paths)]\n",
        "\n",
        "merged = crop_df_all.merge(orig_df, on=['_filename', '_class', '_camera'], how='left')\n",
        "manifest_out = merged.drop(columns=['crop_path', '_filename', '_class', '_camera'], errors='ignore')\n",
        "manifest_out_path = Path(HYBRID_OUTPUT_ROOT) / f'manifest_{VARIANT}.csv'\n",
        "manifest_out.to_csv(manifest_out_path, index=False)\n",
        "print(f'‚úÖ Wrote manifest: {manifest_out_path}')\n",
        "\n",
        "for split_name in ['train', 'val', 'test']:\n",
        "    split_path = splits_root / f'{split_name}.csv'\n",
        "    split_df = pd.read_csv(split_path)\n",
        "    split_df['path'] = split_df['path'].astype(str)\n",
        "    split_df['_filename'] = split_df['path'].map(_extract_filename)\n",
        "    split_df['_class'] = split_df['path'].map(_extract_class)\n",
        "    split_df['_camera'] = split_df['path'].map(_extract_camera).map(_normalize_camera)\n",
        "\n",
        "    split_merged = split_df.merge(crop_df, on=['_filename', '_class', '_camera'], how='inner')\n",
        "    split_merged['original_path'] = split_merged['path_x']\n",
        "    split_merged['path'] = split_merged['path_y']\n",
        "    cols_to_drop = ['path_x', 'path_y', '_filename', '_class', '_camera', 'crop_path', 'fallback_to_full']\n",
        "    split_merged = split_merged.drop(columns=[c for c in cols_to_drop if c in split_merged.columns])\n",
        "\n",
        "    out_split = Path(HYBRID_OUTPUT_ROOT) / f'{split_name}_{VARIANT}.csv'\n",
        "    split_merged.to_csv(out_split, index=False)\n",
        "    print(f'‚úÖ Wrote split: {out_split} ({len(split_merged)} rows)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Section 3b: Generate Control Splits (5-Run Plan)\n",
        "\n",
        "Generate filtered split CSVs for the experimental control runs. This creates full-frame splits filtered to the same images that have face/face+hands crops available.\n",
        "\n",
        "**Run once** after extracting both face and face_hands variants ‚Äî results persist on Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ Generate Control Splits for 5-Run Experimental Plan\n",
        "from pathlib import Path\n",
        "from ddriver.data.id_sets import (\n",
        "    extract_id_sets,\n",
        "    generate_control_splits,\n",
        "    save_id_sets,\n",
        "    print_id_set_summary,\n",
        ")\n",
        "\n",
        "# Paths to manifests\n",
        "manifest_full = Path(OUT_ROOT) / \"manifests\" / \"manifest.csv\"\n",
        "manifest_face = Path(OUT_ROOT) / \"hybrid\" / \"manifest_face.csv\"\n",
        "manifest_fh = Path(OUT_ROOT) / \"hybrid\" / \"manifest_face_hands.csv\"\n",
        "\n",
        "# Verify manifests exist\n",
        "missing = []\n",
        "for name, path in [(\"Full-frame\", manifest_full), (\"Face\", manifest_face), (\"Face+Hands\", manifest_fh)]:\n",
        "    if not path.exists():\n",
        "        missing.append(f\"{name}: {path}\")\n",
        "\n",
        "if missing:\n",
        "    print(\"‚ö†Ô∏è  Missing manifests (run hybrid extraction first):\")\n",
        "    for m in missing:\n",
        "        print(f\"   - {m}\")\n",
        "    raise FileNotFoundError(\"Run hybrid extraction for both face and face_hands variants first.\")\n",
        "\n",
        "# Extract ID sets from manifests\n",
        "print(\"üîç Extracting ID sets from manifests...\")\n",
        "id_sets = extract_id_sets(\n",
        "    manifest_full=manifest_full,\n",
        "    manifest_face=manifest_face,\n",
        "    manifest_fh=manifest_fh,\n",
        ")\n",
        "print_id_set_summary(id_sets)\n",
        "\n",
        "# Save ID sets for reference/auditing\n",
        "id_sets_dir = Path(OUT_ROOT) / \"splits\" / \"id_sets\"\n",
        "print(f\"\\nüíæ Saving ID sets to {id_sets_dir}...\")\n",
        "save_id_sets(id_sets, id_sets_dir)\n",
        "\n",
        "# Generate control split CSVs\n",
        "splits_root = Path(OUT_ROOT) / \"splits\"\n",
        "control_output = Path(OUT_ROOT) / \"splits\" / \"control\"\n",
        "\n",
        "print(\"\\nüîß Generating control splits...\")\n",
        "results = generate_control_splits(\n",
        "    splits_root=splits_root,\n",
        "    id_sets=id_sets,\n",
        "    output_root=control_output,\n",
        "    generate_both=True,  # Also generate S_both splits\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Control splits saved to: {control_output}\")\n",
        "print(\"\\nüìÅ Generated files:\")\n",
        "for subset_name, splits in results.items():\n",
        "    for split_name, path in splits.items():\n",
        "        print(f\"   {subset_name}/{split_name}: {path.name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Section 4: Create Tar Archives (ONE-TIME)\n",
        "\n",
        "Create tar archives of hybrid crops for **fast loading in future sessions**.\n",
        "\n",
        "Copying ~13,000 small files one-by-one over the Drive FUSE mount takes 2+ hours.\n",
        "A single tar archive can be copied in ~5 minutes and extracted instantly.\n",
        "\n",
        "**Run these cells ONCE** after extracting hybrid crops. The archives persist on Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ Create Tar Archives for Hybrid Crops (ONE-TIME)\n",
        "# Run this ONCE after extracting hybrid crops. Archives persist on Drive.\n",
        "\n",
        "from pathlib import Path\n",
        "from ddriver.data.fastcopy import create_tar_archive\n",
        "\n",
        "DRIVE_ROOT = Path(OUT_ROOT) / \"hybrid\"\n",
        "TAR_OUTPUT_DIR = DRIVE_ROOT  # Save tars alongside hybrid folder\n",
        "\n",
        "# Create archives for both variants\n",
        "for variant in [\"face\", \"face_hands\"]:\n",
        "    source_dir = DRIVE_ROOT / variant\n",
        "    tar_path = TAR_OUTPUT_DIR / f\"hybrid_{variant}.tar\"\n",
        "    \n",
        "    if not source_dir.exists():\n",
        "        print(f\"‚ö†Ô∏è  {variant}: Source not found at {source_dir}\")\n",
        "        continue\n",
        "    \n",
        "    if tar_path.exists():\n",
        "        size_mb = tar_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"‚úÖ {variant}: Archive already exists ({size_mb:.1f} MB)\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üì¶ Creating tar archive for: {variant}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    result = create_tar_archive(\n",
        "        source_dir=source_dir,\n",
        "        tar_path=tar_path,\n",
        "        use_gzip=False,  # Faster extraction, JPEG already compressed\n",
        "        verbose=True,\n",
        "    )\n",
        "    \n",
        "    print(f\"   ‚úÖ Created: {tar_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìã Archive Summary\")\n",
        "print(\"=\"*60)\n",
        "for variant in [\"face\", \"face_hands\"]:\n",
        "    tar_path = TAR_OUTPUT_DIR / f\"hybrid_{variant}.tar\"\n",
        "    if tar_path.exists():\n",
        "        size_mb = tar_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"   ‚úÖ hybrid_{variant}.tar: {size_mb:.1f} MB\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå hybrid_{variant}.tar: Not created\")\n",
        "\n",
        "print(\"\\nüí° These archives will be used in 02_training.ipynb for fast data loading.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Section 4b: Create Full-Frame Compressed Tar (ONE-TIME)\n",
        "\n",
        "Create a tar archive of **compressed full-frame images** for fast loading.\n",
        "\n",
        "This involves:\n",
        "1. Compressing images to 320px (shorter side) at 80% JPEG quality\n",
        "2. Creating a tar archive\n",
        "\n",
        "**Run ONCE** ‚Äî the archive persists on Drive for all future sessions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ Create Full-Frame Compressed Tar Archive (ONE-TIME)\n",
        "# This cell compresses full-frame images and creates a tar for fast loading.\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from ddriver.data.fastcopy import (\n",
        "    CompressionSpec, \n",
        "    copy_splits_with_compression,\n",
        "    create_tar_archive,\n",
        ")\n",
        "\n",
        "# Check if tar already exists\n",
        "FULL_TAR_PATH = Path(DRIVE_DATA_ROOT) / \"full_compressed.tar\"\n",
        "\n",
        "if FULL_TAR_PATH.exists():\n",
        "    size_mb = FULL_TAR_PATH.stat().st_size / (1024 * 1024)\n",
        "    print(f\"‚úÖ Full-frame tar already exists: {FULL_TAR_PATH}\")\n",
        "    print(f\"   Size: {size_mb:.1f} MB\")\n",
        "    print(\"   Skipping creation. Delete the tar file if you want to recreate it.\")\n",
        "else:\n",
        "    print(\"üì¶ Creating compressed full-frame tar archive...\")\n",
        "    print(\"   This is a ONE-TIME operation (may take 30-60 minutes).\")\n",
        "    print()\n",
        "    \n",
        "    # Step 1: Compress images to /content (temporary)\n",
        "    SRC_ROOT = Path(DRIVE_DATA_ROOT) / \"auc.distracted.driver.dataset_v2\"\n",
        "    DST_ROOT = Path(\"/content/data/full_compressed\")\n",
        "    \n",
        "    split_csvs = {\n",
        "        \"train\": Path(OUT_ROOT) / \"splits\" / \"train.csv\",\n",
        "        \"val\": Path(OUT_ROOT) / \"splits\" / \"val.csv\",\n",
        "        \"test\": Path(OUT_ROOT) / \"splits\" / \"test.csv\",\n",
        "    }\n",
        "    \n",
        "    compression_spec = CompressionSpec(target_short_side=320, jpeg_quality=80)\n",
        "    \n",
        "    print(\"Step 1/3: Compressing images...\")\n",
        "    summary = copy_splits_with_compression(\n",
        "        split_csvs=split_csvs,\n",
        "        src_root=SRC_ROOT,\n",
        "        dst_root=DST_ROOT,\n",
        "        compression=compression_spec,\n",
        "        skip_existing=False,\n",
        "    )\n",
        "    print(f\"   ‚úÖ Compressed {summary['processed']} images to {DST_ROOT}\")\n",
        "    \n",
        "    # Step 2: Create tar archive\n",
        "    print(\"\\nStep 2/3: Creating tar archive...\")\n",
        "    TEMP_TAR = Path(\"/content/full_compressed.tar\")\n",
        "    result = create_tar_archive(\n",
        "        source_dir=DST_ROOT,\n",
        "        tar_path=TEMP_TAR,\n",
        "        use_gzip=False,\n",
        "        verbose=True,\n",
        "    )\n",
        "    \n",
        "    # Step 3: Copy tar to Drive\n",
        "    print(\"\\nStep 3/3: Copying tar to Drive...\")\n",
        "    shutil.copy2(TEMP_TAR, FULL_TAR_PATH)\n",
        "    size_mb = FULL_TAR_PATH.stat().st_size / (1024 * 1024)\n",
        "    print(f\"   ‚úÖ Saved to {FULL_TAR_PATH} ({size_mb:.1f} MB)\")\n",
        "    \n",
        "    # Cleanup\n",
        "    TEMP_TAR.unlink()\n",
        "    shutil.rmtree(DST_ROOT, ignore_errors=True)\n",
        "    print(\"\\n‚úÖ Full-frame tar archive created!\")\n",
        "    print(\"   Future sessions will load data in ~5 minutes instead of 2+ hours.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Section 5: Quality Audit & Thesis Figures\n",
        "\n",
        "Analyze detection quality and generate figures for your thesis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç Hybrid Crop Quality Audit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "VARIANT = \"face\"  # must match the variant you extracted\n",
        "\n",
        "hybrid_root_local = Path(os.environ.get(\"HYBRID_ROOT_LOCAL\", \"\"))\n",
        "hybrid_root = hybrid_root_local if hybrid_root_local.exists() else Path(OUT_ROOT) / \"hybrid\"\n",
        "\n",
        "metadata_csv = hybrid_root / f\"detection_metadata_{VARIANT}.csv\"\n",
        "if not metadata_csv.exists():\n",
        "    raise FileNotFoundError(f\"Detection metadata not found: {metadata_csv}\")\n",
        "\n",
        "print(f\"üìÅ Loading metadata from: {metadata_csv}\")\n",
        "df = pd.read_csv(metadata_csv)\n",
        "\n",
        "n_total = len(df)\n",
        "n_fallback = df[\"fallback_to_full\"].sum()\n",
        "n_skipped = df[\"skipped\"].sum() if \"skipped\" in df.columns else 0\n",
        "n_saved = n_total - n_skipped\n",
        "n_face = (df[\"face_count\"] > 0).sum()\n",
        "n_left_hand = df[\"left_hand_detected\"].sum()\n",
        "n_right_hand = df[\"right_hand_detected\"].sum()\n",
        "n_both_hands = ((df[\"left_hand_detected\"]) & (df[\"right_hand_detected\"])).sum()\n",
        "n_any_hands = ((df[\"left_hand_detected\"]) | (df[\"right_hand_detected\"])).sum()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä HYBRID DETECTION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total images processed: {n_total}\")\n",
        "print(f\"   Images SAVED: {n_saved} ({100*n_saved/n_total:.1f}%)\")\n",
        "print(f\"\\nüéØ Detection rates:\")\n",
        "print(f\"   Face detected: {n_face} ({100*n_face/n_total:.1f}%)\")\n",
        "print(f\"   Left hand: {n_left_hand} ({100*n_left_hand/n_total:.1f}%)\")\n",
        "print(f\"   Right hand: {n_right_hand} ({100*n_right_hand/n_total:.1f}%)\")\n",
        "print(f\"   Both hands: {n_both_hands} ({100*n_both_hands/n_total:.1f}%)\")\n",
        "print(f\"\\n‚ö†Ô∏è  Fallback to full frame: {n_fallback} ({100*n_fallback/n_total:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìã Breakdown by Camera and Class\n",
        "print(\"\\nüìã BREAKDOWN BY CAMERA\")\n",
        "print(\"-\" * 80)\n",
        "camera_stats = df.groupby(\"camera\").agg({\n",
        "    \"fallback_to_full\": [\"sum\", \"mean\"],\n",
        "    \"roi_area_frac\": \"mean\",\n",
        "    \"face_count\": lambda x: (x > 0).mean(),\n",
        "}).round(3)\n",
        "camera_stats.columns = [\"fallback_count\", \"fallback_pct\", \"mean_roi_area\", \"face_rate\"]\n",
        "camera_stats[\"fallback_pct\"] = (camera_stats[\"fallback_pct\"] * 100).round(1)\n",
        "camera_stats[\"face_rate\"] = (camera_stats[\"face_rate\"] * 100).round(1)\n",
        "print(camera_stats.to_string())\n",
        "\n",
        "print(\"\\nüìã BREAKDOWN BY CLASS\")\n",
        "print(\"-\" * 80)\n",
        "class_stats = df.groupby(\"class_id\").agg({\n",
        "    \"fallback_to_full\": [\"sum\", \"mean\"],\n",
        "    \"roi_area_frac\": \"mean\",\n",
        "    \"face_count\": lambda x: (x > 0).mean(),\n",
        "}).round(3)\n",
        "class_stats.columns = [\"fallback_count\", \"fallback_pct\", \"mean_roi_area\", \"face_rate\"]\n",
        "class_stats[\"fallback_pct\"] = (class_stats[\"fallback_pct\"] * 100).round(1)\n",
        "class_stats[\"face_rate\"] = (class_stats[\"face_rate\"] * 100).round(1)\n",
        "print(class_stats.to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä ROI Quality Distribution Histograms (thesis figure)\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "plt.rcParams.update({\"font.size\": 12})\n",
        "\n",
        "meta_face = pd.read_csv(Path(OUT_ROOT) / \"hybrid/detection_metadata_face.csv\")\n",
        "meta_fh = pd.read_csv(Path(OUT_ROOT) / \"hybrid/detection_metadata_face_hands.csv\")\n",
        "\n",
        "face_valid = meta_face[meta_face[\"fallback_to_full\"] == False].copy()\n",
        "fh_valid = meta_fh[meta_fh[\"fallback_to_full\"] == False].copy()\n",
        "\n",
        "lh_conf = fh_valid[\"left_hand_confidence\"].fillna(0).clip(lower=0)\n",
        "rh_conf = fh_valid[\"right_hand_confidence\"].fillna(0).clip(lower=0)\n",
        "fh_valid[\"any_hand_conf\"] = np.maximum(lh_conf, rh_conf)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 9.5))\n",
        "\n",
        "# Row 1: Face-only\n",
        "axes[0, 0].hist(face_valid[\"roi_area_frac\"], bins=50, alpha=0.7, color=\"coral\", edgecolor=\"black\")\n",
        "axes[0, 0].set_xlabel(\"ROI Area Fraction\")\n",
        "axes[0, 0].set_title(\"Face-Only: ROI Area Distribution\")\n",
        "axes[0, 0].axvline(face_valid[\"roi_area_frac\"].median(), color=\"red\", linestyle=\"--\",\n",
        "                   label=f\"Median: {face_valid['roi_area_frac'].median():.3f}\")\n",
        "axes[0, 0].legend()\n",
        "\n",
        "axes[0, 1].hist(face_valid[\"roi_aspect\"], bins=50, alpha=0.7, color=\"coral\", edgecolor=\"black\")\n",
        "axes[0, 1].set_xlabel(\"ROI Aspect Ratio (W/H)\")\n",
        "axes[0, 1].set_title(\"Face-Only: Aspect Ratio Distribution\")\n",
        "\n",
        "axes[0, 2].hist(face_valid[\"face_confidence\"], bins=50, alpha=0.7, color=\"coral\", edgecolor=\"black\")\n",
        "axes[0, 2].set_xlabel(\"Face Detection Confidence\")\n",
        "axes[0, 2].set_title(\"Face-Only: Detection Confidence\")\n",
        "\n",
        "# Row 2: Face+Hands\n",
        "axes[1, 0].hist(fh_valid[\"roi_area_frac\"], bins=50, alpha=0.7, edgecolor=\"black\")\n",
        "axes[1, 0].set_xlabel(\"ROI Area Fraction\")\n",
        "axes[1, 0].set_title(\"Face+Hands: ROI Area Distribution\")\n",
        "\n",
        "axes[1, 1].hist(fh_valid[\"roi_aspect\"], bins=50, alpha=0.7, edgecolor=\"black\")\n",
        "axes[1, 1].set_xlabel(\"ROI Aspect Ratio (W/H)\")\n",
        "axes[1, 1].set_title(\"Face+Hands: Aspect Ratio Distribution\")\n",
        "\n",
        "axes[1, 2].hist(fh_valid[\"any_hand_conf\"], bins=50, alpha=0.7, edgecolor=\"black\")\n",
        "axes[1, 2].set_xlabel(\"Any-hand Confidence\")\n",
        "axes[1, 2].set_title(\"Face+Hands: Hand Detection Confidence\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(Path(OUT_ROOT) / \"metrics/crop_quality_distributions.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(f\"‚úÖ Saved: {Path(OUT_ROOT) / 'metrics/crop_quality_distributions.png'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Data Preparation Complete!\n",
        "\n",
        "**What was created (persists on Drive):**\n",
        "- `OUT_ROOT/manifests/manifest.csv` ‚Äî Full image manifest\n",
        "- `OUT_ROOT/splits/train.csv`, `val.csv`, `test.csv` ‚Äî Split CSVs\n",
        "- `OUT_ROOT/hybrid/face/` ‚Äî Face-only crops\n",
        "- `OUT_ROOT/hybrid/face_hands/` ‚Äî Face+hands crops\n",
        "- `OUT_ROOT/hybrid/*.csv` ‚Äî Hybrid manifests and splits\n",
        "- `OUT_ROOT/metrics/` ‚Äî Quality audit figures\n",
        "\n",
        "**Tar Archives (for fast loading):**\n",
        "- `DRIVE_DATA_ROOT/full_compressed.tar` ‚Äî Compressed full-frame images\n",
        "- `OUT_ROOT/hybrid/hybrid_face.tar` ‚Äî Face-only crops archive\n",
        "- `OUT_ROOT/hybrid/hybrid_face_hands.tar` ‚Äî Face+hands crops archive\n",
        "\n",
        "**‚ö° Performance Note:**\n",
        "Future sessions will copy tar archives (~5 min) instead of individual files (~2 hours).\n",
        "\n",
        "**Next steps:**\n",
        "- Run **02_training.ipynb** to train models on these crops\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
